<h1>Machine learning (and more) to forecast time series in quantitative</h1>
<h1>wealth and investment management QWIM</h1>
<h2>Cristian Homescu</h2>
<h2>December 2022</h2>
<p><code>Abstract
This document provides details for this QWIM project, and it incorporates the following sections</code>
- Motivation
- Relevant references
- Suggested project tasks and timelines
- Practical info
    Recommended software tools
    Recommended datasets
- Design and implementation for the project codes
- Potentially useful Python and R packages, codes and frameworks
- Appendices
    Appendices include
- Overviews of investment processes and models in QWIM
- Comparison of investment portfolios using portfolios metrics and benchmark portfolios</p>
<h2>Contents</h2>
<p><strong>1 Motivation for the project 4</strong>
1.1 Empirical asset pricing and risk premia forecasting............................ 4</p>
<p><strong>2 Relevant references 6</strong>
2.1 Main References............................................... 6
2.1.1 Main references on empirical asset pricing and forecasting of QWIM time series........ 6
2.1.2 Main references on time series forecasting............................. 7
2.2 Comprehensive list of references....................................... 8
2.2.1 Forecasting QWIM time series, empirical asset pricing, and predictability of financial returns 8
2.2.2 Testing procedures for QWIM time series, empirical asset pricing and predictability of financial
returns................................................ 11
2.2.3 Forecasting time series........................................ 12
2.2.4 Forecasting time series using Machine Learning.......................... 13
2.2.5 Testing procedures to evaluate and compare forecasts...................... 15
2.2.6 Combinations of forecasting methods for time series....................... 16
2.2.7 Combination of statistical and machine learning approaches................... 17
2.2.8 Probabilistic forecasting of time series............................... 18
2.2.9 Metrics to assess forecast performance............................... 18
2.2.10 Software implementations and frameworks............................ 19</p>
<ul>
<li>3 Practical details for the project</li>
<li>3.1 Interaction with students</li>
<li>3.2 Data.</li>
<li>3.3 Private GitHub repository for the QWIM project.</li>
<li>3.4 Deliverables.</li>
<li>3.5 (Optional) Article submission to leading journals</li>
<li>4 Project tasks and timelines</li>
<li>4.1 Suggested timelines for project tasks</li>
<li>4.2 Literature review.</li>
<li>4.3 Write-up summary of literature review.</li>
<li>4.4 Identification of appropriate Python and/or R packages</li>
<li>4.5 Code design.</li>
<li>4.6 Implementation of coding framework and components</li>
<li>4.7 Interactive visualizer.</li>
<li>4.8 Project report and presentation.</li>
<li>5 Design and implementation for the project codes</li>
<li>5.1 Visualize project workflow and coding framework.</li>
<li>5.2 Representative examples of Python libraries with well designed folder structure</li>
<li>6 Practical Info</li>
<li>6.1 Recommended software tools<ul>
<li>6.1.1 Python</li>
<li>6.1.2 R.</li>
<li>6.1.3 R IDE.</li>
<li>6.1.4 Python IDE.</li>
<li>6.1.5 Bibliography Manager</li>
<li>6.1.6 Document processor</li>
<li>6.1.7 Source control manager</li>
<li>6.1.8 File editor.</li>
<li>6.1.9 Runtime libraries.</li>
</ul>
</li>
<li>6.2 Recommended datasets</li>
<li>7 Potentially useful Python and R software implementations: packages, codes and frameworks</li>
<li>7.1 Collections and repositories of resources</li>
<li>7.2 Connection between Python and R codes</li>
<li>7.3 Anomaly detection and data outliers</li>
<li>7.4 Bayesian analysis and modeling.</li>
<li>7.5 Causality, inference and dependencies</li>
<li>7.6 Classification, Motifs, Neighbors, Wavelets, Transforms.</li>
<li>7.7 Clustering.</li>
<li>7.8 Coding utilities and frameworks.</li>
<li>7.9 Computational performance.</li>
<li>7.10 Containers, projects, pipelines and deployment</li>
<li>7.11 Covariances, correlations and volatilities.</li>
<li>7.12 Data analysis and exploration.</li>
<li>7.13 Data augmentation, scenario generation and synthetic time series.</li>
<li>7.14 Data cleaning, preparation and validation</li>
<li>7.15 Data Imputation</li>
<li>7.16 Data regimes, states and changepoints: analysis and modeling.</li>
<li>7.17 Data structures, storage and serialization</li>
<li>7.18 Dates and times</li>
<li>7.19 Dimensionality reduction</li>
<li>7.20 Distances and Similarity.</li>
<li>7.21 ESG and Impact Investing.</li>
<li>7.22 Explainability, Interpretability, Fairness, Data Privacy</li>
<li>7.23 Features for time series.</li>
<li>7.24 Filtering and spectral analysis for time series</li>
<li>7.25 Forecasting time series.</li>
<li>7.26 Graphs and graphical modeling.</li>
<li>7.27 Linear algebra.</li>
<li>7.28 Machine Learning.</li>
<li>7.29 Machine Learning frameworks (includes Automated ML and hyperparameters tuning)</li>
<li>7.30 Network and graph analysis.</li>
<li>7.31 Numerical methods (includes numerical optimization)</li>
<li>7.32 Probabilistic modeling (includes mixture models and Gaussian Processes)</li>
<li>7.33 Reinforcement learning.</li>
<li>7.34 Robust numerical methods.</li>
<li>7.35 Selection of features, variables, models, data splits</li>
<li>7.36 Sensitivity analysis and numerical derivatives</li>
<li>7.37 Statistics and Probability</li>
<li>7.38 Stress testing, rare events, extreme values and scenarios, survival analysis</li>
<li>7.39 Symbolic regression &amp; data-driven model discovery and machine learning</li>
<li>7.40 Testing (numerical, statistical, etc.), comparison and ranking</li>
<li>7.41 Testing software codes</li>
<li>7.42 Time series analysis and modeling</li>
<li>7.43 Text, sentiment and topic analytics (including NLP)</li>
<li>7.44 Uncertainty: analysis and modeling.</li>
<li>7.45 Visualization and reporting</li>
<li>8 Codes for QWIM (Quantitative Wealth and Investment Management)</li>
<li>8.1 Collections of resources</li>
<li>8.2 Research studies with code</li>
<li>8.3 Python software implementations.</li>
<li>8.4 R software implementations</li>
<li>References</li>
<li>Appendix A: Overviews of investment processes and models in QWIM</li>
<li>Appendix C: Comparison of investment using portfolios metrics and benchmark portfolios</li>
</ul>
<h2>1 Motivation for the project</h2>
<p>Over recent decades, machine learning (ML) algorithms have achieved remarkable success in various areas. The
key to their success is the fact that, given a large representative dataset,MLalgorithms can learn to identify
complex non-linear patterns and explore unstructured relationships without hypothesizing them a priori. Thus,
MLalgorithms are not limited by assumptions or pre-defined data generating processes, which allows the data to
speak for itself.
However, the superiority ofMLis not apparent when it comes to typical time series forecasting, where the data
availability is often limited, as shown by results of M4 competition. The strength ofMLalgorithms, and in fact the
requirement for their successful use, is cross-learning, i.e., using many series to train a single model. This is unlike
standard statistical time series algorithms, where a separate model is developed for each series.
Producing probabilistic forecasts for large collections of similar and/or dependent time series is a highly relevant,
yet challenging task in practice. While classical time series models fail to capture complex patterns in the data
and multivariate techniques struggle to scale to large problem sizes, their reliance on strong structural assumptions
makes them data-efficient and allows them to provide estimates of uncertainty. The converse is true for models
based on deep neural networks, which can learn complex patterns and dependencies given enough data. A hybrid
model that incorporates the benefits of both approaches can deliver better results.
Exploiting cross-series information in forecasting is an idea that gets increased attention lately, especially in the
aftermath of the M4 competition. The idea is that instead of developing one model per each time series in the
dataset, a global model is developed by exploiting information from many time series simultaneously.
Time series are often hierarchical. Many forecasting applications require multiple time series to be forecast
simultaneously. These are often hierarchical in nature and often represent sets of time series which can be highly
correlated
Recently, researchers have used the idea of developing global models within a machine learning context. The
winning solution at the M4 forecasting competition uses the global model concept with local parameters as well,
to cater for individual requirements of different series. This can also be done by clustering groups of related time
series. A global model is developed per each cluster.
Performance for time series forecasting has to be assesed through a combination of metrics
It is also argues that it is more practical to compareMLand statistical forecasting from other perspectives:</p>
<ul>
<li>global versus local methods</li>
<li>probabilistic versus point forecasts</li>
<li>data driven versus model-driven</li>
<li>ensemble versus single models</li>
<li>explanatory/interpretable versus predictive</li>
</ul>
<p>Combinations of forecasting methods appear to have best results for forecasting of time series of similar granularity
as the ones used inQWIM.</p>
<h3>1.1 Empirical asset pricing and risk premia forecasting</h3>
<p>Asset pricing literature has produced hundreds of potential risk factors. Literature mainly builds on linear or
linear-like models, due to simplicity, transparency and computational efficiency. However, this simplicity may miss
important features of returns, such as nonlinearity, clustering or dependency structure
The literature shows soncerns for risk premia estimation: potential omission of factors, measurement error,
nonlinearity, while linear risk factors may still fall short.
Relative to traditional empirical methods in asset pricing, machine learning accommodates a far more expansive
list of potential predictor variables, as well as richer specifications of functional form. Machine learning methods
can be successfully applied to the two canonical problems of empirical asset pricing: predicting returns in the cross
section and time series.
Gu et al. (“Empirical asset pricing via machine learning,” 2020):</p>
<ul>
<li>
<p>MLimproved performance compared to traditional methods for both cross section and time series stock return
    prediction</p>
</li>
<li>
<p>Predictive gains traced to inclusion of nonlinear interactions</p>
</li>
<li>MLmethods agree on a fairly small set of dominant predictive signals</li>
</ul>
<p>Leung et al. (“The Promises and Pitfalls of Machine Learning for Predicting Cross-Sectional Stock Returns,” 2020):
Despite statistical advantage ofMLmodel predictions, economic gains corresponding are more limited, and largely
dependent on ability to take risk and implement trades efficiently.
The literature has accumulated a staggering list of predictors that various researchers have argued possess fore-
casting power for returns. The number of stock-level predictive characteristics reported in the literature numbers
in the hundreds and macroeconomic predictors of the aggregate market number in the dozens. Additionally, predic-
tors are often close cousins and highly correlated. Traditional prediction methods break down when the predictor
count approaches the observation count or predictors are highly correlated. With an emphasis on variable selection
and dimension reduction techniques, machine learning is well suited for such challenging prediction problems by
reducing degrees of freedom and condensing redundant variation among predictors.
Moreover, there is ambiguity regarding functional forms through which the high-dimensional predictor set enter
into risk premia. Should they enter linearly? If nonlinearities are needed, which form should they take? Must we
consider interactions among predictors? Such questions rapidly proliferate the set of potential model specifications.
Three aspects of machine learning make it well suited for problems of ambiguous functional form. The first is
its diversity. As a suite of dissimilar methods it casts a wide net in its specification search. Second, with methods
ranging from generalized linear models to regression trees and neural networks, machine learning is explicitly
designed to approximate complex nonlinear associations. Third, parameter penalization and conservative model
selection criteria complement the breadth of functional forms spanned by these methods in order to avoid overfit
biases and false discovery.
Moreover, macroeconomic signals seem to substantially improve out-of-sample performance, especially when
non-linear features are incorporated via machine learning.</p>
<h2>2 Relevant references</h2>
<h3>2.1 Main References</h3>
<p><strong>2.1.1 Main references on empirical asset pricing and forecasting of QWIM time series</strong></p>
<p>List of references:
Abhyankar and Wu (“Circus Ring to Zoo to Museum: The Fragility of Factors in Characteristic-based Asset
Pricing Models,” 2020)
Baitinger and Flegel (“New Concepts in Financial Forecasting: Network-Based Information, Topological Data
Analysis and their Combination,” 2021)
Baltussen et al. (“Predicting Bond Returns: 70 Years of International Evidence,” 2021)
Bessembinder et al. (“Time Series Variation in the Factor Zoo,” 2022)
Bianchi et al. (“Bond Risk Premiums with Machine Learning,” 2021)
Bielinski and Broby (“Machine Learning Methods in Asset Pricing,” 2021)
Breitung and Knuppel (“How far can we forecast? Statistical tests of the predictive content,” 2021)
Bryzgalova et al. (“Bayesian solutions for the factor zoo: we just ran two quadrillion models,” 2021)
Chatigny et al. (“Neural forecasting at scale,” 2021)
Chen and Zimmermann (“Open Source Cross-Sectional Asset Pricing,” 2021)
Chen et al. (“Deep learning in asset pricing,” 2021)
Chiang et al. (“Modeling the cross-section of stock returns using sensible models in a model pool,” 2021)
Chib et al. (“On Comparing Asset Pricing Models,” 2020)
Cohen et al. (“Visual Time Series Forecasting: An Image-driven Approach,” 2021)
Coqueret and Guida ( <em>Machine Learning for Factor Investing: R Version</em> , 2020)
Czasonis et al. (“Addition by Subtraction: A Better Way to Forecast Factor Returns (and Everything Else),”
2020)
Czasonis et al. (“Relevance,” 2021)
Czasonis et al. (“The Past as Prologue: How to Forecast Presidential Elections,” 2021)
Faria and Verona (“Time-frequency forecast of the equity premium,” 2021)
Feng et al. (“Taming the factor zoo: A test of new factors,” 2020)
Freyberger et al. (“Dissecting Characteristics Nonparametrically,” 2020)
Giovannelli et al. (“Forecasting Stock Returns with Large Dimensional Factor Models,” 2021)
Gospodinov and Robotti (“Common pricing across asset classes: Empirical evidence revisited,” 2021)
Gu et al. (“Autoencoder asset pricing models,” 2021)
Gu et al. (“Empirical asset pricing via machine learning,” 2020)
Hassler and Pohle (“Forecasting under Long Memory,” 2021)
Iworiso and Vrontos (“On the Predictability of the Equity Premium Using Deep Learning Techniques,” 2021)
Kozak et al. (“Shrinking the cross-section,” 2020)
Lettau and Pelger (“Factors That Fit the Time Series and Cross-Section of Stock Returns,” 2020)
Leung et al. (“The Promises and Pitfalls of Machine Learning for Predicting Stock Returns,” 2021)
Meligkotsidou et al. (“Out-of-sample equity premium prediction: a complete subset quantile regression ap-
proach,” 2021)
McMillan (“Forecasting sector stock market returns,” 2021)
McMillan (“Forecasting U.S. stock returns,” 2021)
Messmer and Audrino (“The Lasso and the Factor Zoo - Expected Returns in the Cross-Section,” 2020)
Nietert and Otto (“Empirical asset pricing: economic significance and economic model evaluation,” 2020)
Oh and Patton (“Better the Devil You Know: Improved Forecasts from Imperfect Models,” 2021)
Rapach et al. (“Industry return predictability: A machine learning approach,” 2019)
Rapach and Zhou (“Time-series and Cross-sectional Stock Return Forecasting: New Machine Learning Meth-
ods,” 2020)
Rapach and Zhou (“Asset Pricing: Time-Series Predictability,” 2022)
Reschenhofer et al. (“Evaluation of current research on stock return predictability,” 2020)
Remlinger et al. (“Expert Aggregation for Financial Forecasting,” 2022)
Ruan et al. (“Stock Price Prediction Under Anomalous Circumstances,” 2021)
Smith and Timmermann (“Break Risk,” 2021)</p>
<p>Smith et al. (“Equity Premium Forecasts with an Unknown Number of Structural Breaks,” 2020)
Wang et al. (“The Best of Both Worlds: Forecasting US Equity Market Returns Using a Hybrid Machine
Learning Time Series Approach,” 2021)
Wang et al. (“Forecasting stock returns: A time-dependent weighted least squares approach,” 2021)
Weigand (“Machine learning in empirical asset pricing,” 2019)
Yang et al. (“Why Existing Machine Learning Methods Fails At Extracting the Information of Future Returns
Out of Historical Stock Prices : the Curve-Shape-Feature and Non-Curve-Shape-Feature Modes,” 2021)
Yin (“Equity premium prediction: keep it sophisticatedly simple,” 2021)</p>
<p><strong>2.1.2 Main references on time series forecasting</strong></p>
<p>List of references:
Alexander et al. (“Evaluating the Discrimination Ability of Proper Multivariate Scoring Rules,” 2021)
Alexandrov et al. (“GluonTS: Probabilistic and Neural Time Series Modeling in Python,” 2020)
Ansari et al. (“Deep Explicit Duration Switching Models for Time Series,” 2021)
Athanasopoulos and Kourentzes ( <em>On the evaluation of hierarchical forecasts</em> , 2020)
Atiya (“Why does forecast combination work so well?” 2020)
Bandara et al. (“Improving the accuracy of global forecasting models using time series data augmentation,”
2021)
Bandara et al. (“Forecasting across time series databases using recurrent neural networks on groups of similar
series: A clustering approach,” 2020)
Bandara et al. (“Improving the accuracy of global forecasting models using time series data augmentation,”
2021)
Benidis et al. (“Neural forecasting: Introduction and literature overview,” 2020)
Bauer et al. (“Telescope: An Automatic Feature Extraction and Transformation Approach for Time Series
Forecasting on a Level-Playing Field,” 2020)
Botchkarev (“A new typology design of performance metrics to measure errors in machine learning regression
algorithms,” 2019)
Castle et al. (“Forecasting Principles from Experience with Forecasting Competitions,” 2021)
Cerqueira et al. (“Evaluating time series forecasting models: an empirical study on performance estimation
methods,” 2020)
Dama and Sinoquet (“Analysis and modeling to forecast in time series: a systematic review,” 2021)
Faloutsos et al. (“Forecasting Big Time Series: Theory and Practice,” 2019)
Fosten and Gutknecht (“Horizon confidence sets,” 2021)
Gastinger et al. (“A study on Ensemble Learning for Time Series Forecasting and the need for Meta-Learning,”
2021)
Godahewa et al. (“Ensembles of localised models for time series forecasting,” 2021)
Grazzi et al. (“Meta-Forecasting by combining Global Deep Representations with Local Adaptation,” 2021)
Hannadige et al. (“Forecasting a Nonstationary Time Series Using a Mixture of Stationary and Nonstationary
Predictors,” 2021)
Hewamalage et al. (“Recurrent Neural Networks for Time Series Forecasting: Current status and future direc-
tions,” 2021)
Hunt (“In-sample tests of predictability are superior to pseudo-out-of-sample tests, even when data mining,”
2022)
Hyndman and Athanasopoulos ( <em>Forecasting: Principles and Practice (Third Edition)</em> , 2020)
Januschowski et al. (“Forecasting with trees,” 2022)
Kang et al. (“Déjà vu: A data-centric forecasting approach through time series cross-similarity,” 2021)
<strong>Kang-et-al-2021e</strong>
Lichtendahl and Winkler (“Why do some combinations perform better than others?” 2020)
Loning and Kiraly (“Forecasting with sktime: Designing sktime’s New Forecasting API and Applying It to
Replicate and Extend the M4 Study,” 2020)
Martin et al. (“Optimal probabilistic forecasts: When do they work?” 2022)
Masini et al. (“Machine Learning Advances for Time Series Forecasting,” 2021)</p>
<p>Montero-Manso and Hyndman (“Principles and Algorithms for Forecasting Groups of Time Series: Locality and
Globality,” 2021)
Neto et al. (“Uncovering regimes in out of sample forecast errors,” 2021)
Nybrant (“On Robust Forecast Combinations With Applications to Automated Forecasting,” 2021)
Oreshkin et al. (“N-BEATS: Neural basis expansion analysis for interpretable time series forecasting,” 2020)
Perron and Yamamoto (“Testing for Changes in Forecasting Performance,” 2021)
Petropoulos et al. (“Forecasting: theory and practice,” 2022)
Post et al. (“Robust optimization of forecast combinations,” 2019)
Qian et al. (“Combining forecasts for universally optimal performance,” 2022)
Quaedvlieg (“Multi-Horizon Forecast Comparison,” 2021)
Radchenko et al. (“Too similar to combine? On negative weights in forecast combination,” 2022)
Rossi (“Forecasting in the Presence of Instabilities: How Do We Know Whether Models Predict Well and How
to Improve Them,” 2020)
Salinas et al. (“DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks,” 2020)
Siliverstovs and Wochner (“State-Dependent Evaluation of Predictive Ability,” 2021)
Smyl (“A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting,”
2020)
Tadayon and Iwashita (“Comprehensive Analysis of Time Series Forecasting Using Neural Networks,” 2020)
Talagala et al. (“FFORMPP: Feature-based forecast model performance prediction,” 2022)
Thorarinsdottir (“Forecast evaluation,” 2021)
Wu et al. (“AutoCTS: Automated Correlated Time Series Forecasting – Extended Version,” 2021)</p>
<h3>2.2 Comprehensive list of references</h3>
<p><strong>2.2.1 Forecasting QWIM time series, empirical asset pricing, and predictability of financial returns</strong></p>
<p>List of references:
Ahmed et al. (“Best of the Best: A Comparison of Factor Models,” 2019)
Alhnaity and Abbod (“A new hybrid financial time series prediction model,” 2020)
Arias-Calluari et al. (“Methods for forecasting the effect of exogenous risks on stock markets,” 2021)
Babiak and Barunik (“Deep Learning, Predictability, and Optimal Portfolio Returns,” 2020)
Bahrami et al. (“Are advanced emerging market stock returns predictable? A regime-switching forecast combi-
nation approach,” 2019)
Bailey et al. (“Measurement of Factor Strength: Theory and Practice,” 2020)
Baitinger and Flegel (“New Concepts in Financial Forecasting: Network-Based Information, Topological Data
Analysis and their Combination,” 2021)
Bali et al. (“Different Strokes: Return Predictability Across Stocks and Bonds with Machine Learning and Big
Data,” 2021)
Baltas and Karyampas (“Forecasting the Equity Risk Premium: The Importance of Regime-Dependent Evalu-
ation,” 2020)
Baltussen et al. (“Predicting Bond Returns: 70 Years of International Evidence,” 2020)
Bektic et al. (“Factor-based investing in government bond markets: a survey of the current state of research,”
2020)
Bessembinder et al. (“Time Series Variation in the Factor Zoo,” 2022)
Bianchi and McAlinn (“Divide and Conquer: Financial Ratios and Industry Returns Predictability,” 2021)
Bianchi et al. (“Bond Risk Premiums with Machine Learning,” 2021)
Bianchi and Tamoni (“Sparse Predictive Regressions: Statistical Performance and Economic Significance,” 2020)
Bielinski and Broby (“Machine Learning Methods in Asset Pricing,” 2021)
Blitz et al. (“Five Concerns with the Five-Factor Model,” 2018)
Bryzgalova et al. (“Bayesian solutions for the factor zoo: we just ran two quadrillion models,” 2021)
Capolongo and Pacella (“Forecasting inflation in the euro area: countries matter!” 2021)
Carr and Wu ( <em>Decomposing Long Bond Returns: A Decentralized Theory</em> , 2021)
Castilho et al. (“Forecasting Financial Market Structure from Network Features using Machine Learning,” 2021)
Charles et al. (“Stock return predictability: Evaluation based on interval forecasts,” 2022)</p>
<p>Chatterjee et al. (“Stock Price Prediction Using Time Series, Econometric, Machine Learning, and Deep Learning
Models,” 2021)
Chen and Zimmermann (“Open Source Cross-Sectional Asset Pricing,” 2021)
Cheng et al. (“Financial time series forecasting with multi-modality graph neural network,” 2022)
Chevallier et al. (“Is It Possible to Forecast the Price of Bitcoin?” 2021)
Chiang et al. (“Modeling the cross-section of stock returns using sensible models in a model pool,” 2021)
Chib et al. (“Winners from Winners: A Tale of Risk Factors,” 2022)
Chib et al. (“On Comparing Asset Pricing Models,” 2020)
Chu (“Forecasting Recessions with Financial Variables and Temporal Dependence,” 2021)
Chudik et al. (“Variable Selection and Forecasting in High Dimensional Linear Regressions with Structural
Breaks,” 2021)
Cohen et al. (“Visual Forecasting of Time Series with Image-to-Image Regression,” 2020)
Cohen et al. (“Visual Time Series Forecasting: An Image-driven Approach,” 2021)
Collot and Hemauer (“A literature review of new methods in empirical asset pricing: omitted-variable and
errors-in-variable bias,” 2021)
Cong et al. (“Deep Sequence Modeling: Development and Applications in Asset Pricing,” 2021)
Cornell (“Stock characteristics and stock returns: a skeptic’s look at the cross section of expected returns,”
2020)
Czasonis et al. (“Addition by Subtraction: A Better Way to Forecast Factor Returns (and Everything Else),”
2020)
Czasonis et al. (“Relevance,” 2021)
Czasonis et al. (“The Past as Prologue: How to Forecast Presidential Elections,” 2021)
Dai et al. (“Predicting stock returns: A risk measurement perspective,” 2021)
Dai et al. (“Forecasting stock return volatility: The role of shrinkage approaches in a data-rich environment,”
2022)
Dendramis et al. (“A similarity-based approach for macroeconomic forecasting,” 2020)
Dong et al. (“Predictive power of ARIMA models in forecasting equity returns: a sliding window method,” 2020)
Dong et al. (“Anomalies and the expected market return,” 2022)
Drobetz and Otto (“Empirical Asset Pricing via Machine Learning: Evidence from the European Stock Market,”
2020)
Elkamhi et al. (“Factor Investing Using Capital Market Assumptions,” 2021)
Ellingsen et al. (“News media vs. FRED-MD for macroeconomic forecasting,” 2022)
Fama and French (“Choosing factors,” 2018)
Fama and French (“Comparing Cross-Section and Time-Series Factor Models,” 2020)
Fan et al. (“FarmTest: Factor-adjusted robust multiple testing with approximate false discovery control,” 2019)
Faria and Verona (“Time-frequency forecast of the equity premium,” 2021)
Fleiss and Cui (“Forecasting Stock Price Changes Using Natural Language Processing,” 2021)
Fulton and Hubrich (“Forecasting US Inflation in Real Time,” 2021)
Gafka et al. (“Sources of Return Predictability,” 2021)
Geertsema and Lu (“Long-horizon predictability and information decay in equity markets,” 2021)
Ghorbani and Chong (“A dimension reduction method for stock-price prediction using multiple predictors,”
2021)
Giovannelli et al. (“Forecasting stock returns with large dimensional factor models,” 2021)
Goliński and Spencer (“Estimating the Term Structure with Linear Regressions: Getting to the Roots of the
Problem,” 2021)
Gospodinov and Maasoumi (“Generalized aggregation of misspecified models: with an application to asset
pricing,” 2021)
Gospodinov and Robotti (“Common pricing across asset classes: Empirical evidence revisited,” 2021)
Gu et al. (“Autoencoder asset pricing models,” 2021)
Hammerschmid and Lohre (“Regime Shifts and Stock Return Predictability,” 2018)
Haase and Neuenkirch (“Forecasting Stock Market Recessions in the US: Predictive Modeling using Different
Identification Approaches,” 2021)
Harvey et al. (“Real-Time Detection of Regimes of Predictability in the U.S. Equity Premium,” 2021)
Hassler and Pohle (“Forecasting under Long Memory,” 2021)</p>
<p>Hauzenberger et al. (“Real-time Inflation Forecasting Using Non-linear Dimension Reduction Techniques,” 2021)
He and Gu (“Multi-modal Attention Network for Stock Movements Prediction,” 2022)
Ho and Lin (“Training by Rolling: Machine Learning and Stock Returns Forecasting,” 2021)
Hull and Qiao (“A Practitioner’s Defense of Return Predictability,” 2017)
Ilmanen et al. (“Demystifying illiquid assets: expected returns for private equity,” 2020)
Iworiso and Vrontos (“On the Predictability of the Equity Premium Using Deep Learning Techniques,” 2021)
Kalfa and Marquez (“Forecasting FOMC Forecasts,” 2021)
Karolyi and Van Nieuwerburgh (“New Methods for the Cross-Section of Returns,” 2020)
Kelly et al. (“Characteristics are covariances: A unified model of risk and return,” 2019)
Klingberg Malmer and Pettersson (“Tidying up the factor zoo: Using machine learning to find sparse factor
models that predict asset returns,” 2020)
Kozak et al. (“Shrinking the cross-section,” 2020)
Kynigakis and Panopoulou (“Does Model Complexity add Value to Asset Allocation? Evidence from Machine
Learning Forecasting Models,” 2021)
Lee and Seregina (“Optimal Portfolio Using Factor Graphical Lasso,” 2022)
Lettau and Pelger (“Factors That Fit the Time Series and Cross-Section of Stock Returns,” 2020)
Leung et al. (“The Promises and Pitfalls of Machine Learning for Predicting Stock Returns,” 2021)
Li and Bastos (“Stock Market Forecasting Using Deep Learning and Technical Analysis: A Systematic Review,”
2020)
Martinez et al. ( <em>Smooth Robust Multi-Horizon Forecasts</em> , 2020)
McMillan (“Forecasting sector stock market returns,” 2021)
McMillan (“Forecasting U.S. stock returns,” 2021)
Meligkotsidou et al. (“Out-of-sample equity premium prediction: a complete subset quantile regression ap-
proach,” 2021)
Neri (“Domain Specific Concept Drift Detectors for Predicting Financial Time Series,” 2021)
Nevasalmi (“Recession forecasting with high-dimensional data,” 2022)
Nietert and Otto (“Empirical asset pricing: economic significance and economic model evaluation,” 2020)
Noguer i Alonso and Srivastava (“The Shape of Performance Curve in Financial Time Series,” 2021)
Noguer i Alonso et al. (“Deep Learning for Equity Time Series Prediction,” 2020)
Nonejad (“Bayesian model averaging and the conditional volatility process: an application to predicting aggre-
gate equity returns by conditioning on economic variables,” 2021)
Oh and Patton (“Better the Devil You Know: Improved Forecasts from Imperfect Models,” 2021)
Paranhos (“Predicting Inflation with Neural Networks,” 2021)
Pinho (“Forecast comparison of volatility models and their combinations (FTSE100): a tied race,” 2020)
Rahimikia and Poon (“Machine Learning for Realised Volatility Forecasting,” 2021)
Rapach et al. (“Industry return predictability: A machine learning approach,” 2019)
Rapach and Zhou (“Time-series and Cross-sectional Stock Return Forecasting: New Machine Learning Meth-
ods,” 2020)
Rapach and Zhou (“Asset Pricing: Time-Series Predictability,” 2022)
Reschenhofer et al. (“Evaluation of current research on stock return predictability,” 2020)
Roy (“A six-factor asset pricing model: The Japanese evidence,” 2021)
Salisu and Tchankam (“US Stock return predictability with high dimensional models,” 2022)
Smith and Timmermann (“Break Risk,” 2021)
Smith et al. (“Equity Premium Forecasts with an Unknown Number of Structural Breaks,” 2020)
Son and Lee (“Graph-based multi-factor asset pricing model,” 2022)
Stein (“Out-of-Sample Equity Premium Prediction: Combination Forecasts with Frequency-Decomposed Vari-
ables,” 2021)
Stivers (“Equity premium predictions with many predictors: A risk-based explanation of the size and value
factors,” 2018)
Stoyanov and Fabozzi (“Dynamics of Equity Factor Returns and Asset Pricing,” 2021)
Tilly and Livan (“Macroeconomic forecasting with statistically validated knowledge graphs,” 2021)
Tilly et al. (“Macroeconomic forecasting through news, emotions and narrative,” 2021)
Timmermann (“Forecasting methods in finance,” 2018)</p>
<p>Trucı́os et al. (“Forecasting Conditional Covariance Matrices in High-Dimensional Time Series: a General Dy-
namic Factor Approach,” 2021)
Viswanathan and Stephen (“Does Machine Learning Algorithms Improve Forecasting Accuracy? Predicting
Stock Market Index Using Ensemble Model,” 2020)
Wang et al. (“The Best of Both Worlds: Forecasting US Equity Market Returns Using a Hybrid Machine
Learning Time Series Approach,” 2021)
Wang et al. (“Forecasting stock returns: A time-dependent weighted least squares approach,” 2021)
Weigand (“Machine learning in empirical asset pricing,” 2019)
Wu et al. (“Equity2Vec: End-to-end Deep Learning Framework for Cross-sectional Asset Pricing,” 2021)
Xu et al. (“HIST: A Graph-based Framework for Stock Trend Forecasting via Mining Concept-Oriented Shared
Information,” 2022)
Yang et al. (“NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-task Financial Forecast-
ing,” 2022)
Yang et al. (“Why Existing Machine Learning Methods Fails At Extracting the Information of Future Returns
Out of Historical Stock Prices : the Curve-Shape-Feature and Non-Curve-Shape-Feature Modes,” 2021)
Yara et al. (“Value return predictability across asset classes and commonalities in risk premia,” 2021)
Yin (“Equity premium prediction: keep it sophisticatedly simple,” 2021)
Zeng et al. (“Deep Video Prediction for Time Series Forecasting,” 2021)
Zhan and Xiao (“A Fast Evidential Approach for Stock Forecasting,” 2021)
Zhang (“Empirical asset pricing and ensemble machine learning,” 2021)
Zhu et al. (“High-Dimensional Estimation, Basis Assets, and the Adaptive Multi-Factor Model,” 2021)</p>
<p><strong>2.2.2 Testing procedures for QWIM time series, empirical asset pricing and predictability of finan-
cial returns</strong></p>
<p>List of references:
Ardia and Dufays (“Measuring uncertainty and uncertainty dispersion from a large set of model predictions,”
2021)
Barillas and Shanken (“Real-time Portfolio Choice Implications of Asset Pricing Models,” 2019)
Barras (“A large-scale approach for evaluating asset pricing models,” 2019)
Bryzgalova et al. (“Bayesian solutions for the factor zoo: we just ran two quadrillion models,” 2021)
Cai et al. (“Testing capital asset pricing models using functional-coefficient panel data models with cross-sectional
dependence,” 2022)
Chai et al. (“Which model best explains the returns of large Australian stocks?” 2019)
Charles et al. (“Stock return predictability: Evaluation based on interval forecasts,” 2022)
Chen et al. (“Predicting returns out of sample: A naive model averaging approach,” 2020)
Chiah et al. (“A Better Model? An Empirical Investigation of the Fama-French Five-factor Model in Australia,”
2016)
Chib and Zeng (“Which factors are risk factors in asset pricing? A model scan framework,” 2020)
Chib et al. (“On Comparing Asset Pricing Models,” 2020)
Chib et al. (“Winners from Winners: A Tale of Risk Factors,” 2022)
Chordia et al. (“Anomalies and false rejections,” 2020)
Frenkel et al. (“Testing for the rationality of central bank interest rate forecasts,” 2021)
Gospodinov and Robotti (“Common pricing across asset classes: Empirical evidence revisited,” 2021)
Goyal and Jegadeesh (“Cross-Sectional and Time-Series Tests of Return Predictability: What Is the Difference?”
2018)
Gramespacher and Banziger (“The Bias in Two-Pass Regression Tests of Asset-Pricing Models in Presence of
Idiosyncratic Errors with Cross-Sectional Dependence,” 2019)
Harvey and Liu (“Detecting Repeatable Performance,” 2020)
Harvey et al. (“An Evaluation of Alternative Multiple Testing Methods for Finance Applications,” 2020)
Hoga and Dimitriadis (“On Testing Equal Conditional Predictive Ability Under Measurement Error,” 2021)
Janssen (“Multi-horizon comparison of multivariate inflation forecasting,” 2019)
Jegadeesh et al. (“Empirical tests of asset pricing models with individual assets: Resolving the errors-in-variables
bias in risk premium estimation,” 2019)</p>
<p>Kelly et al. (“Characteristics are covariances: A unified model of risk and return,” 2019)
Kruse et al. (“Comparing Predictive Accuracy under Long Memory, With an Application to Volatility Forecast-
ing,” 2019)
Kyriakou et al. (“Longer-Term Forecasting of Excess Stock Returns – The Five-Year Case,” 2020)
Ledoit et al. (“Efficient Sorting: A More Powerful Test for Cross-Sectional Anomalies,” 2019)
Mikeliani and Kavlashvili (“Evaluation and comparison of machine learning and classical econometric AR model
on financial time series data,” 2020)
Odendahl et al. (“Comparing Forecast Performance with State Dependence,” 2020)
Pesaran and Smith (“The Role of Factor Strength and Pricing Errors for Estimation and Inference in Asset
Pricing Models,” 2019)
Pinho (“Forecast comparison of volatility models and their combinations (FTSE100): a tied race,” 2020)
Prasad et al. (“Prediction of Stock Prices Using Statistical and Machine Learning Models: A Comparative
Analysis,” 2021)
Siami-Namini et al. (“A Comparative Analysis of Forecasting Financial Time Series Using ARIMA, LSTM, and
BiLSTM,” 2019)
Siliverstovs and Wochner (“State-Dependent Evaluation of Predictive Ability,” 2021)
Suhonen et al. (“Quantifying Backtest Overfitting in Alternative Beta Strategies,” 2017)
Tang et al. (“Out-of-sample equity premium prediction: A scenario analysis approach,” 2018)
Tunaru et al. (“Testing the Forecasting Ability of Multi-Factor Models on Non-US Interbank Rates,” 2021)
Vincent et al. (“Investment styles and the multiple testing of cross-sectional stock return predictability,” 2020)
Xie (“Forecasting Long-Term Equity Returns: A Comparison of Popular Methodologies,” 2021)
Zhao (“Essays on Asset Pricing: A Model Comparison Perspective,” 2020)</p>
<p><strong>2.2.3 Forecasting time series</strong></p>
<p>List of references:
Alexandrov et al. (“GluonTS: Probabilistic and Neural Time Series Modeling in Python,” 2020)
Ankile and Krange (“The DONUT Approach to Ensemble Combination Forecasting,” 2022)
Ansari et al. (“Deep Explicit Duration Switching Models for Time Series,” 2021)
Ashouri et al. (“Fast Forecast Reconciliation Using Linear Models,” 2022)
Athanasopoulos and Kourentzes ( <em>On the evaluation of hierarchical forecasts</em> , 2020)
Athanasopoulos et al. (“Hierarchical Forecasting,” 2019)
Bandara et al. (“Improving the accuracy of global forecasting models using time series data augmentation,”
2021)
Berardi et al. (“Mind the (Convergence) Gap: Bond Predictability Strikes Back!” 2021)
Berrisch and Ziel (“CRPS Learning,” 2021)
Bisaglia and Grigoletto (“A new time-varying model for forecasting long-memory series,” 2021)
Castle et al. (“Forecasting Principles from Experience with Forecasting Competitions,” 2021)
Castle et al. (“Selecting a Model for Forecasting,” 2021)
Cerqueira et al. (“Evaluating time series forecasting models: an empirical study on performance estimation
methods,” 2020)
Cerqueira et al. (“Model Selection for Time Series Forecasting: Empirical Analysis of Different Estimators,”
2021)
Challu et al. (“DMIDAS: Deep Mixed Data Sampling Regression for Long Multi-Horizon Time Series Forecast-
ing,” 2021)
Chen et al. (“Learning from Multiple Time Series: A Deep Disentangled Approach to Diversified Time Series
Forecasting,” 2021)
Dama and Sinoquet (“Analysis and modeling to forecast in time series: a systematic review,” 2021)
Deshpande et al. (“Long Horizon Forecasting With Temporal Point Processes,” 2021)
Faloutsos et al. (“Forecasting Big Time Series: Theory and Practice,” 2019)
Feldkircher et al. (“Factor Augmented Vector Autoregressions, Panel VARs, and Global VARs,” 2020)
Godahewa et al. (“Monash Time Series Forecasting Archive,” 2021)
Grazzi et al. (“Meta-Forecasting by combining Global Deep Representations with Local Adaptation,” 2021)
Harris et al. (“Construction and visualization of confidence sets for frequentist distributional forecasts,” 2019)</p>
<p>Hassler and Pohle (“Forecasting under Long Memory,” 2021)
Hewamalage et al. (“Global Models for Time Series Forecasting: A Simulation Study,” 2020)
Hewamalage et al. (“Recurrent Neural Networks for Time Series Forecasting: Current status and future direc-
tions,” 2021)
Hyndman and Athanasopoulos ( <em>Forecasting: Principles and Practice (Third Edition)</em> , 2020)
Inoue et al. (“Rolling window selection for out-of-sample forecasting with time-varying parameters,” 2017)
Kang et al. (“Déjà vu: A data-centric forecasting approach through time series cross-similarity,” 2021)
Liu et al. (“Forecast Methods for Time Series Data: A Survey,” 2021)
Loning and Kiraly (“Forecasting with sktime: Designing sktime’s New Forecasting API and Applying It to
Replicate and Extend the M4 Study,” 2020)
Makridakis et al. (“Forecasting in social settings: The state of the art,” 2020)
Masini et al. (“Machine Learning Advances for Time Series Forecasting,” 2021)
Montero-Manso and Hyndman (“Principles and Algorithms for Forecasting Groups of Time Series: Locality and
Globality,” 2020)
Nystrup et al. (“Dimensionality reduction in forecasting with temporal hierarchies,” 2021)
Petropoulos and Grushka-Cockayne (“Fast and Frugal Time Series Forecasting,” 2021)
Petropoulos et al. (“Forecasting: theory and practice,” 2022)
Okuno et al. (“Combining multiple forecasts for multivariate time series via state-dependent weighting.,” 2019)
Oreshkin et al. (“N-BEATS: Neural basis expansion analysis for interpretable time series forecasting,” 2019)
Salinas et al. (“DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks,” 2020)
Smyl (“A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting,”
2020)
Tadayon and Iwashita (“Comprehensive Analysis of Time Series Forecasting Using Neural Networks,” 2020)
Talagala et al. (“FFORMPP: Feature-based forecast model performance prediction,” 2022)
Timmermann (“Forecasting methods in finance,” 2018)
Wang et al. (“Deep Factors for Forecasting,” 2019)
Wellens et al. (“Transfer learning for hierarchical forecasting: Reducing computational efforts of M5 winning
methods,” 2022)
Wen et al. (“Forecasting realized volatility of Chinese stock market: A simple but efficient truncated approach,”
2022)
Wu et al. (“AutoCTS: Automated Correlated Time Series Forecasting – Extended Version,” 2021)</p>
<p><strong>2.2.4 Forecasting time series using Machine Learning</strong></p>
<p>List of references:
Alexandrov et al. (“GluonTS: Probabilistic and Neural Time Series Modeling in Python,” 2020)
Ansari et al. (“Deep Explicit Duration Switching Models for Time Series,” 2021)
Babii et al. (“Machine Learning Panel Data Regressions with Heavy-tailed Dependent Data: Theory and Ap-
plication,” 2021)
Bhatnagar et al. (“Merlion: A Machine Learning Library for Time Series,” 2021)
Bielinski and Broby (“Machine Learning Methods in Asset Pricing,” 2021)
Bloemheuvel et al. (“Multivariate Time Series Regression with Graph Neural Networks,” 2022)
Castilho et al. (“Forecasting Financial Market Structure from Network Features using Machine Learning,” 2021)
Chen et al. (“Deep learning in asset pricing,” 2021)
Chen et al. (“Multi-Scale Adaptive Graph Neural Network for Multivariate Time Series Forecasting,” 2022)
Chatterjee et al. (“Stock Price Prediction Using Time Series, Econometric, Machine Learning, and Deep Learning
Models,” 2021)
Chatigny et al. (“Neural forecasting at scale,” 2021)
Cholakov and Kolev (“Transformers predicting the future. Applying attention in next-frame and time series
forecasting,” 2021)
Cohen et al. (“Visual Time Series Forecasting: An Image-driven Approach,” 2021)
Debnath et al. (“Exploring Generative Data Augmentation in Multivariate Time Series Forecasting : Opportu-
nities and Challenges,” 2021)
Du et al. (“AdaRNN: Adaptive Learning and Forecasting of Time Series,” 2021)</p>
<p>Faloutsos et al. (“Forecasting Big Time Series: Theory and Practice,” 2019)
Filipovic and Khalilzadeh (“Machine Learning for Predicting Stock Return Volatility,” 2021)
Fjellstrom (“Long Short-Term Memory Neural Network for Financial Time Series,” 2022)
Geertsema and Lu (“Long-horizon predictability and information decay in equity markets,” 2021)
Harris et al. (“Construction and visualization of confidence sets for frequentist distributional forecasts,” 2019)
Herzen et al. (“Darts: User-Friendly Modern Machine Learning for Time Series,” 2022)
Hewamalage et al. (“Recurrent Neural Networks for Time Series Forecasting: Current status and future direc-
tions,” 2021)
Januschowski et al. (“Forecasting with trees,” 2022)
Jin et al. (“Robust Forecast Comparison,” 2017)
Kiefer et al. (“Univariate Time Series Forecasting: Machine Learning Prediction of the Best Suitable Forecast
Model Based on Time Series Characteristics,” 2021)
Kosman and Castro (“Vision-Guided Forecasting – Visual Context for Multi-Horizon Time Series Forecasting,”
2021)
Kynigakis and Panopoulou (“Does Model Complexity add Value to Asset Allocation? Evidence from Machine
Learning Forecasting Models,” 2021)
Lara-Benı́tez et al. (“Evaluation of the Transformer Architecture for Univariate Time Series Forecasting,” 2021)
Lara-Benitez et al. (“An Experimental Review on Deep Learning Architectures for Time Series Forecasting,”
2021)
Le Guen and Thome (“Deep Time Series Forecasting with Shape and Temporal Criteria,” 2021)
Li and Bastos (“Stock Market Forecasting Using Deep Learning and Technical Analysis: A Systematic Review,”
2020)
Li et al. (“Bayesian forecast combination using time-varying features,” 2021)
Lim and Zohren (“Time-series forecasting with deep learning: a survey,” 2021)
Liu et al. (“Time Series is a Special Sequence: Forecasting with Sample Convolution and Interaction,” 2021)
Loning and Kiraly (“Forecasting with sktime: Designing sktime’s New Forecasting API and Applying It to
Replicate and Extend the M4 Study,” 2020)
Mancuso et al. (“A machine learning approach for forecasting hierarchical time series,” 2021)
Masini et al. (“Machine Learning Advances for Time Series Forecasting,” 2021)
Nevasalmi (“Forecasting multinomial stock returns using machine learning methods,” 2020)
Noguer i Alonso and Srivastava (“The Shape of Performance Curve in Financial Time Series,” 2021)
Papaioannou et al. (“Time Series Forecasting Using Manifold Learning,” 2021)
Paranhos (“Predicting Inflation with Neural Networks,” 2021)
Oreshkin et al. (“N-BEATS: Neural basis expansion analysis for interpretable time series forecasting,” 2019)
Petropoulos and Spiliotis (“The Wisdom of the Data: Getting the Most Out of Univariate Time Series Fore-
casting,” 2021)
Petropoulos et al. (“Forecasting: theory and practice,” 2022)
Pinto and Castle ( <em>A machine learning dynamic switching approach to forecasting when there are structural
breaks</em> , 2021)
Rajapaksha et al. (“LoMEF: A Framework to Produce Local Explanations for Global Model Time Series Fore-
casts,” 2021)
Rožanec et al. (“Explaining Bad Forecasts in Global Time Series Models,” 2021)
Salinas et al. (“DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks,” 2020)
Smyl (“A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting,”
2020)
Spiliotis et al. (“Hierarchical forecast reconciliation with machine learning,” 2021)
Tadayon and Iwashita (“Comprehensive Analysis of Time Series Forecasting Using Neural Networks,” 2020)
Theodosiou and Kourentzes (“Forecasting with Deep Temporal Hierarchies,” 2021)
Viswanathan and Stephen (“Does Machine Learning Algorithms Improve Forecasting Accuracy? Predicting
Stock Market Index Using Ensemble Model,” 2020)
Wang et al. (“Deep Factors for Forecasting,” 2019)
Wellens et al. (“Transfer learning for hierarchical forecasting: Reducing computational efforts of M5 winning
methods,” 2022)</p>
<p>Wen et al. (“Forecasting realized volatility of Chinese stock market: A simple but efficient truncated approach,”
2022)
Wu et al. (“Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting,”
2022)
Xu et al. (“Instance-wise Graph-based Framework for Multivariate Time Series Forecasting,” 2021)
Xu et al. (“HIST: A Graph-based Framework for Stock Trend Forecasting via Mining Concept-Oriented Shared
Information,” 2022)
Yang et al. (“Why Existing Machine Learning Methods Fails At Extracting the Information of Future Returns
Out of Historical Stock Prices : the Curve-Shape-Feature and Non-Curve-Shape-Feature Modes,” 2021)
Zhang (“Empirical asset pricing and ensemble machine learning,” 2021)</p>
<p><strong>2.2.5 Testing procedures to evaluate and compare forecasts</strong></p>
<p>List of references:
Anghel (“Data Snooping Bias in Tests of the Relative Performance of Multiple Forecasting Models,” 2021)
Aparicio and Lopez de Prado (“How Hard Is It to Pick the Right Model? MCS and backtest overfitting,” 2018)
Arnold et al. (“Sequentially valid tests for forecast calibration,” 2022)
Arvanitis et al. (“Nonparametric tests for Optimal Predictive Ability,” 2021)
Bates et al. (“Cross-validation: what does it estimate and how well does it do it?” 2021)
Ben Baccar (“Comparative Study on Time Series Forecasting Models,” 2019)
Bergmeir et al. (“A note on the validity of cross-validation for evaluating autoregressive time series prediction,”
2018)
Bouallegue et al. (“The diagonal score: Definition, properties, and interpretations,” 2018)
Brehmer and Gneiting (“Properization: constructing proper scoring rules via Bayes acts,” 2020)
Brehmer et al. (“Using scoring functions to evaluate point process forecasts,” 2021)
Breitung and Knuppel (“How far can we forecast? Statistical tests of the predictive content,” 2021)
Bulut (“Does Statistical Significance Help to Evaluate Predictive Performance of Competing Models?” 2019)
Cerqueira et al. (“Model Selection for Time Series Forecasting: Empirical Analysis of Different Estimators,”
2021)
Cetin and Yavuz (“Comparison of forecast accuracy of Ata and exponential smoothing,” 2021)
Choe and Ramdas (“Comparing Sequential Forecasters,” 2022)
Coroneo et al. (“Testing the Predictive Accuracy of COVID-19 Forecasts,” 2021)
Costantini and Kunst (“On using predictive-ability tests in the selection of time-series prediction models: A
Monte Carlo evaluation,” 2021)
Davydenko and Goodwin (“Assessing Point Forecast Bias Across Multiple Time Series: Measures and Visual
Tools,” 2021)
De Baets and Harvey (“Using judgment to select and adjust forecasts from statistical models,” 2020)
Diebold (“Comparing Predictive Accuracy, Twenty Years Later: A Personal Perspective on the Use and Abuse
of Diebold-Mariano Tests,” 2015)
Fauvel et al. (“A Performance-Explainability Framework to Benchmark Machine Learning Methods: Application
to Multivariate Time Series Classifiers,” 2021)
Fosten and Gutknecht (“Horizon confidence sets,” 2021)
Geweke and Amisano (“Comparing and evaluating Bayesian predictive distributions of asset returns,” 2010)
Gilleland et al. (“Testing the Tests: What Are the Impacts of Incorrect Assumptions When Applying Confidence
Intervals or Hypothesis Tests to Compare Competing Forecasts?” 2018)
Gneiting and Resin (“Regression Diagnostics meets Forecast Evaluation: Conditional Calibration, Reliability
Diagrams, and Coefficient of Determination,” 2022)
Hounyo and Lahiri (“Estimating the variance of a combined forecast: Bootstrap-based approach,” 2022)
Hunt (“In-sample tests of predictability are superior to pseudo-out-of-sample tests, even when data mining,”
2022)
Ilic et al. (“Augmented Out-of-Sample Comparison Method for Time Series Forecasting Techniques,” 2020)
Jin et al. (“Robust Forecast Comparison,” 2017)
Kang et al. (“Assessing Goodness of Fit for Verifying Probabilistic Forecasts,” 2021)
Koutsandreas et al. (“On the selection of forecasting accuracy measures,” 2021)</p>
<p>Lerch et al. (“Forecaster’s Dilemma: Extreme Events and Forecast Evaluation,” 2015)
Lerch et al. (“Forecaster’s Dilemma: Extreme Events and Forecast Evaluation,” 2017)
Martin et al. (“Optimal probabilistic forecasts: When do they work?” 2022)
McCracken (“Tests of Conditional Predictive Ability: Existence, Size, and Power,” 2020)
Murray and Blume (“False Discovery Rate Computation: Illustrations and Modifications,” 2020)
Neto et al. (“Uncovering regimes in out of sample forecast errors,” 2021)
Patton (“Comparing Possibly Misspecified Forecasts,” 2020)
Perron and Yamamoto (“Testing for Changes in Forecasting Performance,” 2021)
Pitarakis (“A Novel Approach to Predictive Accuracy Testing in Nested Environments,” 2020)
Qu et al. (“Comparing forecasting performance in cross-sections,” 2021)
Quaedvlieg (“Multi-Horizon Forecast Comparison,” 2021)
Rožanec et al. (“Explaining Bad Forecasts in Global Time Series Models,” 2021)
Rytchkov and Zhong (“Information Aggregation and P-Hacking,” 2020)
Sharma et al. (“Prediction-Oriented Model Selection in Partial Least Squares Path Modeling,” 2020)
Siliverstovs and Wochner (“State-Dependent Evaluation of Predictive Ability,” 2021)
Spiliotis et al. (“Tales from tails: On the empirical distributions of forecasting errors and their implication to
risk,” 2019)
Stauskas and Westerlund (“Tests of Equal Forecasting Accuracy for Nested Models with Estimated CCE Fac-
tors,” 2022)
Taggart (“Evaluation of point forecasts for extreme events using consistent scoring functions,” 2021)
Taillardat et al. (“Extreme events evaluation using CRPS distributions,” 2022)
Vovk and Wang (“E-values: Calibration, combination, and applications,” 2021)
Wang and Ramdas (“False discovery rate control with e-values,” 2020)
Westerlund et al. (“Testing for Predictability in panels with General Predictors,” 2017)
Yeoleka et al. (“Feature Selection on a Flare Forecasting Testbed: A Comparative Study of 24 Methods,” 2021)
Zhao et al. (“Empirical Quantitative Analysis of COVID-19 Forecasting Models,” 2021)
Zhu and Timmermann (“Can Two Forecasts Have the Same Conditional Expected Accuracy?” 2020)
Ziel and Berk (“Multivariate Forecasting Evaluation: On Sensitive and Strictly Proper Scoring Rules,” 2019)</p>
<p><strong>2.2.6 Combinations of forecasting methods for time series</strong></p>
<p>List of references:
Atiya (“Why does forecast combination work so well?” 2020)
Bahrami et al. (“Are advanced emerging market stock returns predictable? A regime-switching forecast combi-
nation approach,” 2019)
Caldeira et al. (“Yield curve forecast combinations based on bond portfolio performance,” 2018)
Cerqueira et al. (“Model Compression for Dynamic Forecast Combination,” 2021)
Chan and Pauwels (“Some theoretical results on forecast combinations,” 2018)
Di Fonzo and Girolimetto (“Cross-temporal forecast reconciliation: Optimal combination method and heuristic
alternatives,” 2020)
Di Fonzo and Girolimetto (“Forecast combination based forecast reconciliation: insights and extensions,” 2021)
Di Fonzo and Girolimetto (“Cross-temporal forecast reconciliation: Optimal combination method and heuristic
alternatives,” 2022)
Fameliti and Skintzi (“Predictive ability and economic gains from volatility forecast combinations,” 2020)
Fang et al. (“Optimal forecast combination based on ensemble empirical mode decomposition for agricultural
commodity futures prices,” 2020)
Godahewa et al. (“Ensembles of localised models for time series forecasting,” 2021)
Hannadige et al. (“Forecasting a Nonstationary Time Series Using a Mixture of Stationary and Nonstationary
Predictors,” 2021)
Hofmarcher and Grun (“Bayesian Model Averaging,” 2020)
Hollyman et al. (“Understanding forecast reconciliation,” 2021)
Holzmann and Klar (“Using Proxies to Improve Forecast Evaluation,” 2021)
Hsiao and Wan (“Is there an optimal forecast combination?” 2014)
Jaganathan and Prakash (“A combination-based forecasting method for the M4-competition,” 2020)</p>
<p>Lee and Seregina (“Optimal Portfolio Using Factor Graphical Lasso,” 2022)
Lichtendahl and Winkler (“Why do some combinations perform better than others?” 2020)
Montero-Manso et al. (“FFORMA: Feature-based Forecast Model Averaging,” 2020)
Montero-Manso and Hyndman (“Principles and Algorithms for Forecasting Groups of Time Series: Locality and
Globality,” 2021)
Nybrant (“On Robust Forecast Combinations With Applications to Automated Forecasting,” 2021)
Okuno et al. (“Combining multiple forecasts for multivariate time series via state-dependent weighting.,” 2019)
Patton (“Comparing Possibly Misspecified Forecasts,” 2020)
Petropoulos and Svetunkov (“A simple combination of univariate models,” 2020)
Petropoulos et al. (“Model combinations through revised base-rates,” 2021)
Post et al. (“Robust optimization of forecast combinations,” 2019)
Qian et al. (“On the forecast combination puzzle,” 2019)
Qian et al. (“Combining forecasts for universally optimal performance,” 2022)
Radchenko et al. (“Too similar to combine? On negative weights in forecast combination,” 2022)
Rehman et al. (“Individual and combination approaches to forecasting hierarchical time series with correlated
data: an empirical study,” 2019)
Roccazzella et al. (“Optimal and robust combination of forecasts via constrained optimization and shrinkage,”
2022)
Shaub (“Fast and accurate yearly time series forecasting with forecast combinations,” 2020)
Stein (“Out-of-Sample Equity Premium Prediction: Combination Forecasts with Frequency-Decomposed Vari-
ables,” 2021)
Svensson (“An Evaluation of Methods for Combining Univariate Time Series Forecasts,” 2018)
Thomson et al. (“Combining forecasts: Performance and coherence,” 2019)
Vaiciukynas et al. (“Two-Step Meta-Learning for Time-Series Forecasting Ensemble,” 2022)
van Dijk and Franses (“Combining expert-adjusted forecasts,” 2019)
Weiss et al. (“Forecast Combinations in R using the ForecastComb Package,” 2018)
Winkler (“Equal Versus Differential Weighting in Combining Forecasts,” 2015)
Zhang (“Empirical asset pricing and ensemble machine learning,” 2021)
Zhao (“The robustness of forecast combination in unstable environments: a Monte Carlo study of advanced
algorithms,” 2021)</p>
<p><strong>2.2.7 Combination of statistical and machine learning approaches</strong></p>
<p>List of references:
Allende and Valle (“Ensemble methods for time series forecasting,” 2017)
Barrow and Crone (“A comparison of AdaBoost algorithms for time series forecast combination,” 2016)
Bergmeir et al. (“Time Series Modeling and Forecasting Using Memetic Algorithms for Regime-Switching Mod-
els,” 2012)
Billio et al. (“Time-varying combinations of predictive densities using nonlinear filtering,” 2013)
Gilliland (“The value added by machine learning approaches in forecasting,” 2020)
Grazzi et al. (“Meta-Forecasting by combining Global Deep Representations with Local Adaptation,” 2021)
Habibnia (“Essays in high-dimensional nonlinear time series analysis,” 2016)
Hewamalage et al. (“Recurrent Neural Networks for Time Series Forecasting: Current status and future direc-
tions,” 2021)
Joshi (“Time Series Analysis and Forecasting of the US Housing Starts using Econometric and Machine Learning
Model,” 2019)
Karathanasopoulos et al. (“Modelling and Trading the English and German Stock Markets with Novelty Opti-
mization Techniques,” 2017)
Kuznetsov and Mohri (“Time series prediction and online learning,” 2016)
McDonald et al. (“A comparison of forecasting approaches for capital markets,” 2014)
Menezes and Mastelini (“MegazordNet: combining statistical and machine learning standpoints for time series
forecasting,” 2021)
Pinto and Marçal (“Cross-Validation Based Forecasting Method: A Machine Learning Approach,” 2019)</p>
<p>Pinto and Marçal (“Inflation Rate Forecasting: Extreme Learning Machine as a Model Combination Method,”
2020)
Qian et al. (“Combining forecasts for universally optimal performance,” 2022)
Risse (“Combining Wavelet Decomposition with Machine Learning to Forecast Gold Returns,” 2017)
Talagala et al. (“FFORMPP: Feature-based forecast model performance prediction,” 2021)
Vaiciukynas et al. (“Two-Step Meta-Learning for Time-Series Forecasting Ensemble,” 2022)
Viswanathan and Stephen (“Does Machine Learning Algorithms Improve Forecasting Accuracy? Predicting
Stock Market Index Using Ensemble Model,” 2020)
Zang (“Deep Learning in Multiple Multistep Time Series Prediction,” 2017)</p>
<p><strong>2.2.8 Probabilistic forecasting of time series</strong></p>
<p>List of references:
Bjerregård et al. (“An introduction to multivariate probabilistic forecast evaluation,” 2021)
Bouallegue et al. (“The diagonal score: Definition, properties, and interpretations,” 2018)
Deshpande and Sarawagi (“Long Range Probabilistic Forecasting in Time-Series using High Order Statistics,”
2021)
Gonzalez-Rivera et al. (“Prediction regions for interval-valued time series,” 2020)
Graziani et al. (“Probabilistic recalibration of forecasts,” 2021)
Greenberg (“Calibration Scoring Rules for Practical Prediction Training,” 2020)
Jordan et al. (“Evaluating probabilistic forecasts with scoringRules,” 2019)
Le Guen and Thome (“Probabilistic Time Series Forecasting with Structured Shape and Temporal Diversity,”
2020)
Lerch et al. (“Forecaster’s Dilemma: Extreme Events and Forecast Evaluation,” 2017)
Lerch et al. (“Forecaster’s Dilemma: Extreme Events and Forecast Evaluation,” 2015)
Kamarthi et al. (“CAMul: Calibrated and Accurate Multi-view Time-Series Forecasting,” 2021)
Kang et al. (“Assessing Goodness of Fit for Verifying Probabilistic Forecasts,” 2021)
Martin et al. (“Optimal probabilistic forecasts: When do they work?” 2020)
Prayogo et al. (“Time Series Sampling for Probabilistic Forecasting,” 2020)
Taylor and Taylor (“Combining probabilistic forecasts of COVID-19 mortality in the United States,” 2021)</p>
<p><strong>2.2.9 Metrics to assess forecast performance</strong></p>
<p>List of references:
Alexander et al. (“Evaluating the Discrimination Ability of Proper Multivariate Scoring Rules,” 2021)
Botchkarev (“A new typology design of performance metrics to measure errors in machine learning regression
algorithms,” 2019)
Cheng et al. (“Forecast Evaluation,” 2019)
Chiu et al. (“A new approach for detecting shifts in forecast accuracy,” 2019)
Gasthaus et al. (“Probabilistic Forecasting with Spline Quantile Function RNNs,” 2019)
Hyndman and Athanasopoulos ( <em>Forecasting: Principles and Practice (Third Edition)</em> , 2020)
Makridakis et al. (“The M4 Competition: 100,000 time series and 61 forecasting methods,” 2019)
Makridakis and Petropoulos (“The M4 competition: Conclusions,” 2020)
Neto et al. (“Uncovering regimes in out of sample forecast errors,” 2021)
Petropoulos et al. (“The inventory performance of forecasting methods: Evidence from the M3 competition
data,” 2019)
Ryll and Seidens (“Evaluating the Performance of Machine Learning Algorithms in Financial Market Forecasting:
A Comprehensive Survey,” 2019)
Samuels and Sekkel (“Model Confidence Sets and forecast combination,” 2017)
Thorarinsdottir (“Forecast evaluation,” 2021)</p>
<p><strong>2.2.10 Software implementations and frameworks</strong></p>
<p>List of references:
Alexandrov et al. (“GluonTS: Probabilistic and Neural Time Series Modeling in Python,” 2020)
Bacher et al. (“onlineforecast: An R package for adaptive and recursive forecasting,” 2022)
Bernardi and Catania (“The Model Confidence Set package for R,” 2014)
Bhatnagar et al. (“Merlion: A Machine Learning Library for Time Series,” 2021)
Bokde et al. (“ForecastTB - An R Package as a Test-Bench for Time Series Forecasting, with Application of
Wind Speed and Solar Radiation Modeling,” 2020)
Burns and Whyne (“Seglearn: A Python Package for Learning Sequences and Time Series,” 2018)
Charte et al. (“predtoolsTS: R package for streamlining time series forecasting,” 2019)
de Valk et al. (“Nowcasting: An R Package for Predicting Economic Variables Using Dynamic Factor Models,”
2019)
Golyandina et al. ( <em>Singular Spectrum Analysis with R</em> , 2018)
Herzen et al. (“Darts: User-Friendly Modern Machine Learning for Time Series,” 2022)
Hyndman (“Tidy Time Series and Forecasting in R,” 2020)
Jordan et al. (“Evaluating probabilistic forecasts with scoringRules,” 2019)
Leroy et al. (“MAGMA: Inference and Prediction with Multi-Task Gaussian Processes,” 2020)
Loning et al. (“sktime: A Unified Interface for Machine Learning with Time Series,” 2019)
Qian et al. (“Combining forecasts for universally optimal performance,” 2022)
Salles et al. (“TSPred: A framework for nonstationary time series prediction,” 2022)
Seca (“TimeGym: Debugging for Time Series Modeling in Python,” 2021)
Shaub (“Fast and accurate yearly time series forecasting with forecast combinations,” 2020)
Siebert et al. (“A systematic review of Python packages for time series analysis,” 2021)</p>
<h2>3 Practical details for the project</h2>
<p>The main purpose of the project described in this document is to provide exposure to students on important (and
interesting) practical topics in quantitative wealth and investment management QWIM.
The level of complexity depends on the number of hours designated for the project. For example, 50-60 hours for
a regular project, and 100-120 hours for a thesis/capstone project. Upon request, the scope (and the corresponding
number if hours) of any given project can be extended.
The students would work on the project as part of a team (usually with 2-3 students).
All QWIM projects were selected such that the students’ efforts have a good chance of producing results relevant
to the industry, and at least as good as the results presented in the QWIM literature. Thus for each project we
may consider (on an optional basis, based primarily on students’ preference) to submit a corresponding article to
journals widely followed by practitioners and academics in investment and wealth management, with participating
students included as the leading coauthors of the submitted article.
The main challenge for each project is to identify the criteria for what would be considered <strong>“good enough</strong> ”.
Similar to projects in the industry, the meaning of “good enough” is based on a combination of comprehensive
literature review, discussions within team and with me (and/or my colleagues) and analysis of results. Emphasis is
placed on creating a narrative (with the aid of an interactive visualizer) for convincing the intended audience that
what was done in the project delivers <strong>“good enough</strong> ” outcome.</p>
<h3>3.1 Interaction with students</h3>
<p>For each project I would make myself available for meetings on a weekly basis (for discussions and guidance).
Some of my colleagues have also expressed interest to participate in such meetings. Due to our work schedule and
deliverables, most of the discussions will have to be scheduled outisde working hours (in weekends or evenings).
The meetings will take place through video conferencing such as WebEx, Zoom, Google Meet, Microsoft Teams,
etc., based on the team’s preference. If the meetings are through WebEx, I would provide a link, while the student
team will provide a link for any other video conferencing tool.
The students working on a given project can also send questions by email (my recommendation is to aggregate
the questions from team members into an email sent once a day). We aim to provide answers within 1-2 days, either
by email or through a phone discussion.</p>
<h3>3.2 Data.</h3>
<p>Due to compliance reasons all projects would be based on publicly available, non-proprietary and non-confidential
data (indices, ETFs, mutual funds, etc.). Since neither I nor my team are allowed to provide these datasets, I can
only provide a list of suggested datasets. This list is included in a later section named Practical Info.
The datasets were selected to have the following features:</p>
<ul>
<li>be good proxies for most representative asset and subasset classes</li>
<li>to be widely available</li>
<li>to be as liquid as possible</li>
<li>to have daily granularity</li>
<li>to encompass periods with as many market regimes as possibles (most proposed daily datasets are from 1990
    or 1991)</li>
<li>time series have “nicer” statistical properties compared to time series of, say, individual stocks or bonds</li>
</ul>
<h3>3.3 Private GitHub repository for the QWIM project.</h3>
<p>The team will create a private GitHub repository, which will store relevant project materials, including codes. The
team will use Git Desktop application as source control repository linked to the GitHub repository.</p>
<h3>3.4 Deliverables.</h3>
<p>The project deliverables include literature survey, numerical results, analysis and visualization. For each project
references will be provided for a comprehensive literature survey, and students are encouraged to identify additional
relevant literature. Regarding the implementation, the project will primarily use existing codes:</p>
<ul>
<li>Python and R packages from official repositories (PyPi for Python and CRAN for R)</li>
<li>machine learning platforms such as TensorFlow, PyTorch, CNTK, Chainer, mlr3, H20, PlaidML, mlpack, etc.</li>
<li>implementations of articles through codes available in repositories such as GitHub, BitBucket, GitLab, etc.</li>
</ul>
<p>Visualization of data and results visualization will be interactive and it will be based on Shiny R framework; to
reduce programming effort, a template for such a Shiny visualizer will be provided in the team private GitHub
repository.
The deliverables are:</p>
<ul>
<li>written report including literature survey and numerical results</li>
<li>interactive visualizer (most likely Shiny-based visualizer using R and Python packages)</li>
<li>(optional) presentation slides, and/or RMarkdown presentation, and/or Jupyter Notebook(s)</li>
</ul>
<h3>3.5 (Optional) Article submission to leading journals</h3>
<p>On an optional basis (based primarily on students’ preference), a version of the report can be prepared for submision
to leading journals such as Journal of Financial Data Science, Journal of Portfolio Management, Journal of Asset
Management, Journal of Investment Strategies, Quantitative Finance, Journal of Wealth Management, Journal of
Investing, Journal of Machine Learning in Finance, etc.</p>
<h2>4 Project tasks and timelines</h2>
<p>For each project the main tasks are:</p>
<p><code>1) literature review</code>
<code>2) decide on the appropriate metrics and quantitative methods within context of "good enough" for the project</code>
<code>3) write-up summary of literature review: methods, metrics, testing procedures</code>
<code>4) identification of Python and/or R packages which are most appropriate for the selected methods and metrics</code>
<code>5) code design to decide on main code components</code>
<code>6) implementation of code components</code>
<code>7) interactive visualization of numerical results</code>
<code>8) project report containing description of methods, metrics, and tests, and analysis of results.</code></p>
<h3>4.1 Suggested timelines for project tasks</h3>
<p>The table below suggests a timeline for the project tasks and the corresponding percentages of project time:</p>
<p><code>Table 1: Suggested timeline for project tasks
Task ID Task Name Percentage of project time
1 Literature review 15%
2 Identification of "good enough" metrics and quantitative methods 5%
3 Write-up of summary of literature review 5%
4 Identification of appropriate packages in Python and/or R 10%
5 Code design for main components of project coding framework 5%
6 Implementation of coding framework and components 40%
7 Interactive visualizer using the provided Shiny template 10%
8 Project report and presentation 10%</code></p>
<h3>4.2 Literature review.</h3>
<p>The first task is based on a comprehensive literature survey, included in the preliminary document of the project.
Students are encouraged to identify additional relevant literature.
This task may be the most important of the project, since it provides an overview of what was done, what works
well and less well, and what appear to be the most promising avenues to complete the project.
Emphasis is placed on information contained in the Main References; the other References would be considered
only if time permits and the team is interested in exploring other avenues.
When reading the literature, there are 4 main directions to consider:</p>
<p><code>1) methods</code>
<code>2) metrics to assess the performance/robustness of the methods</code>
<code>3) testing procedures</code>
<code>4) numerical results</code>
The primary focus would be on the the references included in "Main References" subsection of the document for your
QWIM project. Then, to the extent there is time, to consider the other references included in the project document.
In the same time, you are encouraged to identify other references that might be considered "Main references", and
to share those references with me for discussion.
For the articles in Main References category, the suggested approach would be the following:</p>
<ul>
<li>For each article focus primarily on Abstract, Conclusion, and Numerical Results</li>
<li>Do this for all articles considered to be Main References, such that you gain a high-level understanding of
    what is currently done in the literature</li>
<li>Select the metrics that you may want to use in order to quantify the meaning of "good enough" for the project.</li>
<li>Select the quantitative methods which appear to be most likely to be "good enough" for the project.</li>
<li>Perform a "deeper dive" into the articles containing the approaches you consider the most promising,</li>
</ul>
<p>For the articles which are not in "Main References" category, read Abstract, Conclusion, and Numerical Results, to
see whether any of those articles might need to be considered for inclusion in your summary.</p>
<h3>4.3 Write-up summary of literature review.</h3>
<p>The write-up summary summarizes the methods, metrics, testing procedures, and numerical results identified during
the literature review. The write-up could also be incorporated within reports and/or presentations for the QWIM
project.</p>
<h3>4.4 Identification of appropriate Python and/or R packages</h3>
<p>Based on the literature review and on diiscussions, we identify the most potentially useful methods, metrics and
testing procedures. Then wee identify the most appropriate implementations of the selected methods and metrics.
The primary sources of implementatins are existing codes from:</p>
<ul>
<li>Python and R packages from official repositories (PyPi for Python and CRAN for R)</li>
<li>machine learning platforms such as TensorFlow, PyTorch, CNTK, Chainer, mlr3, H20, PlaidML, mlpack, etc.</li>
<li>Codes available in repositories such as GitHub, BitBucket, GitLab, etc.</li>
</ul>
<h3>4.5 Code design.</h3>
<p>An important task is to have a code design session to decide in advance on the main code components, which are
meant to be modular and encapsulated, such that the entire team can work on the codes.
Examples of such main components include extracting data, calculate metrics for the considered procedures,
portfolio metrics, performing tests, construct interactive visualizer, etc.
The code design procedure consists of:</p>
<p><code>1) visual display of major components of the coding framework</code>
<code>2) UML diagrams for each of the components.</code>
The Appendix contains an illustrative example within context of a QWIM project on forecasting of financial
time series. The first figure shows the major components, while the second figure shows UML diagrams of those
components (the names of data members and methods are currently generic, and one would need to change them
to appropriate names)
While these figures were obtained through Microsoft Visio using a code design file (.vsd file), there are other
software tools (either online or installed locally) which can be used to create such code design diagrams. NOTE: if
you have access to Microsoft Visio and you want to use it for code design diagrams, you can ask me for the .vsd
file which was exported into the PDF from which I have extracted the snapshots.
List of software tools for code design diagrams, which are either free (open source) or have a free type of account</p>
<ul>
<li>Modelio (eitherdesktopversion oronlineversion)</li>
<li>LucidChart (online)</li>
<li>
<p>draw.io (eitherdesktopversion oronlineversion, now called app.diagrams.net)</p>
</li>
<li>
<p>Visual Paradigm (online)</p>
</li>
<li>UMLet (eitherdesktopor onlineversion)</li>
<li>Curated list of UML tools – 2019 edition</li>
<li>Top online UML modeling tools in 2019</li>
</ul>
<h3>4.6 Implementation of coding framework and components</h3>
<p>The implementation is done using identified packages or codes, in Python and/or R. The project will primarily use
existing codes:</p>
<ul>
<li>Python and R packages from official repositories (PyPi for Python and CRAN for R)</li>
<li>machine learning platforms such as TensorFlow, PyTorch, CNTK, Chainer, mlr3, H20, PlaidML, mlpack, etc.</li>
<li>implementations of articles through codes available in repositories such as GitHub, BitBucket, GitLab, etc.</li>
</ul>
<h3>4.7 Interactive visualizer.</h3>
<p>While visualization of data and numerical results can be done through various tools (including Jupyter notebooks
or Dash in Python), my recommendation is to consider an interactive visualizer based on Shiny framework in R.
A template for the Shiny visualizer will be provided in the private GitHub repository set up by the team for the
project.
Some information about Shiny:</p>
<ul>
<li>Shiny from RStudio: tutorialsand gallery</li>
<li>Why R Shiny Trumps UI and JavaScript Based Visualization Tools</li>
<li>Shiny’s Holy Grail: Interactivity with reproducibility</li>
</ul>
<h3>4.8 Project report and presentation.</h3>
<p>The report containing description of methods, metrics, and tests, and analysis of results.
While the report can be written using various tools (including Microsoft Word), my recommendation is to use
LyX to write both the project report and the project presentation. Two LyX templates for creating reports and,
respectively, presentations will be provided in the private GitHub repository set up by the team for the project.
Some information about Shiny:</p>
<ul>
<li>LyX features</li>
<li>LyX tutorialwith PDFhere</li>
<li>LyX Tutorial videoPart Oneand Part Two</li>
<li>LyX tutorial videoPart Oneand Part Twoand Part Threeand Part Four</li>
<li>Introduction to LyX</li>
<li>Insert figures in LyX</li>
<li>Essentials of LyX</li>
</ul>
<h2>5 Design and implementation for the project codes</h2>
<p>This section describes a possible approach for the design process and for the implementation (folder structure) of
the project. This approach is presented only to exemplify how it could be done. Each student team has freedom to
consider their own design process.
Design and implementation would be based on following principles:</p>
<ul>
<li>coding framework is Python based, with calls to functions available in existing Python and R packages</li>
<li>leverage common components (such as data input/output, numerical methods, time series, testing, interactive
    visualization and reporting, etc.)</li>
<li>reusability</li>
<li>incorporate best practices in coding and numerical implementations</li>
<li>use, augment and enhance (to largest extent possible) existing Python and R packages and codes</li>
</ul>
<h3>5.1 Visualize project workflow and coding framework.</h3>
<p>The starting point is to visualize the project workflow in terms of major components, and then to design the code
framework.
The code design procedure consists of:</p>
<p><code>1) visual display of major components of the coding framework</code>
<code>2) UML diagrams for each of the components.</code>
We present examples below for projects including time series forecasting and analysis, machine learning for portfolio
construction, etc.</p>
<p><code>Figure 1: Examples of architecture of coding framework: AlphaPy</code>
Source:AlphaPy</p>
<p><code>Figure 2: Examples of architecture of coding framework: Greykite</code>
Source:Geykite</p>
<p><code>Figure 3: QLib Framework</code>
<code>Trading Agent</code>
<code>Meta Controller</code>
<code>Analyser</code>
<code>Decision</code>
<code>Forecast Model</code>
<code>Interface</code>
<code>Multi-level Workflow</code>
<code>Infrastracture</code>
<code>Forecasting...Portfolio A...Execution...</code>
<code>Information Extractor</code>
<code>Online Serving</code>
<code>Graph Event</code>
<code>Factor Text
Alpha Risk</code>
<code>Data Server</code>
<code>local remote</code>
<code>Trainer</code>
<code>Algorithms Auto-ML</code>
<code>Model Manager</code>
<code>ModelModel
Models
Decision GeneratorsModelModel</code>
<code>Model Interpreter</code>
<code>Decision Generator</code>
<code>Order executi...</code>
<code>Execution Results</code>
<code>Execution Env</code>
<code>VWAP/Close/......Sub-workfl...</code>
<code>Highly Customiz...</code>
<code>Module in devel...</code>
<code>Explanation</code>
<code>Sub-workflow(1) (E.g. High-frequ...</code>
<code>Execution E...
...</code>
<code>(1) The sub-workflow will make more fine-grained decisions according to the decision from the upper-level trading agent</code>
<code>Asset allocat...Stock selecti...</code>
<code>Trading...</code>
<code>Viewer does not support full SVG 1.1</code></p>
<p>Figure 4: Examples of major components of coding framework (top) and UML diagrams (bottom)</p>
<p><code>Figure 5: Financial Machine Learning in Portfolio Construction</code>
Source:Machine Learning in Asset Management</p>
<h5>28</h5>
<h3>5.2 Representative examples of Python libraries with well designed folder structure</h3>
<p>List of Python libraries</p>
<ul>
<li>QLib is a AI-oriented quantitative investment platform in Python developed by Microsoft researchers</li>
<li>GluonTSis a Python library deveoped by Amazon researchers for probabilistic time series modeling</li>
<li>sktimeis a unified framework for machine learning with time series, developed by researchers at Alan Turing
    Institute for data science and artificial intelligence.</li>
<li>darts is a Python library for easy manipulation and forecasting of time series, developed by researchers at
    Unit8 AI and data analytics company.</li>
<li>Kats is a Python library developed by Facebook researchers to analyze time series data.</li>
<li>Kats is a Python library developed by Tinkoff AI researchers to analyze time series data.</li>
<li>MLFinLab(Machine Learning Financial Laboratory) is a Python library developed by researchers at Hudson
    &amp; Thames.</li>
</ul>
<h2>6 Practical Info</h2>
<h3>6.1 Recommended software tools</h3>
<p>The sections below describe the recommended software tools, including corresponding versions/subversions, tutorials
and details</p>
<h4>6.1.1 Python</h4>
<p>The recommended versions are:</p>
<ul>
<li>Python version 3.8 (subversion Python 3.8.10 or 3.8.15)</li>
<li>Python version 3.9 (subversion Python 3.9.10 or 3.9.15)</li>
<li>Python version 3.10 (latest subversion, currently Python 3.10.8)</li>
</ul>
<p>There are also relevant Python packages, identified while you are working the project. As a starting point you can
consider the packages included in section on Potentially useful Python and R packages.</p>
<h4>6.1.2 R.</h4>
<p>The recommended versions are:</p>
<ul>
<li>R version 4.2 (recommended is latest subversion, currently R 4.2.2)</li>
<li>R version 3.6 (subversion R 3.6.3)</li>
</ul>
<p>On Windows computers you also need to installRtoolsto build R packages from source through compilation, since
not all packages have associated Windows binaries.
There are also relevant R packages, identified while you are working the project. As a starting point you can
consider the packages included in section on Potentially useful Python and R packages.</p>
<h4>6.1.3 R IDE.</h4>
<p>The recommended R IDE is RStudio Desktop Open Source</p>
<ul>
<li>latest version, currently 2022.07.2+576</li>
</ul>
<h4>6.1.4 Python IDE.</h4>
<p>The recommended Python IDE is Visual Studio Code VSC</p>
<ul>
<li>latest version, currently VSC 1.73</li>
</ul>
<p>Then add Python extension and other Visual Studio Code extensions from Visual Studio MarketPlace.
Note: Upon request I can provide a list of potentially useful VSC extensions, which can be installed on your
computer (see for examplelink)</p>
<h4>6.1.5 Bibliography Manager</h4>
<p>The recomemnded bibliography manager isJabRef</p>
<ul>
<li>latest version: version 5.7, or</li>
<li>latest development version fromlink</li>
</ul>
<p>I can provide you with a bibliography file which contains all refeernces mentioned in the project description, This
file (of extension bib) can be viewed and edited with JabRef, and used together with LyX to write your project
related documents (report, presentation, etc.).
You can easily add/delete/edit this bib file using JabRef.
There are video tutorials on using JabRef:link 1 , link 2 , link 3.
In addition to these video tutorials, I can also have a video online session, to provide an overview and answer
your questions on using LyX and JabRef. This online session (through Google Meet Google Meet) can be recorded
and shared with you afterwards.</p>
<h4>6.1.6 Document processor</h4>
<p>The recommended document processor isLyX, which is a document processor that encourages an approach to
writing based on the structure of your documents (WYSIWYM) and not simply their appearance (WYSIWYG).
LyX combines the power and flexibility of TeX/LaTeX with the ease of use of a graphical interface. It shoudl
be emohasized that you do not need to know/learn LaTeX in order to tuse LyX.
To install LyX, you need to download and install first TeXLive (seelink), which is a packaged distribution of
LaTeX and associated packages
Then install LyX usinginstallers, making sure that you are pointing to location of installed TeXLive when asked
for a LaTeX distribution during the run of LyX installer.
Recommended versions:</p>
<ul>
<li>TexLive (recommended is latest version, currently TeXLive 2022)</li>
<li>LyX (recommended is latest subversion, currently LyX 2.3.6.1)</li>
</ul>
<p>There are video tutorials (link 1and link 2 ).
In addition to these video tutorials, I can also have a video online session, to provide an overview and answer
your questions on using LyX and JabRef. This online session (through Google Meet Google Meet) can be recorded
and shared with you afterwards.</p>
<h4>6.1.7 Source control manager</h4>
<p>The recommended source control manager isGitHub desktop, which can be used in conjunction with thr private
GitHub repository that each student team will create for their project</p>
<ul>
<li>latest subversion, currently GitHub Desktop 3.1.2</li>
</ul>
<h4>6.1.8 File editor.</h4>
<p>The recommended file editor isNotepad++</p>
<ul>
<li>latest subversion (currently Notepad++ 8.4.7) with various plugins (see list of available plugins atlink 1and
    link 2 )</li>
</ul>
<h4>6.1.9 Runtime libraries.</h4>
<p>Many Python and R packages require runtime libraries such asMicrosoft Visual C++ Redistributable</p>
<ul>
<li>latest version, currently Microsoft Visual C++ Redistributable 64-bit for Visual Studio 2015, 2017, 2019, and
    2022</li>
</ul>
<h3>6.2 Recommended datasets</h3>
<p>The datasets below were selected to have the following features:</p>
<ul>
<li>to be representative proxies for most relevant asset and subasset classes</li>
<li>
<p>to be widely available</p>
</li>
<li>
<p>to be as liquid as possible</p>
</li>
<li>to have daily granularity</li>
<li>to encompass time periods containing as many market regimes as possibles (under this consideration, the
    recommended daily datasets start from early 1990s)</li>
<li>to have “nicer” statistical properties, which will make modeling easier (under this consideration, time series of
    recommended financial indices have “nicer” statistical properties compared to time series of individual stocks
    or bonds)</li>
</ul>
<p>The following datasets are suggested</p>
<p><code>Table 2: Daily data sets
Name Description Name Description
BCOMTR Bloomberg Commodity Index Total
Return</code>
<code>RU20VATR iShares Russell 2000 Value ETF</code>
<code>HFRIFWI HFRI Fund Weighted Composite Index RUMCINTR iShares Russell Mid-Cap ETF
LBUSTRUU Bloomberg Barclays US Aggregate Bond
Index</code>
<code>RUMRINTR iShares Micro-Cap ETF</code>
<code>LG30TRUU Bloomberg Barclays Global High Yield
Total Return Index Value Unhedge</code>
<code>RUTPINTR iShares Russell Top 200 ETF</code>
<code>LMBITR Bloomberg Barclays Municipal Bond
Index Total Return Index Value
Unhedged USD</code>
<code>S5COND S&amp;P 500 Consumer Discretionary Index</code>
<code>NDDUE15X Amundi MSCI Europe Ex UK Ucits ETF
Dr</code>
<code>S5CONS S&amp;P 500 Consumer Staples Index</code>
<code>NDDUJN MSCI Japan Index S5ENRS S&amp;P 500 Energy Index
NDDUNA iShares MSCI North America UCITS
ETF</code>
<code>S5FINL S&amp;P 500 Financials Sector GICS Level 1
Index
NDDUPXJ MSCI Pacific ex Japan UCITS ETF S5HLTH S&amp;P 500 Health Care Index
NDDUUK iShares MSCI UK ETF S5INDU S&amp;P 500 Industrials Index
NDDUWXUS MSCI World ex USA total net return S5INFT S&amp;P 500 Information Technology Index
NDUEEGF SPDR MSCI Emerging Markets UCITS
ETF</code>
<code>S5MATR S&amp;P 500 Materials Index</code>
<code>RU10GRTR iShares Russell 1000 Growth ETF S5RLST S&amp;P 500 Real Estate Index
RU10VATR  iShares Russell 1000 Value ETF S5TELS S&amp;P 500 Communication Services Index
RU20GRTR  iShares Russell 2000 Growth ETF S5UTIL S&amp;P 500 Utilities Index
RU20INTR Russell 2000 Total Return SPXT Proshares S&amp;P 500 EX Technology ETF</code></p>
<p>Table 3: Monthly data sets
Name Description Name Description</p>
<p>IBXXSHY1 iShares 0-5 Year High Yield Corporate
Bond ETF</p>
<p><code>M2USEV MSCI USA Enhanced Value Index</code>
IDCT20RT ICE U.S. Treasury 20+ Year Bond Total
Return Index</p>
<p><code>M2USRWGT MSCI USA Risk Weighted Index</code>
LBUSTRUU Bloomberg Barclays US Agg Total Return
Value Unhedged USD</p>
<p><code>M2USSNQ MSCI USA Sector Neutral Quality Index</code>
LC07TRUU Bloomberg Barclays U.S. Universal Total
Return Index Value Unhedged</p>
<p><code>MID S&amp;P 400 Mid Cap Index index</code>
LD01TRUU Bloomberg Barclays 1-3 Yr Credit Total
Return Index Value Unhedged US</p>
<p><code>MXEA MSCI EAFE Index</code>
LT01TRUU Bloomberg Barclays US Treasury 1-3
Year Index</p>
<p><code>MXEF MSCI Emerging Markets Index</code>
LUICTRUU Bloomberg Barclays U.S. Intermediate
Credit Total Return Index</p>
<p><code>MXUSMVOL MSCI USA Minimum Volatility Index</code>
LULCTRUU Bloomberg Barclays U.S. Long Credit
Index</p>
<p><code>MXWD MSCI All Countries World Index</code>
M1CXBRU iShares Core MSCI International
Developed Markets ETF</p>
<p><code>MXWOUIM MSCI All Countries World Index</code>
M1USMVOL MSCI USA Minimum Volatility (USD)
Index</p>
<p><code>NDDUUS MSCI Daily Total Return Net USA USD
Index</code>
M2US000$ iShares Edge MSCI USA Momentum
Factor ETF</p>
<p><code>SPX S&amp;P 500 Index</code></p>
<h2>7 Potentially useful Python and R software implementations: packages, codes and frameworks</h2>
<h2>ages, codes and frameworks</h2>
<h3>7.1 Collections and repositories of resources</h3>
<p><strong>For Data Science, Numerical Methods/ Algorithms, Programming</strong></p>
<p>List of links:</p>
<ul>
<li>Data Science CheatSheet</li>
<li>professional-programming: collection of full-stack resources for programmers.</li>
</ul>
<p><strong>For Python</strong></p>
<p>List of links:</p>
<ul>
<li>Awesome Python</li>
<li>Awesome Python frameworks, libraries, software and resources</li>
<li>Best of Python</li>
<li>Curated list of Python frameworks, libraries, software and resources</li>
<li>Pythonidae: Curated decibans of scientific programming resources in Python</li>
<li>Ranked list of Python open-source Machine Learning libraries and tools</li>
<li>Ranked list of Python open-source libraries and tools</li>
<li>Ranked list of Python developer tools and libraries</li>
<li>Time series: analytics, statistics, machine learning, frameworks and databases</li>
<li>Time series Python packages</li>
</ul>
<p><strong>For R</strong></p>
<p>List of links:</p>
<ul>
<li>Available CRAN Packages By Date of Publication</li>
<li>CRAN Task Views</li>
</ul>
<h3>7.2 Connection between Python and R codes</h3>
<p>List of links:</p>
<ul>
<li>arrow: R interface to ’Apache’ ’Arrow’, a cross-language for accelerated data interchange in-memory data</li>
<li>pyarrow: Python library for Apache Arrow</li>
<li>reticulate: R Interface to ’Python’ modules, classes, and functions</li>
<li>rpy2: Python interface to the R language</li>
<li>rpy2-arrow: Share Apache Arrow datasets between Python and R</li>
<li>R Extension for Visual Studio Code</li>
</ul>
<h3>7.3 Anomaly detection and data outliers</h3>
<p><strong>Collections of resources</strong></p>
<p>List of links:</p>
<ul>
<li>Anomaly detection related books, papers, videos, and toolboxes</li>
</ul>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>adtk: Python toolkit for rule-based/unsupervised anomaly detection in time series</li>
<li>Anomaly Detection Learning Resources</li>
<li>Awesome anomaly detection resources</li>
<li>Curve: time series data anomaly detection by Baidu</li>
<li>kats: kit to analyze time series data by Facebook</li>
<li>luminaire: ML driven package by Zillow for monitoring time series data</li>
<li>Merlion: A Machine Learning Framework for Time Series Intelligence by SalesForce</li>
<li>PyGOD: Graph Outlier Detection (Anomaly Detection)</li>
<li>PyOD: Python Toolbox for Scalable Outlier Detection (Anomaly Detection)</li>
<li>PyODDS: An End-to-end Outlier Detection System</li>
<li>ruptures: change point detection in Python</li>
<li>seriesdistancematrix: implements the Series Distance Matrix framework, a flexible component-based frame-
    work that bundles various Matrix Profile related techniques</li>
<li>Software tools and datasets for anomaly detection on time series data</li>
<li>Tools and datasets for anomaly detection on time-series data.</li>
<li>tsad: Time Series Forecasting and Anomaly Detection</li>
<li>TODS: An Automated Time-series Outlier Detection System</li>
<li>tsmoothie: time-series smoothing and outlier detection</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>amelie: Anomaly Detection with Normal Probability Functions</li>
<li>ANN2: Artificial Neural Networks for Anomaly Detection</li>
<li>anomaly: Detecting Anomalies in Data</li>
<li>AnomalyDetection: package by Twitter to detect anomalies</li>
<li>anomalize: Tidy Anomaly Detection</li>
<li>composits: Compositional, Multivariate and Univariate Time Series Outlier Ensemble</li>
<li>
<p>dobin: Dimension Reduction for Outlier Detection</p>
</li>
<li>
<p>dsos: Dataset Shift with Outlier Scores</p>
</li>
<li>HDoutliers: Leland Wilkinson’s Algorithm for Detecting Multidimensional Outliers</li>
<li>isotree: Isolation-Based Outlier Detection</li>
<li>kssa: automatically identify and validate the best method for missing data imputation in a time series</li>
<li>lookout: Leave One Out Kernel Density Estimates for Outlier Detection</li>
<li>mvoutlier: Multivariate Outlier Detection Based on Robust Methods</li>
<li>odetector: Outlier Detection Using Partitioning Clustering Algorithms</li>
<li>otsad: Online Time Series Anomaly Detectors</li>
<li>outForest: Multivariate Outlier Detection and Replacement</li>
<li>outliers: Tests for Outliers</li>
<li>outliertree: Explainable Outlier Detection Through Decision Tree Conditioning</li>
<li>stray: Anomaly Detection in High Dimensional and Temporal Data</li>
<li>TagAnomaly: Anomaly detection analysis and labeling tool by Microsoft</li>
<li>trendsegmentR: Linear Trend Segmentation and Point Anomaly Detection</li>
<li>tsoutliers: Detection of Outliers in Time Series</li>
<li>univOutl: Detection of Univariate Outliers</li>
</ul>
<h3>7.4 Bayesian analysis and modeling.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ArviZ: Exploratory analysis of Bayesian models with Python</li>
<li>baal: enable Bayesian active learning in your research or labeling work</li>
<li>bambi: BAyesian Model-Building Interface (Bambi)</li>
<li>bilby: Bayesian inference library</li>
<li>BayesianOptimization: implementation of global optimization with gaussian processes</li>
<li>BayesTSA: ayesian methods for solving estimation and forecasting problems in time series analysis</li>
<li>BoTorch: Bayesian optimization in PyTorch</li>
<li>Bumps: data fitting and uncertainty estimation</li>
<li>nutpie: A fast sampler for bayesian posteriors</li>
<li>Orbit: Bayesian forecasting package by Uber</li>
<li>PyApprox: high-dimensional approximation and uncertainty quantification</li>
<li>pyMC: Bayesian Modeling and Probabilistic Machine Learning with Aesara</li>
<li>PyStan: Python interface to Stan, a platform for statistical modeling</li>
<li>zeus: Lightning Fast MCMC</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ashr: Methods for Adaptive Shrinkage, using Empirical Bayes</li>
<li>bain: Bayes Factors for Informative Hypotheses (equality, inequality, and about equality constrained hypothe-
    ses)</li>
<li>bamp: Bayesian Age-Period-Cohort Modeling and Prediction</li>
<li>bsamGP: Bayesian Spectral Analysis Models using Gaussian Process Priors</li>
<li>bayesdfa: Bayesian Dynamic Factor Analysis (DFA) with ’Stan’</li>
<li>bayefdr: Bayesian Estimation and Optimisation of Expected False Discovery Rate</li>
<li>BayesFM: Bayesian Inference for Factor Modeling</li>
<li>bayesforecast: Bayesian Time Series Modeling with Stan</li>
<li>BayesHMM: Full Bayesian Inference for Hidden Markov Models</li>
<li>bayesian: Bindings for Bayesian TidyModels</li>
<li>bayesmodels: The ’Tidymodels’ Extension for Bayesian Models</li>
<li>bayesplot: Plotting for Bayesian Models</li>
<li>BayesPostEst: Generate Postestimation Quantities for Bayesian MCMC Estimation</li>
<li>bayestestR: Understand and Describe Bayesian Models and Posterior Distributions</li>
<li>BayesTools: Tools for Bayesian Analyses</li>
<li>BayesVarSel: Bayes Factors, Model Choice and Variable Selection in Linear Models</li>
<li>BEST: Bayesian Estimation Supersedes the t-Test</li>
<li>beyondWhittle: Bayesian Spectral Inference for Stationary Time Series</li>
<li>BFpack: Flexible Bayes Factor Testing of Scientific Expectations</li>
<li>BMAmevt: Multivariate Extremes: Bayesian Estimation of the Spectral Measure</li>
<li>bmixture: Bayesian Estimation for Finite Mixture of Distributions</li>
<li>bnmonitor: An Implementation of Sensitivity Analysis in Bayesian Networks</li>
<li>BNPmix: Bayesian Nonparametric Mixture Models</li>
<li>bpcs: Bayesian Paired Comparison Analysis with Stan</li>
<li>bpgmm: Bayesian Model Selection Approach for Parsimonious Gaussian Mixture Models</li>
<li>brms: Bayesian Regression Models using ’Stan’</li>
<li>BSL: Bayesian Synthetic Likelihood</li>
<li>bspec: Bayesian Spectral Inference</li>
<li>bsvars: Bayesian Estimation of Structural Vector Autoregressive Models</li>
<li>dalmatian: Automating the Fitting of Double Linear Mixed Models in ’JAGS’ and ’nimble’</li>
<li>
<p>dbnR: Dynamic Bayesian Network Learning and Inference</p>
</li>
<li>
<p>DEBBI: Differential Evolution-Based Bayesian Inference</p>
</li>
<li>ensembleBMA: Probabilistic Forecasting using Ensembles and Bayesian Model Averaging</li>
<li>fbst: The Full Bayesian Evidence Test, Full Bayesian Significance Test and the e-Value</li>
<li>greta: scalable statistical modelling in R</li>
<li>LaplacesDemon: Complete Environment for Bayesian Inference</li>
<li>mBvs: Bayesian Variable Selection Methods for Multivariate Data</li>
<li>mlr3mbo: Flexible Bayesian Optimization</li>
<li>mombf: Bayesian Model Selection and Averaging for Non-Local and Local Priors</li>
<li>networkABC: Network Reverse Engineering with Approximate Bayesian Computation</li>
<li>nimble: MCMC, Particle Filtering, and Programmable Hierarchical Modeling</li>
<li>Nmix: Bayesian Inference on Univariate Normal Mixtures</li>
<li>posterior: Tools for Working with Posterior Distributions</li>
<li>rBayesianOptimization: Bayesian Optimization of Hyperparameters</li>
<li>Rbeast: Bayesian Change-Point Detection and Time Series Decomposition</li>
<li>REBayes: Empirical Bayes Estimation and Inference</li>
<li>Revticulate: Interaction with ”RevBayes” in R</li>
<li>rstan: R Interface to Stan</li>
<li>rstanarm: Bayesian Applied Regression Modeling via Stan</li>
<li>SequenceSpikeSlab: Exact Bayesian Model Selection Methods for the Sparse Normal Sequence Model</li>
<li>shrinkTVP: Efficient Bayesian Inference for Time-Varying Parameter Models with Shrinkage</li>
<li>tidybayes: Tidy Data and ’Geoms’ for Bayesian Models</li>
</ul>
<h3>7.5 Causality, inference and dependencies</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>bilby: Bayesian inference library</li>
<li>CausalDiscoveryToolbox: causal inference in graphs and in the pairwise settings</li>
<li>causality: Tools for causal analysis</li>
<li>causalml: package by Uber for Uplift modeling and causal inference with machine learning algorithms</li>
<li>copulae: Multivariate data modelling with Copulas</li>
<li>DoWhy: library by Microsoft for causal inference that supports explicit modeling and testing of causal as-
    sumptions</li>
<li>HiDimStat: High-dimensional statistical inference tool</li>
<li>tigramite: time series analysis python module for causal discovery</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>causal.decomp: Causal Decomposition Analysis</li>
<li>CausalImpact: toolkit by Google to infer Causal Effects using Bayesian Structural Time-Series Models</li>
<li>causaloptim: An Interface to Specify Causal Graphs and Compute Bounds on Causal Effects</li>
<li>copula: Multivariate Dependence with Copulas</li>
<li>dCovTS: Distance Covariance and Correlation for Time Series Analysis</li>
<li>estimatr: Fast Estimators for Design-Based Inference</li>
<li>flipr: Flexible Inference via Permutations in R</li>
<li>generalCorr: Generalized Correlations, Causal Paths and Portfolio Selection</li>
<li>HellCor: The Hellinger Correlation</li>
<li>infer: Tidy Statistical Inference</li>
<li>jackstraw: Statistical Inference for Unsupervised Learning</li>
<li>konfound: Quantify the Robustness of Causal Inferences</li>
<li>mashr: Multivariate Adaptive Shrinkage</li>
<li>multivariance: Measuring Multivariate Dependence Using Distance Multivariance</li>
<li>NlinTS: Models for Non Linear Causality Detection in Time Series</li>
<li>NNS: Nonlinear nonparametric statistics using partial moments</li>
<li>pcalg: Methods for Graphical Models and Causal Inference</li>
<li>qmd: Quantification of Multivariate Dependence</li>
<li>rmcfs: The MCFS-ID Algorithm for Feature Selection and Interdependency Discovery</li>
<li>sherlock: package by Netflix for causal machine learning for segment discovery and analysis</li>
<li>SIHR: Statistical Inference in High Dimensional Regression</li>
<li>tlverse: One Stop to Targeted Learning in R</li>
<li>tscopula: Time Series Copula Models</li>
<li>VLTimeCausality: Variable-Lag Time Series Causality Inference Framework</li>
</ul>
<h3>7.6 Classification, Motifs, Neighbors, Wavelets, Transforms.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>abess: Fast Best-Subset Selection Library</li>
<li>catboost: Gradient Boosting on Decision Trees by Yandex</li>
<li>
<p>HiClass: hierarchical classification compatible with scikit-learn</p>
</li>
<li>
<p>LightGBM: fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART)
    framework by Microsoft</p>
</li>
<li>Local Cascade Ensemble (LCE) is a high-performing, scalable and user-friendly machine learning method for
    the general tasks of Classification and Regression</li>
<li>matrixprofile: time series data mining tasks, utilizing matrix profile algorithms</li>
<li>pyts: time series classification</li>
<li>scikit-learn: machine learning in Python</li>
<li>seriesdistancematrix: implements the Series Distance Matrix framework, a flexible component-based frame-
    work that bundles various Matrix Profile related techniques</li>
<li>sktime: unified framework for machine learning with time series</li>
<li>stumpy: modern time series analysis</li>
<li>tslearn: machine learning toolkit dedicated to time-series data</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>abess: Fast Best-Subset Selection Library</li>
<li>AUC: Threshold Independent Performance Measures for Probabilistic Classifiers</li>
<li>bcTSNE: Projected t-SNE for Batch Correction</li>
<li>biwavelet: Conduct Univariate and Bivariate Wavelet Analyses</li>
<li>caret: Classification and Regression Training</li>
<li>classmap: Visualizing Classification Results</li>
<li>classifly: Explore Classification Models in High Dimensions</li>
<li>ContaminatedMixt: Clustering and Classification with the Contaminated Normal</li>
<li>CORElearn: Classification, Regression and Feature Evaluation</li>
<li>cvms: Cross-Validation for Model Selection</li>
<li>ddalpha: Depth-Based Classification and Calculation of Data Depth</li>
<li>dtw: Dynamic Time Warping Algorithms</li>
<li>greed: Clustering and Model Selection with the Integrated Classification Likelihood</li>
<li>ipred: Improved Predictors</li>
<li>klaR: Classification and Visualization</li>
<li>matrixProfile: Matrix Profile</li>
<li>matrixprofiler: Matrix Profile for R</li>
<li>mclust: Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation</li>
<li>MixGHD: Model Based Clustering, Classification and Discriminant Analysis Using the Mixture of Generalized
    Hyperbolic Distributions</li>
<li>
<p>MixMatrix: Classification with Matrix Variate Normal and t Distributions</p>
</li>
<li>
<p>mixSPE: Mixtures of Power Exponential and Skew Power Exponential Distributions for Use in Model-Based
    Clustering and Classification</p>
</li>
<li>mixture: Mixture Models for Clustering and Classification</li>
<li>randomUniformForest: Random Uniform Forests for Classification, Regression and Unsupervised Learning</li>
<li>rbooster: AdaBoost Framework for Any Classifier</li>
<li>rebmix: Finite Mixture Modeling, Clustering &amp; Classification</li>
<li>regtools: Regression and Classification Tools</li>
<li>Rmixmod: Classification with Mixture Modelling</li>
<li>RSSL: Implementations of Semi-Supervised Learning Approaches for Classification</li>
<li>Rtsne: T-Distributed Stochastic Neighbor Embedding using a Barnes-Hut Implementation</li>
<li>sbfc: Selective Bayesian Forest Classifier</li>
<li>SKNN: A Super K-Nearest Neighbor (SKNN) Classification Algorithm</li>
<li>stacks: Tidy Model Stacking</li>
<li>TSMining: Mining Univariate and Multivariate Motifs in Time-Series Data</li>
<li>tsmp: Time Series with Matrix Profile</li>
<li>yardstick: Tidy Characterizations of Model Performance</li>
</ul>
<h3>7.7 Clustering.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>cclust: Convex Clustering Methods and Clustering Indexes</li>
<li>ChronoClust: perform clustering on each of a time-series of discrete datasets, and explicitly track the evolution
    of clusters over time</li>
<li>classix: Fast and explainable clustering based on sorting</li>
<li>ClusterEnsembles: package for cluster ensembles</li>
<li>clustergram: Visualization and diagnostics for cluster analysis in Python</li>
<li>Clusteval: methods for unsupervised cluster validation</li>
<li>deeptime: analysis of time series data including dimensionality reduction, clustering, and Markov model
    estimation</li>
<li>dtaidistance: Time series distances: Dynamic Time Warping</li>
<li>DTCR: Learning Representations for Time Series Clustering</li>
<li>DTW_kmedoids: Multivariate time series clustering using Dynamic Time Warping (DTW) and k-mediods</li>
<li>ETNA Time Series Library by Tinkoff AI</li>
<li>faiss: efficient similarity search and clustering of dense vectors</li>
<li>
<p>fastcluster: Fast hierarchical clustering routines</p>
</li>
<li>
<p>genieclust: Fast and Robust Hierarchical Clustering with Noise Point Detection</p>
</li>
<li>hcluster: Hierarchical Clustering Algorithms</li>
<li>hdbscan: high performance implementation of HDBSCAN clustering</li>
<li>scikit-learn: machine learning in Python</li>
<li>TimeSeriesDeepClustering: End-to-end deep representation learning for time series clustering</li>
<li>tslearn: machine learning toolkit dedicated to time-series data</li>
<li>validclust: Validate clustering results</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>apcluster: Affinity Propagation Clustering</li>
<li>bahc: bahc: Filter Covariance and Correlation Matrices with Bootstrapped-Averaged Hierarchical Ansatz</li>
<li>bootcluster: Bootstrapping Estimates of Clustering Stability</li>
<li>cclust: Convex Clustering Methods and Clustering Indexes</li>
<li>clue: Cluster Ensembles</li>
<li>clusrank: Wilcoxon Rank Tests for Clustered Data</li>
<li>clustAnalytics: Cluster Evaluation on Graphs</li>
<li>ClustAssess: Tools for Assessing Clustering</li>
<li>ClustBlock: Hierarchical and partitioning algorithms of blocks of variables</li>
<li>cluster: ”Finding Groups in Data”: Cluster Analysis Extended Rousseeuw et al.</li>
<li>clusterability: Performs Tests for Cluster Tendency of a Data Set</li>
<li>ClusterBootstrap: Analyze Clustered Data with Generalized Linear Models using the Cluster Bootstrap</li>
<li>Clustering: Techniques for Evaluating Clustering</li>
<li>clusterSEs: Calculate Cluster-Robust p-Values and Confidence Intervals</li>
<li>ClusterR: Gaussian Mixture Models, K-Means, Mini-Batch-Kmeans, K-Medoids and Affinity Propagation
    Clustering</li>
<li>clusterSim: Searching for Optimal Clustering Procedure for a Data Set</li>
<li>clustrd: Methods for Joint Dimension Reduction and Clustering</li>
<li>clustree: Visualise Clusterings at Different Resolutions</li>
<li>clValid: Validation of Clustering Results</li>
<li>cmbClust: Conditional Mixture Modeling and Model-Based Clustering</li>
<li>Ckmeans.1d.dp: Optimal, Fast, and Reproducible Univariate Clustering</li>
<li>diceR: Diverse Cluster Ensemble in R</li>
<li>
<p>dtwclust: Time Series Clustering Along with Optimizations for the Dynamic Time Warping Distance</p>
</li>
<li>
<p>evclust: Evidential Clustering</p>
</li>
<li>fastcluster: Fast Hierarchical Clustering Routines for R and ’Python’</li>
<li>fastkmedoids: Faster K-Medoids Clustering Algorithms: FastPAM, FastCLARA, FastCLARANS</li>
<li>FCPS: Fundamental Clustering Problems Suite</li>
<li>flexclust: Flexible Cluster Algorithms</li>
<li>fpc: Flexible Procedures for Clustering</li>
<li>genie: Fast, Robust, and Outlier Resistant Hierarchical Clustering</li>
<li>genieclust: The Genie++ Hierarchical Clustering Algorithm with Noise Points Detection</li>
<li>heatmaply: Interactive Cluster Heat Maps Using ’plotly’ and ’ggplot2’</li>
<li>HierPortfolios: Hierarchical Clustering-Based Portfolio Allocation Strategies</li>
<li>htestClust: Reweighted Marginal Hypothesis Tests for Clustered Data</li>
<li>kselection: Selection of K in K-Means Clustering</li>
<li>l1spectral: An L1-Version of the Spectral Clustering</li>
<li>leaderCluster: Leader Clustering Algorithm</li>
<li>LearnClust: Learning Hierarchical Clustering Algorithms</li>
<li>MatTransMix: Clustering with Matrix Gaussian and Matrix Transformation Mixture Models</li>
<li>mclust: Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation</li>
<li>mclustcomp: Measures for Comparing Clusters</li>
<li>mdendro: Extended Agglomerative Hierarchical Clustering</li>
<li>Mercator: Clustering and Visualizing Distance Matrices</li>
<li>MixGHD: Model Based Clustering, Classification and Discriminant Analysis Using the Mixture of Generalized
    Hyperbolic Distributions</li>
<li>MixSim: Simulating Data to Study Performance of Clustering Algorithms</li>
<li>mixSPE: Mixtures of Power Exponential and Skew Power Exponential Distributions for Use in Model-Based
    Clustering and Classification</li>
<li>mixture: Mixture Models for Clustering and Classification</li>
<li>MKMeans: A Modern K-Means (MKMeans) Clustering Algorithm</li>
<li>mlr3cluster: Cluster Extension for ’mlr3’</li>
<li>motifcluster: Motif-Based Spectral Clustering of Weighted Directed Networks</li>
<li>MSclust: Multiple-Scaled Clustering</li>
<li>mstknnclust: MST-kNN Clustering Algorithm</li>
<li>NNS: Nonlinear nonparametric statistics using partial moments</li>
<li>ProjectionBasedClustering: Projection Based Clustering</li>
<li>
<p>protoclust: Hierarchical Clustering with Prototypes</p>
</li>
<li>
<p>pytorch_cluster: PyTorch Extension Library of Optimized Graph Cluster Algorithms</p>
</li>
<li>QuClu: Quantile-Based Clustering Algorithms</li>
<li>rebmix: Finite Mixture Modeling, Clustering &amp; Classification</li>
<li>RCTS: Clustering Time Series While Resisting Outliers</li>
<li>RMBC: Robust Model Based Clustering</li>
<li>sClust: R Toolbox for Unsupervised Spectral Clustering</li>
<li>sigclust: Statistical Significance of Clustering</li>
<li>SLBDD: Statistical Learning for Big Dependent Data</li>
<li>Spectrum: Fast Adaptive Spectral Clustering for Single and Multi-View Data</li>
<li>T4cluster: Tools for Cluster Analysis</li>
<li>tclust: Robust Trimmed Clustering</li>
<li>tglkmeans: Efficient Implementation of K-Means++ Algorithm</li>
<li>TSclust: Time Series Clustering Utilities</li>
<li>vimpclust: Variable Importance in Clustering</li>
</ul>
<h3>7.8 Coding utilities and frameworks.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>Algviz is an algorithm visualization tool for your Python code</li>
<li>asteval: minimalistic evaluator of python expression using ast module</li>
<li>autoflake: Removes unused imports and unused variables as reported by pyflakes</li>
<li>autopep8: automatically formats Python code to conform to the PEP 8 style guide</li>
<li>autoray: Write numeric code that automatically works with any numpy-ish libraries</li>
<li>bandit: find common security issues in Python code</li>
<li>birdseye: Graphical debugger to view the values of all evaluated expressions</li>
<li>black: uncompromising Python code formatter</li>
<li>BLUE: The slightly less uncompromising Python code formatter</li>
<li>Bowler: Safe code refactoring by Facebook for modern Python</li>
<li>Comprehensive Python Cheatsheet</li>
<li>conda-deps: Generate conda environment files from Python and R source code</li>
<li>Crashtest is a Python library that makes exceptions handling and inspection easier.</li>
<li>darker: Apply black reformatting to Python files only in regions changed since a given commit</li>
<li>enum_tools: Tools to expand Python’s enum module.</li>
<li>
<p>erdantic: tool for drawing entity relationship diagrams (ERDs) for Python data model classes.</p>
</li>
<li>
<p>flake8: glues together pycodestyle, pyflakes, mccabe, and third-party plugins to check the style and quality of
    code</p>
</li>
<li>flake8-black: flake8 plugin to run black for checking Python coding style</li>
<li>friendly: replaces standard tracebacks by something easier to understand</li>
<li>Hatch is a modern, extensible Python project manager.</li>
<li>icecream: Never use print() to debug again</li>
<li>ipdb: exports functions to access the IPython debugger</li>
<li>isort: utility / library to sort imports</li>
<li>jedi: autocompletion, static analysis and refactoring library</li>
<li>jsonschema: implementation of the JSON Schema specification for Python</li>
<li>kedro: framework for creating reproducible, maintainable and modular data science code</li>
<li>kedro-viz: Visualise your Kedro data and machine-learning pipelines and track your experiments.</li>
<li>libfyaml: Fully feature complete YAML parser and emitter</li>
<li>luddite: Checks for out-of-date package versions</li>
<li>makepackage: easy packaging of Python code</li>
<li>mamba: Fast Cross-Platform Package Manager (reimplementation of the conda package manager in C++)</li>
<li>memray: memory profiler for Python</li>
<li>metaflow: build and manage real-life data science projects</li>
<li>mkdocs: Project documentation with Markdown</li>
<li>mkdocs-material: Technical documentation that just works</li>
<li>MonkeyType: toolkit by Instagram to generate static type annotations by collecting runtime types</li>
<li>Monty: supplementary useful functions for Python that are not part of the standard library</li>
<li>mypy: Optional static typing for Python</li>
<li>nptyping: Type hints for Numpy</li>
<li>numpydoc: Numpy’s Sphinx extensions</li>
<li>pdbpp: a drop-in replacement for pdb (the Python debugger)</li>
<li>PlantUML: Generate UML diagram from textual description</li>
<li>poetry: dependency management and packaging made easy</li>
<li>Pretty_Errors: Prettify Python exception output to make it legible</li>
<li>prospector: Inspects source files and provides information about type and location of classes, methods</li>
<li>ptvsd: debugger package by Microsoft for use with Visual Studio and Visual Studio Code</li>
<li>pudb: Full-screen console debugger for Python</li>
<li>pyan: Static call graph generator</li>
<li>
<p>pycodestyle: Simple Python style checker</p>
</li>
<li>
<p>pydantic: Data parsing and validation using Python type hints</p>
</li>
<li>pyDeprecate: tooling for marking deprecated functions or classes and re-routing to the new successors’ in-
    stance.</li>
<li>pyflakes: checks Python source files for errors</li>
<li>pylint: static code analysis tool</li>
<li>pyquickhelper: automation of many things</li>
<li>pyre: framework for building scientific applications in Python</li>
<li>pyre-check: Performant type-checking toolkit by Facebook</li>
<li>pyright: Static type checker by Microsoft</li>
<li>PyScaffold: Python project template generator with batteries included</li>
<li>PySnooper: Never use print for debugging again</li>
<li>py-spy: Sampling profiler for Python programs</li>
<li>pytools: a big bag of things that are ”missing” from the Python standard library</li>
<li>pytype: static type analyzer by Google</li>
<li>radon: tool that computes various metrics from the source code</li>
<li>rope: refactoring library</li>
<li>scalene: high-performance, high-precision CPU, GPU, and memory profiler for Python</li>
<li>sphinx: Sphinx documentation builder</li>
<li>StrictYAML is a type-safe YAML parser that parses and validates a restricted subset of the YAML specification</li>
<li>tryceratops: linter to prevent exception handling antipatterns in Python</li>
<li>typeguard: Run-time type checker for Python</li>
<li>TypePigeon: type converter focused on converting values between various Python data types.</li>
<li>varname:Dark magics about variable names in python</li>
<li>vulture: Find dead Python code</li>
<li>xlwings: ibrary that makes it easy to call Python from Excel and vice versa</li>
<li>yapf: formatter by Google for Python files</li>
<li>yappi: Yet Another Python Profiler, but this time multithreading, asyncio and gevent aware.</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>adaptalint: Check Code Style Painlessly</li>
<li>baguette: Efficient Model Functions for Bagging</li>
<li>box: Write Reusable, Composable and Modular R Code</li>
<li>butcher: Model Butcher: axe components of fitted model objects and help reduce the size of model objects
    saved to disk</li>
<li>cartbonate: Create beautiful images of source code using ’carbon.js</li>
<li>checkmate: Fast and Versatile Argument Checks</li>
<li>checkpoint: Install Packages from Snapshots on the Checkpoint Server for Reproducibility</li>
<li>cleanr: Helps You to Code Cleaner</li>
<li>delayed: A Framework for Parallelizing Dependent Tasks</li>
<li>goodpractice: Advice on R Package Building</li>
<li>hardhat: Construct Modeling Packages</li>
<li>IRdisplay: ’Jupyter’ Display Machinery</li>
<li>IRkernel: Native R Kernel for the ’Jupyter Notebook’</li>
<li>jetpack: A Friendly Package Manager</li>
<li>leprechaun: Create Simple ’Shiny’ Applications as Packages</li>
<li>lintr: A ’Linter’ for R Code</li>
<li>lvec: Out of Memory Vectors</li>
<li>memuse: Memory Estimation Utilities</li>
<li>metaflow: build and manage real-life data science projects</li>
<li>miniCRAN: Create a Mini Version of CRAN Containing Only Selected Packages</li>
<li>mongolite: Fast and Simple ’MongoDB’ Client for R</li>
<li>packager: Create, Build and Maintain Packages</li>
<li>parsnip: A Common API to Modeling and Analysis Functions</li>
<li>prettifyAddins: ’RStudio’ Addins to Prettify ’JavaScript’, ’C++’, ’Python’, and More</li>
<li>R6: Encapsulated Classes with Reference Semantics</li>
<li>R6P: Design Patterns in R</li>
<li>recipes: Preprocessing and Feature Engineering Steps for Modeling</li>
<li>renv: Project Environments</li>
<li>rhino: A Framework for Enterprise Shiny Applications</li>
<li>roxut: Document Unit Tests Roxygen-Style</li>
<li>
<p>roxygen2: In-Line Documentation for R</p>
</li>
<li>
<p>rstudio.prefs: Set ’RStudio’ Preferences</p>
</li>
<li>tidymodules: obust framework for developing ‘Shiny’ modules based on R6 classes which should facilitates
    inter-modules communication.</li>
<li>waldo: Find Differences Between R Objects</li>
<li>vetiver: Version, Share, Deploy, and Monitor Models</li>
<li>workflows: Modeling Workflows</li>
<li>workflowsets: Create a Collection of ’tidymodels’ Workflows</li>
</ul>
<h3>7.9 Computational performance.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>Aesara: definie, optimize, and efficiently evaluate mathematical expressions involving multi-dimensional ar-
    rays.</li>
<li>arctic: High performance datastore by Man Group for time series and tick data</li>
<li>bottleneck: Fast NumPy array functions written in C</li>
<li>Dask: Parallel computing with task scheduling</li>
<li>Dask-ML provides scalable machine learning in Python using Dask alongside popular machine learning libraries</li>
<li>datatable: library for fast multi-threaded data manipulation and munging</li>
<li>fairscale: PyTorch extensions for high performance and large scale training.</li>
<li>fastcore: Python supercharged for the fastai library</li>
<li>hypre: high performance preconditioners</li>
<li>jax: automatically differentiate native Python and NumPy functions</li>
<li>modin: ake your pandas code run faster by changing one line of code</li>
<li>multiprocess: better multiprocessing and multithreading in python</li>
<li>numexpr: Fast numerical expression evaluator for NumPy</li>
<li>PandaPy: speed of NumPy and the usability of Pandas but much faster</li>
<li>pandarallel: parallelize Pandas operations on all available CPUs</li>
<li>pandasvault:Advanced Pandas Vault - Utilities, Functions and Snippets</li>
<li>polars: Fast multi-threaded DataFrame library</li>
<li>ppft: distributed and parallel python</li>
<li>PyArma: Linear algebra library for Python</li>
<li>PyArmadillo: an alternative approach to linear algebra in Python</li>
<li>pyperf: Toolkit to run Python benchmarks</li>
<li>pyperformance: Python Performance Benchmark Suite</li>
<li>
<p>py-spy: Sampling profiler for Python programs</p>
</li>
<li>
<p>scalene: high-performance, high-precision CPU, GPU, and memory profiler for Python</p>
</li>
<li>swifter: efficiently applies any function to a pandas dataframe or series in the fastest available manner</li>
<li>tempeh is a framework to TEst Machine learning PErformance exHaustively which includes tracking memory
    usage and run time.</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>collapse: Advanced and Fast Data Transformation</li>
<li>dataPreparation: Automated Data Preparation</li>
<li>delayed: A Framework for Parallelizing Dependent Tasks</li>
<li>dplyr: A Grammar of Data Manipulation</li>
<li>MatrixStats: Methods that Apply to Rows and Columns of Matrices (and to Vectors)</li>
<li>mirai: Minimalist Async Evaluation Framework for R</li>
<li>purrr: Functional Programming Tools</li>
<li>tidyverse: set of packages that work in harmony because they share common data representations and ’API’
    design</li>
<li>timetk: A Tool Kit for Working with Time Series in R</li>
<li>tibble: Simple Data Frames</li>
<li>tidytidbits: A Collection of Tools and Helpers Extending the Tidyverse</li>
<li>tsibble: Tidy Temporal Data Frames and Tools</li>
</ul>
<h3>7.10 Containers, projects, pipelines and deployment</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>Driblet - Google Cloud based ML pipeline by Google</li>
<li>MLflow: A Machine Learning Lifecycle Platform</li>
<li>metaflow: Python/R library by Netflix to build and manage real-life data science projects</li>
<li>mlflow: Interface to ’MLflow’</li>
<li>mlxtend: extension and helper modules for data analysis and machine learning libraries</li>
<li>NNI: toolkit by Microsoft to help users automate Feature Engineering, Neural Architecture Search, Hyperpa-
    rameter Tuning and Model Compression</li>
<li>petastorm: toolkit by Uber for single machine or distributed training and evaluation of deep learning models
    (Tensorflow, Pytorch, and PySpark) from datasets in Apache Parquet format</li>
<li>pipelines: Machine Learning Pipelines for Kubeflow</li>
<li>Prefect: second-generation dataflow coordination and orchestration platform</li>
<li>PyTorch Lightning: The lightweight PyTorch wrapper for high performance AI research</li>
<li>Tango: toolkit by Allen Institute of Articial Intelligence for choreographing machine learning research</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>DriveML: Self-Drive Machine Learning Projects</li>
<li>metaflow: Python/R library by Netflix to build and manage real-life data science projects</li>
<li>mlflow: Interface to ’MLflow’</li>
</ul>
<h3>7.11 Covariances, correlations and volatilities.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>numpy: scientific computing</li>
<li>precise: online covariance and precision forecasting, portfolios, and model ensembles</li>
<li>PyPortfolioOpt: Financial portfolio optimization</li>
<li>sklearn.covariance: covariance estimation in scikit-learn</li>
<li>statsmodels: statistical modeling and econometrics</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>bahc: Filter Covariance and Correlation Matrices with Bootstrapped-Averaged Hierarchical Ansatz</li>
<li>BBcor: Bayesian Bootstrapping Correlations</li>
<li>BEKKs: Multivariate Conditional Volatility Modelling and Forecasting</li>
<li>BSCOV: Detection of Multiple Structural Breaks in Large Covariance Matrices</li>
<li>clubSandwich: Cluster-Robust (Sandwich) Variance Estimators with Small-Sample Corrections</li>
<li>cocor: Comparing Correlations</li>
<li>corpcor: Efficient Estimation of Covariance and (Partial) Correlation</li>
<li>correlation: Methods for Correlation Analysis</li>
<li>corx: Create and Format Correlation Matrices</li>
<li>CovTools: Statistical Tools for Covariance Analysis</li>
<li>cvCovEst: Cross-Validated Covariance Matrix Estimation</li>
<li>dcortools: Providing Fast and Flexible Functions for Distance Correlation Analysis</li>
<li>dCovTS: Distance Covariance and Correlation for Time Series Analysis</li>
<li>fitHeavyTail: Mean and Covariance Matrix Estimation under Heavy Tails</li>
<li>FRCC: Fast Regularized Canonical Correlation Analysis</li>
<li>gencor: Generate Customized Correlation Matrices</li>
<li>generalCorr: Generalized Correlations, Causal Paths and Portfolio Selection</li>
<li>
<p>mashr: Multivariate Adaptive Shrinkage</p>
</li>
<li>
<p>MatrixCorrelation: Matrix Correlation Coefficients</p>
</li>
<li>MTS: All-Purpose Toolkit for Analyzing Multivariate Time Series (MTS) and Estimating Multivariate Volatil-
    ity Models</li>
<li>NonParRolCor: a Non-Parametric Statistical Significance Test for Rolling Window Correlation</li>
<li>NNS: Nonlinear nonparametric statistics using partial moments</li>
<li>rags2ridges: Ridge Estimation of Precision Matrices from High-Dimensional Data</li>
<li>rmcorr: Repeated Measures Correlation</li>
<li>robcor: Robust Correlations</li>
<li>robustcov: Collection of Robust Covariance and (Sparse) Precision Matrix Estimators</li>
<li>RSC: Robust and Sparse Correlation Matrix</li>
<li>sandwich: Robust Covariance Matrix Estimators</li>
<li>WGCNA: Weighted Correlation Network Analysis</li>
</ul>
<h3>7.12 Data analysis and exploration.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>AutoViz: Automatically Visualize any dataset, any size with a single line of code.</li>
<li>daal4py: simplified API to Intel oneAPI Data Analytics Library</li>
<li>DeepGraph: scalable, general-purpose data analysis with Pandas-based Networks</li>
<li>D-tale:Visualizer by Man Group for pandas data structures</li>
<li>dython: Data analysis tools</li>
<li>empiricaldist: empirical distribution functions</li>
<li>hyperspy: Multidimensional data analysis</li>
<li>Lux: automate the visualization and data analysis process</li>
<li>mlxtend: extension and helper modules for Python’s data analysis and machine learning libraries.</li>
<li>numericalunits: Units and dimensional analysis compatible with everything</li>
<li>Orange: Interactive data analysis</li>
<li>pandas-profiling: Create HTML profiling reports from pandas DataFrame objects</li>
<li>PyApprox: high-dimensional approximation and uncertainty quantification</li>
<li>sweetviz: Visualize and compare datasets, target values and associations</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>checkmate: Fast and Versatile Argument Checks</li>
<li>collapse: Advanced and Fast Data Transformation</li>
<li>datacleanr: Interactive and Reproducible Data Cleaning</li>
<li>DataEditR: An Interactive Editor for Viewing, Entering, Filtering &amp; Editing Data</li>
<li>DataExplorer: Automate Data Exploration and Treatment</li>
<li>datamods: Modules to Import and Manipulate Data in ’Shiny’</li>
<li>dataprep: Efficient and Flexible Data Preprocessing Tools</li>
<li>DataVisualizations: Visualizations of High-Dimensional Data</li>
<li>datawizard: Easy Data Wrangling</li>
<li>DescTools: Tools for Descriptive Statistics</li>
<li>dimensio: Multivariate Data Analysis</li>
<li>discoveR: Exploratory Data Analysis System</li>
<li>dlookr: Tools for Data Diagnosis, Exploration, Transformation</li>
<li>EasyDescribe: A Convenient Way of Descriptive Statistics</li>
<li>esquisse: Explore and Visualize Your Data Interactively</li>
<li>explor: Interactive Interfaces for Results Exploration</li>
<li>exploratory: A Tool for Large-Scale Exploratory Analyses</li>
<li>explore: Simplifies Exploratory Data Analysis</li>
<li>factoextra: extract and visualize the output of multivariate data analyses, including ’PCA’ (Principal Compo-
    nent Analysis), ’CA’ (Correspondence Analysis), ’MCA’ (Multiple Correspondence Analysis), ’FAMD’ (Factor
    Analysis of Mixed Data), ’MFA’ (Multiple Factor Analysis) and ’HMFA’ (Hierarchical Multiple Factor Anal-
    ysis)</li>
<li>FactoInvestigate: Automatic Description of Factorial Analysis</li>
<li>FactoMineR: Multivariate Exploratory Data Analysis and Data Mining</li>
<li>ggESDA: Exploratory Symbolic Data Analysis with ’ggplot2’</li>
<li>HDTSA: High Dimensional Time Series Analysis Tools</li>
<li>infotheo: Information-Theoretic Measures</li>
<li>kfa: K-Fold Cross Validation for Factor Analysis</li>
<li>MazamaRollUtils: Efficient Rolling Functions</li>
<li>mmpca: Integrative Analysis of Several Related Data Matrices</li>
<li>praznik: Tools for Information-Based Feature Selection and Scoring</li>
<li>
<p>predictoR: Predictive Data Analysis System</p>
</li>
<li>
<p>rigr: Regression, Inference, and General Data Analysis Tools in R</p>
</li>
<li>robCompositions: Compositional Data Analysis</li>
<li>rrcov: Scalable Robust Estimators with High Breakdown Point</li>
<li>SmartEDA: Summarize and Explore the Data</li>
<li>statsExpressions: Tidy Dataframes and Expressions with Statistical Details</li>
<li>Statsomat: Shiny Apps for Automated Data Analysis and Automated Interpretation</li>
<li>thinkr: Tools for Cleaning Up Messy Files</li>
<li>tswge: Time Series for Data Science.Accompanies the texts Time Series for Data Science and Applied Time
    Series Analysis with R,</li>
<li>validata: Validate Data Frames</li>
<li>validate: Data Validation Infrastructure</li>
<li>validatetools: Checking and Simplifying Validation Rule Sets</li>
<li>wrangle: A Systematic Data Wrangling Idiom</li>
</ul>
<h3>7.13 Data augmentation, scenario generation and synthetic time series.</h3>
<p><strong>Collections of resources</strong></p>
<p>List of links:</p>
<ul>
<li>Synthetic data generation by Van Der Schaar Lab</li>
<li>Useful data augmentation resources</li>
</ul>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>agots: Anomaly Generator on Time Series</li>
<li>benchmark_VAE: Unifying Generative Autoencoder implementations in Python</li>
<li>Copulas: model multivariate data using copulas</li>
<li>CTGAN: Conditional GAN for Tabular Data</li>
<li>COMET Flows: Towards Generative Modeling of Multivariate Extremes and Tail Dependence</li>
<li>DataGeneration: Synthetic financial correlation matrix and time series generation</li>
<li>DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks</li>
<li>DeepEcho: Synthetic Data Generation for mixed-type, multivariate time series</li>
<li>deltapy: Tabular Data Augmentation</li>
<li>extremeIndex: Forecast Verification for Extreme Events</li>
<li>ixmp: platform for integrated and cross-cutting scenario analysis</li>
<li>MLlforHealthLab: Machine Learning and Artificial Intelligence for Medicine</li>
<li>
<p>pydantic-factories: Pydantic based mock data generation</p>
</li>
<li>
<p>pythae: Unifying Generative Autoencoder implementations in Python</p>
</li>
<li>RDT: Reversible Data Transforms to reproduce realistic data</li>
<li>scattering_covariance: analysis and generation of time series</li>
<li>SDMetrics: Metrics for Synthetic Data Generation Projects</li>
<li>SDGym: Benchmarking synthetic data generation methods</li>
<li>SDV: Synthetic Data Generation for tabular, relational and time series data</li>
<li>SignalFilters: Signal Filtering and Generation of Synthetic Time-Series</li>
<li>snorkel: system for quickly generating training data with weak supervision</li>
<li>synthia: Multidimensional synthetic data generation in Python</li>
<li>TGAN: Generative adversarial training for generating synthetic tabular data</li>
<li>TimeGAN: Time-series Generative Adversarial Networks</li>
<li>time-series-generator: Time Series Generator</li>
<li>TimeSynth: Synthetic Time Series Generation</li>
<li>tsaug: time series augmentation</li>
<li>tsBNgen: Generate Time Series Data Based on an Arbitrary Bayesian Network Structure</li>
<li>tsGAN: Time-series Generative Adversarial Networks</li>
<li>ydata-synthetic: Synthetic structured data generators</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>anySim: Simulation of Non-Gaussian Correlated Random Variables, Stochastic Processes and Random Fields</li>
<li>bootComb: Combine Parameter Estimates via Parametric Bootstrap</li>
<li>conjurer: A Parametric Method for Generating Synthetic Data</li>
<li>covsim: VITA, IG and PLSIM Simulation for Given Covariance and Marginals</li>
<li>fabricatr: Imagine Your Data Before You Collect It</li>
<li>fwb: Fractional Weighted Bootstrap</li>
<li>gencor: Generate Customized Correlation Matrices</li>
<li>gratis: Generating Time Series with Diverse and Controllable Characteristics</li>
<li>meboot: Maximum Entropy Bootstrap for Time Series</li>
<li>metamer: Create Data with Identical Statistics</li>
<li>missMethods: Methods for Missing Data</li>
<li>MixSim: Simulating Data to Study Performance of Clustering Algorithms</li>
<li>modeltime.resample: Resampling Tools for Time Series Forecasting</li>
<li>
<p>MonteCarlo: Automatic Parallelized Monte Carlo Simulations</p>
</li>
<li>
<p>MSCMT: Multivariate Synthetic Control Method Using Time Series</p>
</li>
<li>mvlognCorrEst: Sampling from Multivariate Lognormal Distributions and Estimating Correlations from Un-
    complete Correlation Matrix</li>
<li>naive: Empirical Extrapolation of Time Feature Patterns</li>
<li>RMT4DS: Computation of Random Matrix Models</li>
<li>rsample: General Resampling Infrastructure</li>
<li>segen: Sequence Generalization Through Similarity Network</li>
<li>SimJoint: Simulate Joint Distribution</li>
<li>simmer: Discrete-Event Simulation for R</li>
<li>simts: Time Series Analysis Tools</li>
<li>spooky: Time Feature Extrapolation Using Spectral Analysis and Jack-Knife Resampling</li>
<li>Synth: Synthetic Control Group Method for Comparative Case Studies</li>
<li>synthesis: Generate Synthetic Data from Statistical Models</li>
<li>tetragon: Automatic Sequence Prediction by Expansion of the Distance Matrix</li>
<li>TidyDensity: Functions for Tidy Analysis and Generation of Random Data</li>
<li>tscopula: Time Series Copula Models</li>
</ul>
<h3>7.14 Data cleaning, preparation and validation</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>cerberus: Lightweight, extensible data validation library</li>
<li>datatest: Tools for test driven data-wrangling and data validation</li>
<li>doubtlab: Doubt your data, find bad labels</li>
<li>framework: Data management framework for Python that provides functionality to describe, extract, validate,
    and transform tabular data</li>
<li>formencode: validation and form generation</li>
<li>pandera: perform data validation on dataframes</li>
<li>pydantic: Data parsing and validation using Python type hints</li>
<li>pyjanitor: Clean APIs for data cleaning. Python implementation of R package Janitor</li>
<li>PyOptimus: framework for cleaning and pre-processing data in a distributed fashion</li>
<li>scikit-learn: machine learning in Python</li>
<li>schema: library for validating Python data structures</li>
<li>serde: framework for defining, serializing, deserializing, and validating data structures</li>
<li>typical: Fast, simple, &amp; correct data-validation using Python 3 typing.</li>
<li>
<p>validators: Python data validation for Humans</p>
</li>
<li>
<p>Voluptuous: data validation library.</p>
</li>
<li>validr: simple, fast, extensible python library for data validation</li>
<li>wtforms: flexible forms validation and rendering library</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>cleanTS: Testbench for Univariate Time Series Cleaning</li>
<li>dataPreparation: Automated Data Preparation</li>
<li>data.validator: Automatic Data Validation and Reporting</li>
<li>datawizard: Easy Data Wrangling</li>
<li>errorlocate: Locate Errors with Validation Rules</li>
<li>pointblank: Data Validation and Organization of Metadata for Local and Remote Tables</li>
<li>testdat: Data Unit Testing for R</li>
<li>tsrobprep: Robust Preprocessing of Time Series Data</li>
<li>validate: Data Validation Infrastructure</li>
<li>validatetools: Checking and Simplifying Validation Rule Sets</li>
<li>wrangle: A Systematic Data Wrangling Idiom</li>
</ul>
<h3>7.15 Data Imputation</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>AutoImpute: Imputation Methods</li>
<li>Clairvoyance: a Unified, End-to-End AutoML Pipeline for Medical Time Series</li>
<li>fancyimpute: Multivariate imputation and matrix completion algorithms</li>
<li>HyperImpute: framework for prototyping and benchmarking imputation methods</li>
<li>imputena: automated and customized treatment of missing values in datasets</li>
<li>miceforest: Fast, Memory Efficient Imputation with LightGBM</li>
<li>MissForestExtra: nonparametric imputation on missing values</li>
<li>scikit-learn: machine learning in Python</li>
<li>statsmodels: statistical modeling and econometrics</li>
<li>tsai: time series tasks like classification, regression, forecasting, imputation</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>Amelia: A Program for Missing Data</li>
<li>CoImp: Copula Based Imputation Method</li>
<li>deductive: Data Correction and Imputation Using Deductive Methods</li>
<li>ggmice: Visualizations for ’mice’ with ’ggplot2’</li>
<li>howManyImputations: Calculate How many Imputations are Needed for Multiple Imputation</li>
<li>imputeFin: Imputation of Financial Time Series with Missing Values and/or Outliers</li>
<li>imputeGeneric: Ease the Implementation of Imputation Methods</li>
<li>imputeTestbench: Test Bench for the Comparison of Imputation Methods</li>
<li>imputeTS: Time Series Missing Value Imputation</li>
<li>Iscores: Proper Scoring Rules for Missing Value Imputation</li>
<li>mdgc: Missing Data Imputation Using Gaussian Copulas</li>
<li>mice: Multivariate Imputation by Chained Equations</li>
<li>miceadds: Some Additional Multiple Imputation Functions, Especially for ’mice’</li>
<li>miceafter: Data and Statistical Analyses after Multiple Imputation</li>
<li>miceFast: Fast Imputations Using ’Rcpp’ and ’Armadillo’</li>
<li>micemd: Multiple Imputation by Chained Equations with Multilevel Data</li>
<li>misPRIME: Partial Replacement Imputation Estimation for Missing Covariates</li>
<li>missMDA: Handling Missing Values with Multivariate Data Analysis</li>
<li>missMethods: Methods for Missing Data</li>
<li>missRanger: Fast Imputation of Missing Values</li>
<li>mlim: Multiple Imputation with Automated Machine Learning</li>
<li>NADIA: NA Data Imputation Algorithms</li>
<li>naniar: Data Structures, Summaries, and Visualisations for Missing Data</li>
<li>rego: Automatic Time Series Forecasting and Missing Value Imputation</li>
<li>Rforestry: Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability</li>
<li>simputation: Simple Imputation</li>
<li>SLBDD: Statistical Learning for Big Dependent Data</li>
<li>smcfcs: Multiple Imputation of Covariates by Substantive Model Compatible Fully Conditional Specification</li>
<li>univOutl: Detection of Univariate Outliers</li>
<li>VIM: Visualization and Imputation of Missing Values</li>
<li>yaImpute: Nearest Neighbor Observation Imputation and Evaluation Tools</li>
</ul>
<h3>7.16 Data regimes, states and changepoints: analysis and modeling.</h3>
<p><strong>Collections of resources</strong></p>
<p>List of links:</p>
<ul>
<li>Classifying market regimes</li>
<li>TCPD: toolkit by UK national institute for data science and artificial intelligence for Turing Change Point
    Dataset - A collection of time series for the evaluation and development of change point detection algorithms</li>
<li>TCPDBench: toolkit by UK national institute for data science and artificial intelligence for Turing Change
    Point Detection Benchmark: An Extensive Benchmark Evaluation of Change Point Detection Algorithms on
    real-world data</li>
</ul>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>changeforest: Random Forests for Change Point Detection</li>
<li>deeptime: analysis of time series data including dimensionality reduction, clustering, and Markov model
    estimation</li>
<li>greykite: flexible, intuitive and fast forecasting library</li>
<li>HMMLearn: Hidden Markov Models in Python with scikit-learn like API</li>
<li>kalmanfilter: Kalman Filter</li>
<li>kats: tookit by Facebook for time series analysis and forecasting</li>
<li>kimfilter: Rcpp’ implementation of the multivariate Kim filter, which combines the Kalman and Hamilton
    filters for state probability inference</li>
<li>Merlion: A Machine Learning Framework for Time Series Intelligence by SalesForce</li>
<li>msmtools: estimation and analysis of discrete state space Markov chains via Markov state models (MSM)</li>
<li>PyEMMA: Emma’s Markov Model Algorithms</li>
<li>pyGPCCA: Generalized Perron Cluster Cluster Analysis to coarse-grain reversible and non-reversible Markov
    state models.</li>
<li>pyhsmm: Bayesian inference in HSMMs and HMMs</li>
<li>pymc3-hmm: Hidden Markov models in PyMC3</li>
<li>ruptures: change point detection</li>
<li>SST: fast implementation of Singular Spectrum Transformation</li>
<li>Stone-Soup: framework for the development and testing of tracking algorithms</li>
<li>statsmodels: Markov switching models in statsmodels</li>
<li>transitionMatrix: Statistical analysis and visualization of state transition phenomena</li>
<li>tsmoothie: time-series smoothing and outlier detection</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>BayesHMM: Full Bayesian Inference for Hidden Markov Models</li>
<li>breakfast: Methods for Fast Multiple Change-Point Detection and Estimation</li>
<li>BSCOV: Detection of Multiple Structural Breaks in Large Covariance Matrices</li>
<li>ChangepointInference: Tools to test for a change in mean after changepoint detection</li>
<li>changepoints: A Collection of Change-Point Detection Methods</li>
<li>ChangePointTaylor: Identify Changes in Mean</li>
<li>chngpt: Estimation and Hypothesis Testing for Threshold Regression</li>
<li>cpss: Change-Point Detection by Sample-Splitting Methods</li>
<li>crossvalidationCP: Cross-Validation for Change-Point Regression</li>
<li>depmixS4: Dependent Mixture Models - Hidden Markov Models of GLMs and Other Distributions in S4</li>
<li>dynr: Dynamic Models with Regime-Switching</li>
<li>earlywarnings: Early Warning Signals Toolbox for Detecting Critical Transitions in Timeseries</li>
<li>fabisearch: Change Point Detection in High-Dimensional Time Series Networks</li>
<li>fHMM: Fitting Hidden Markov Models to Financial Data</li>
<li>inflection: Finds the Inflection Point of a Curve</li>
<li>InspectChangepoint: High-Dimensional Changepoint Estimation via Sparse Projection</li>
<li>jcp: Joint Change Point Detection</li>
<li>HMM: Hidden Markov Models</li>
<li>hmm.discnp: Hidden Markov Models with Discrete Non-Parametric Observation Distributions</li>
<li>hmmr: ”Mixture and Hidden Markov Models with R” Datasets and Example Code</li>
<li>KFAS: Kalman Filter and Smoother for Exponential Family State Space Models</li>
<li>ldhmm: Hidden Markov Model for Financial Time-Series Based on Lambda Distribution</li>
<li>mHMMbayes: Multilevel Hidden Markov Models Using Bayesian Estimation</li>
<li>MSGARCH: Markov-Switching GARCH Models</li>
<li>MSTest: Hypothesis Testing for Markov Switching Models</li>
<li>NHMSAR: Non-Homogeneous Markov Switching Autoregressive Models</li>
<li>onlineBcp: Online Bayesian Methods for Change Point Analysis</li>
<li>plotHMM: Plot Hidden Markov Models</li>
<li>pomp: Statistical Inference for Partially Observed Markov Processes</li>
<li>Rbeast: Bayesian Change-Point Detection and Time Series Decomposition</li>
<li>
<p>RChest: Locating Distributional Changes in Highly Dependent Time Series</p>
</li>
<li>
<p>robcp: Robust Change-Point Tests</p>
</li>
<li>segmented: Regression Models with Break-Points / Change-Points Estimation</li>
<li>seqHMM: Mixture Hidden Markov Models for Social Sequence Data and Other Multivariate, Multichannel
    Categorical Time Series</li>
<li>trendchange: Innovative Trend Analysis and Time-Series Change Point Analysis</li>
<li>tsDyn: Nonlinear Time Series Models with Regime Switching</li>
<li>wbsts: Multiple Change-Point Detection for Nonstationary Time Series</li>
</ul>
<h3>7.17 Data structures, storage and serialization</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>addict: Python Dict</li>
<li>anndata: package for handling annotated data matrices in memory and on disk</li>
<li>arctic: High performance datastore by Man Group for time series and tick data</li>
<li>cloudpickle: serialize Python constructs not supported by the default pickle module</li>
<li>dataclassy is a reimplementation of data classes in Python</li>
<li>datatable: fast multi-threaded data manipulation and munging</li>
<li>dill: extends Python’s pickle module for serializing and deserializing python objects to the majority of the
    built-in python types.</li>
<li>extendedjson: Easily extend JSON to encode and decode arbitrary Python objects</li>
<li>framework: Data management framework for Python that provides functionality to describe, extract, validate,
    and transform tabular data</li>
<li>MarketStore: DataFrame Server for Financial Timeseries Data</li>
<li>marshmallow: lightweight library for converting complex objects to and from simple Python datatypes</li>
<li>modin.pandas DataFrame is a parallel and distributed drop-in replacement for panda</li>
<li>Mongita is to MongoDB as SQLite is to SQL</li>
<li>mongo-arrow: Tools for using Apache Arrow with MongoDB</li>
<li>multidict: multidict implementation</li>
<li>Odo provides a uniform API for moving data between different formats</li>
<li>pandas: data structures for data analysis, time series, and statistics</li>
<li>pandasvault: Advanced Pandas Vault - Utilities, Functions and Snippets</li>
<li>pickle: Python object serialization</li>
<li>polars: Fast multi-threaded DataFrame library</li>
<li>
<p>pyarrow: Python API for Apache Arrow, a language independent columnar memory format for flat and
    hierarchical data</p>
</li>
<li>
<p>PyMongo - the Python driver for MongoDB</p>
</li>
<li>PyStore: Fast data store for Pandas time-series data</li>
<li>PyTables: package for managing hierarchical datasets</li>
<li>rpy2-arrow: Share Apache Arrow datasets between Python and R</li>
<li>serde: framework for defining, serializing, deserializing, and validating data structures</li>
<li>sklearn-pandas: bridge between Scikit-Learn’s machine learning methods and pandas-style Data Frames</li>
<li>sortedcontainers: Sorted Containers – Sorted List, Sorted Dict, Sorted Set</li>
<li>sqlite: Persistent dict, backed by sqlite3 and pickle, multithread-safe.</li>
<li>sparse: Sparse Multidimensional Arrays</li>
<li>srsly: Modern high performance serialization utilities</li>
<li>tablib: Module for Tabular Datasets in XLS, CSV, JSON, YAML,</li>
<li>tabmat: Efficient matrix representations for working with tabular data</li>
<li>TileDB: powerful engine for storing and accessing dense and sparse multi-dimensional arrays</li>
<li>tidypandas: grammar of data manipulation for pandas inspired by tidyverse</li>
<li>tinyarray: Tinyarrays are similar to NumPy arrays, but optimized to be much faster for small sizes</li>
<li>TinyDB is a lightweight document oriented database optimized for your happiness</li>
<li>tinyflux: iny time series database optimized for your happiness</li>
<li>torcharrow: torch.Tensor-like DataFrame library by Facebook using Arrow as a common memory format</li>
<li>ubermagtable: package for manipulating tabular data</li>
<li>ultrajson: Ultra fast JSON decoder and encoder written in C with Python bindings</li>
<li>Vector: arrays of 2D, 3D, and Lorentz vectors</li>
<li>Woodwork is a Python library that provides robust methods for managing and communicating data typing
    information</li>
<li>xarray: multidimensional labeled arrays and datasets</li>
<li>xpandas: Universal 1d/2d data containers with Transformers functionality for data analysis</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>arrow: Integration to Apache Arrow</li>
<li>dibble: Dimensional Data Frames</li>
<li>fst: Lightning Fast Serialization of Data Frames</li>
<li>gluedown: Wrap Vectors in Markdown Formatting</li>
<li>listdown: Create R Markdown from Lists</li>
<li>
<p>motifcluster: Motif-Based Spectral Clustering of Weighted Directed Networks</p>
</li>
<li>
<p>qs: Quick Serialization of R Objects</p>
</li>
<li>RcppSimdJson: ’Rcpp’ Bindings for the ’simdjson’ Header-Only Library for ’JSON’ Parsing</li>
<li>tibble: stricter checking and better formatting than the traditional data frame</li>
<li>tibblify: Rectangle Nested Lists</li>
<li>tidytable: Tidy Interface to ’data.table’</li>
<li>tiledb: Universal Storage Engine for Sparse and Dense Multidimensional Arrays</li>
<li>tsibble: Tidy Temporal Data Frames and Tools</li>
<li>tsbox: Class-Agnostic Time Series</li>
<li>vtreat: A Statistically Sound data.frame Processor/Conditioner</li>
</ul>
<h3>7.18 Dates and times</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>arrow: Better dates and times for Python</li>
<li>dateparser: parser for human readable dates</li>
<li>dateutil: Useful extensions to the standard Python datetime features</li>
<li>orjson: Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy</li>
<li>parsedatetime: human-readable date/time strings</li>
<li>pendulum: datatimes made easy</li>
<li>Pyrsistent: Persistent/Functional/Immutable data structures</li>
<li>python-dateutil: Useful extensions to the standard Python datetime features</li>
<li>PyTime: operate datetime by string</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>clock: Date-Time Types and Tools</li>
<li>lubridate: Make Dealing with Dates a Little Easier</li>
<li>qlcal: R Bindings to the Calendaring Functionality of ’QuantLib’</li>
<li>tidyquant: Tidy Quantitative Financial Analysis</li>
<li>timechange: Efficient Manipulation of Date-Times</li>
<li>timetk: A Tool Kit for Working with Time Series in R</li>
<li>tsbox: Class-Agnostic Time Series</li>
<li>TSrepr: Time Series Representations</li>
<li>xts: eXtensible Time Series</li>
<li>zoo: S3 Infrastructure for Regular and Irregular Time Series</li>
</ul>
<h3>7.19 Dimensionality reduction</h3>
<p><strong>Python</strong></p>
<p>List of packages/codes/frameworks/links:</p>
<ul>
<li>abess: Fast Best-Subset Selection Library</li>
<li>deeptime: analysis of time series data including dimensionality reduction, clustering, and Markov model
    estimation</li>
<li>direpack: State-of-the-Art Statistical Dimension Reduction Methods</li>
<li>EZyRB: Easy Reduced Basis method ; performs a data-driven model order reduction for parametrized prob-
    lems exploiting the recent approaches.</li>
<li>humap: Hierarchical Manifold Approximation and Projection (HUMAP) is a technique based on UMAP for
    hierarchical non-linear dimensionality reduction.</li>
<li>pyFIt-SNE: FFT-accelerated Interpolation-based t-SNE (FIt-SNE)</li>
<li>scikit-dimension: intrinsic dimension estimation</li>
<li>scikit-learn: machine learning in Python</li>
<li>(t-SNE: t-Distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction</li>
<li>UMAP: Uniform Manifold Approximation and Projection</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>abess: Fast Best-Subset Selection Library</li>
<li>abundant: High-Dimensional Principal Fitted Components and Abundant Regression</li>
<li>bayesdfa: Bayesian Dynamic Factor Analysis (DFA) with ’Stan’</li>
<li>clustrd: Methods for Joint Dimension Reduction and Clustering</li>
<li>dimRed: A Framework for Dimensionality Reduction</li>
<li>DLPCA: The Distributed Local PCA Algorithm</li>
<li>dobin: Dimension Reduction for Outlier Detection</li>
<li>dyndimred: Dimensionality Reduction Methods in a Common Format</li>
<li>EMD: Empirical Mode Decomposition and Hilbert Spectral Analysis</li>
<li>ForeCA: Forecastable Component Analysis</li>
<li>freqdom: Frequency Domain Based Analysis: Dynamic PCA</li>
<li>ica: Independent Component Analysis</li>
<li>ICtest: Estimating and Testing the Number of Interesting Components in Linear Dimension Reduction</li>
<li>prinvars: Principal Variables (methods for reducing the number of features within a data set)</li>
<li>quantdr: Dimension Reduction Techniques for Conditional Quantiles</li>
<li>
<p>rrpack: Reduced-Rank Regression</p>
</li>
<li>
<p>Rdimtools: Dimension Reduction and Estimation Methods</p>
</li>
<li>RSpectra: Solvers for Large-Scale Eigenvalue and SVD Problems</li>
<li>shrinkTVP: Efficient Bayesian Inference for Time-Varying Parameter Models with Shrinkage</li>
<li>spcr: Sparse Principal Component Regression</li>
<li>SuperPCA: Supervised Principal Component Analysis</li>
<li>svd: Interfaces to Various State-of-Art SVD and Eigensolvers</li>
<li>tapkee: tapkee: Wrapper for ’tapkee’ Dimension Reduction Library</li>
<li>tidydr: Unify Dimensionality Reduction Results</li>
<li>TSrepr: Time Series Representations (dimensionality reduction, preprocessing, feature extraction)</li>
<li>umap: Uniform Manifold Approximation and Projection</li>
</ul>
<h3>7.20 Distances and Similarity.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>DataGene: Identify How Similar TS Datasets Are to One Another</li>
<li>dcor: Distance correlation and related E-statistics</li>
<li>dtaidistance: Distance measures for time series</li>
<li>dtw-python: comprehensive implementation of dynamic time warping (DTW) algorithms</li>
<li>faiss: efficient similarity search and clustering of dense vectors</li>
<li>FLANN: Fast Library for Approximate Nearest Neighbors</li>
<li>GraKeL: scikit-learn compatible library for graph kernels</li>
<li>khiva-python: Python binding for Khiva library for time series analytics</li>
<li>mass-ts: MASS (Mueen’s Algorithm for Similarity Search)</li>
<li>MatrixProfile: ime series data mining tasks utilizing matrix profile</li>
<li>matrixprofile-ts: detect patterns and anomalies in massive datasets using Matrix Profile</li>
<li>netrd: library for network {reconstruction, distances, dynamics}</li>
<li>POT : Python Optimal Transport</li>
<li>PyMD: imple but general framework for embedding, called Minimum-Distortion Embedding (MDE), for finite
    sets of items, such as images, biological cells, nodes in a network, or any other abstract object</li>
<li>PySCAMP: SCAlable Matrix Profile</li>
<li>seriesdistancematrix: implements the Series Distance Matrix framework, a flexible component-based frame-
    work that bundles various Matrix Profile related techniques</li>
<li>sktime: unified framework for machine learning with time series by UK national institute for data science and
    artificial intelligence</li>
<li>
<p>Stone-Soup: framework for the development and testing of tracking algorithms</p>
</li>
<li>
<p>stumpy: variety of time series data mining tasks</p>
</li>
<li>tidydr: Unify Dimensionality Reduction Results</li>
<li>timesmash: Quantifier of universal similarity amongst arbitrary data streams without a priori knowledge,
    features, or training</li>
<li>wildboar: Time series learning</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>dispRity: Measuring Disparity (multidimensional space occupancy)</li>
<li>Distance: Distance Sampling Detection Function and Abundance Estimation</li>
<li>distantia: Assessing Dissimilarity Between Multivariate Time Series</li>
<li>dtw: Dynamic Time Warping Algorithms</li>
<li>dtwclust: Time Series Clustering Along with Optimizations for the Dynamic Time Warping Distance</li>
<li>fICA: Classical, Reloaded and Adaptive FastICA Algorithms</li>
<li>gdm: Generalized Dissimilarity Modeling</li>
<li>IncDTW: Incremental Calculation of Dynamic Time Warping</li>
<li>KernelKnn: Extends the simple k-nearest neighbors algorithm by incorporating numerous kernel functions
    and a variety of distance metrics</li>
<li>MatrixCorrelation: Matrix Correlation Coefficients</li>
<li>mclustcomp: Measures for Comparing Clusters</li>
<li>Mercator: Clustering and Visualizing Distance Matrices</li>
<li>philentropy: Similarity and Distance Quantification Between Probability Functions</li>
<li>proxy: Distance and Similarity Measures</li>
<li>segen: Sequence Generalization Through Similarity Network</li>
<li>tetragon: Automatic Sequence Prediction by Expansion of the Distance Matrix</li>
<li>TSclust: set of measures of dissimilarity between time series to perform time series clustering</li>
<li>TSdist: Distance Measures for Time Series Data</li>
<li>tsmp: UCR Matrix Profile Algorithm</li>
<li>VPdtw: Variable Penalty Dynamic Time Warping</li>
</ul>
<h3>7.21 ESG and Impact Investing.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ESG AI: ESG scoring as an automatic, data-driven process</li>
<li>ESG-BERT: Domain Specific BERT Model for Text Mining in Sustainable Investing</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>EnvStats: Package for Environmental Statistics, Including US EPA Guidance</li>
<li>ESGBoost: ESG and ECHO-based model for smart investing</li>
<li>gfer: Green Finance and Environmental Risk</li>
<li>text2sdg: Detecting UN Sustainable Development Goals in Text</li>
</ul>
<h3>7.22 Explainability, Interpretability, Fairness, Data Privacy</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>AIF360: comprehensive set of fairness metrics for datasets and machine learning models, explanations for
    these metrics, and algorithms to mitigate bias in datasets and models</li>
<li>captum: Model interpretability and understanding for PyTorch</li>
<li>CrypTen: framework for Privacy Preserving Machine Learning</li>
<li>Dice-ML: Generate Diverse Counterfactual Explanations for any machine learning model</li>
<li>DoWhy: toolkit by Microsoft for causal inference that supports explicit modeling and testing of causal as-
    sumptions</li>
<li>Interpret: Fit interpretable models by Microsoft. Explain blackbox machine learning</li>
<li>Interpretability dashboard, for understanding model predictions</li>
<li>Lime: Local Interpretable Model-Agnostic Explanations for machine learning classifiers</li>
<li>Lucid: neural network interpretability</li>
<li>PyExplainer: A Local Rule-Based Model-Agnostic Technique</li>
<li>Skater: Model Interpretation/Explanations</li>
<li>transformers-interpret: Model explainability that works seamlessly with transformers</li>
<li>XAI: eXplainability toolbox for machine learning</li>
<li>Xplique: toolkit dedicated to explainability, currently based on Tensorflow</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>DALEX: moDel Agnostic Language for Exploration and eXplanation</li>
<li>distillML: Model Distillation and Interpretability Methods for Machine Learning Models</li>
<li>fairml: Fair Models in Machine Learning</li>
<li>iml: Interpretable Machine Learning</li>
<li>interpret: Fit Interpretable Machine Learning Models</li>
<li>modelDown: Make Static HTML Website for Predictive Models</li>
<li>
<p>modelStudio: Interactive Studio for Explanatory Model Analysis</p>
</li>
<li>
<p>pdp: Partial Dependence Plots</p>
</li>
<li>pre: Prediction Rule Ensembles</li>
<li>Rforestry: Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability</li>
<li>rSAFE: Surrogate-Assisted Feature Extraction</li>
<li>sensitivity: Global Sensitivity Analysis of Model Outputs</li>
<li>triplot: Explaining Correlated Features in Machine Learning Models</li>
<li>yhat: Interpreting Regression Effects</li>
</ul>
<h3>7.23 Features for time series.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>cesium: Platform for Time Series Inference</li>
<li>Clairvoyance: a Unified, End-to-End AutoML Pipeline for Medical Time Series</li>
<li>FeatureTools: automated feature engineering</li>
<li>Featurewiz: advanced feature engineering strategies</li>
<li>khiva-python: Python binding for Khiva library for time series analytics</li>
<li>mne-features: MNE-Features software for extracting features from multivariate time series</li>
<li>scikit-learn: machine learning in Python</li>
<li>seglearn: integrated pipeline for segmentation, feature extraction, feature processing, and final estimator</li>
<li>tsfeatures: Calculates various features from time series data</li>
<li>tsfel: extract features from time series</li>
<li>tsflex: Flexible time series feature extraction &amp; processing</li>
<li>tsfresh: extract features from time series</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>autostsm: Automatic Structural Time Series Models</li>
<li>bfast: Breaks for Additive Season and Trend</li>
<li>entropy: Estimation of Entropy, Mutual Information and Related Quantities</li>
<li>feasts: Feature Extraction and Statistics for Time Series</li>
<li>fsMTS: Feature Selection for Multivariate Time Series</li>
<li>naive: Empirical Extrapolation of Time Feature Patterns</li>
<li>plsVarSel: Variable Selection in Partial Least Squares</li>
<li>MDFS: MultiDimensional Feature Selection</li>
<li>
<p>Rcatch22: Calculation of 22 CAnonical Time-Series CHaracteristics</p>
</li>
<li>
<p>theft: Tools for Handling Extraction of Features from Time Series</p>
</li>
<li>tsfeatures: Time Series Feature Extraction</li>
<li>TSrepr: Time Series Representations (dimensionality reduction, preprocessing, feature extraction)</li>
</ul>
<h3>7.24 Filtering and spectral analysis for time series</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>FilterPy: Kalman filtering and optimal estimation library</li>
<li>mkl_fft: NumPy-based Python interface to Intel (R) MKL FFT functionality</li>
<li>pyfilter: Particle filtering and sequential parameter inference</li>
<li>PyWavelets: Wavelet Transforms in Python</li>
<li>simdkalman: Kalman filters vectorized as Single Instruction, Multiple Data</li>
<li>Stone-Soup: framework for the development and testing of tracking algorithms</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ASSA: Applied Singular Spectrum Analysis (ASSA)</li>
<li>beyondWhittle: Bayesian Spectral Inference for Stationary Time Series</li>
<li>BMAmevt: Multivariate Extremes: Bayesian Estimation of the Spectral Measure</li>
<li>bsamGP: Bayesian Spectral Analysis Models using Gaussian Process Priors</li>
<li>bspec: Bayesian Spectral Inference</li>
<li>cohortBuilder: Data Source Agnostic Filtering Tools</li>
<li>EMD: Empirical Mode Decomposition and Hilbert Spectral Analysis</li>
<li>FKF: Fast Kalman Filter</li>
<li>FKF.SP: Fast Kalman Filtering Through Sequential Processing</li>
<li>frequencyConnectedness: Spectral Decomposition of Connectedness Measures</li>
<li>kalmanfilter: Kalman Filter</li>
<li>KFAS: Kalman Filter and Smoother for Exponential Family State Space Models</li>
<li>kimfilter: Rcpp’ implementation of the multivariate Kim filter, which combines the Kalman and Hamilton
    filters for state probability inference</li>
<li>LMfilteR: Filter Methods for Parameter Estimation in Linear and Non Linear Regression Models</li>
<li>mlr3filters: Filter Based Feature Selection for ’mlr3’</li>
<li>multitaper: Spectral Analysis Tools using the Multitaper Method</li>
<li>neverhpfilter: An Alternative to the Hodrick-Prescott Filter</li>
<li>
<p>praznik: Tools for Information-Based Feature Selection and Scoring</p>
</li>
<li>
<p>psd: Adaptive, Sine-Multitaper Power Spectral Density and Cross Spectrum Estimation</p>
</li>
<li>psdr: Use Time Series to Generate and Compare Power Spectral Density</li>
<li>quantspec: Quantile-Based Spectral Analysis of Time Series</li>
<li>Rfssa: Functional Singular Spectrum Analysis</li>
<li>rhosa: Higher-Order Spectral Analysis</li>
<li>robfilter: Robust Time Series Filters</li>
<li>RobKF: Innovative and/or Additive Outlier Robust Kalman Filtering</li>
<li>RSpectra: Solvers for Large-Scale Eigenvalue and SVD Problems</li>
<li>Rssa: A Collection of Methods for Singular Spectrum Analysis</li>
<li>Rwave: Time-Frequency Analysis of 1-D Signals</li>
<li>SLBDD: Statistical Learning for Big Dependent Data</li>
<li>spectral: Common Methods of Spectral Data Analysis</li>
<li>Spectrum: Fast Adaptive Spectral Clustering for Single and Multi-View Data</li>
<li>spooky: Time Feature Extrapolation Using Spectral Analysis and Jack-Knife Resampling</li>
<li>wavethresh: Wavelets Statistics and Transforms</li>
</ul>
<h3>7.25 Forecasting time series.</h3>
<p><strong>Collections of resources</strong></p>
<p>List of links:</p>
<ul>
<li>Popular Python Time Series Packages</li>
<li>State of the art research (with codes) on time series forecasting</li>
</ul>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>anticipy: time series forecasting</li>
<li>atspy: Automated Time Series Models in Python</li>
<li>Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</li>
<li>AutoTS: Automated Time Series Forecasting</li>
<li>Auto_TS: Automatically build multiple Time Series models using a Single Line of Code</li>
<li>Clairvoyance: Unified, End-to-End AutoML Pipeline for Medical Time Series</li>
<li>darts: toolkit by Unit8 for easy manipulation and forecasting of time series</li>
<li>ETNA Time Series Library by Tinkoff AI</li>
<li>fbprophet: forecasting toolkit by Facebook</li>
<li>fireTS: multi-variate time series prediction library working with sklearn</li>
<li>
<p>Flow Forecast: Deep learning PyTorch library for time series forecasting, classification, and anomaly detection</p>
</li>
<li>
<p>glum: Generalized linear models</p>
</li>
<li>GluonTS: toolkit by Amazon for Probabilistic time series modeling in Python</li>
<li>greykite: flexible, intuitive and fast forecasting library by LinkedIn</li>
<li>hcrystallball: unifies the API for most commonly used libraries and modeling techniques for time-series
    forecasting in the Python ecosystem</li>
<li>HierarchicalForecast: Hierarchical forecasting with statistical and econometric methods</li>
<li>kats: tookit by Facebook for time series analysis and forecasting</li>
<li>lazypredict: build models without much code</li>
<li>Local Cascade Ensemble (LCE) is a high-performing, scalable and user-friendly machine learning method for
    the general tasks of Classification and Regression</li>
<li>MAPIE: scikit-learn-compatible module for estimating prediction intervals.</li>
<li>Merlion: A Machine Learning Framework for Time Series Intelligence by SalesForce</li>
<li>MLForecast: Scalable machine learning based time series forecasting</li>
<li>NGBoost: Natural Gradient Boosting for Probabilistic Prediction</li>
<li>N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting</li>
<li>NeuralForecast: time series forecasting with deep learning models</li>
<li>nixtla: Automated time series processing and forecasting</li>
<li>Orbit: Bayesian forecasting package by Uber</li>
<li>piecewise-regression: For fitting straight line models to data with one or more breakpoints where the gradient
    changes</li>
<li>pmdarima: tatistical library designed to fill the void in Python’s time series analysis capabilities</li>
<li>predictionrevisited: implements the core statistical concepts from the book ”Prediction Revisited: The Im-
    portance of Observation”</li>
<li>Prophet: Automatic Forecasting Procedure by Facebook</li>
<li>PyAF: Automatic Time Series Forecasting</li>
<li>PyFlux: modern time series models, nference options (frequentist and Bayesian) that can be applied to these
    models</li>
<li>pyFTS: Fuzzy Time Series for Python</li>
<li>pysf: Supervised forecasting of sequential data by UK national institute for data science and artificial intelli-
    gence</li>
<li>PyTorch Forecasting: Forecasting timeseries with PyTorch - dataloaders, normalizers, metrics and models</li>
<li>pyts: time series classification</li>
<li>pytsal: Time Series analysis, visualization, forecasting along with AutoTS</li>
<li>scikit-hts: Hierarchical Time Series Forecasting</li>
<li>
<p>scikit-learn: machine learning in Python</p>
</li>
<li>
<p>sktime: unified framework for machine learning with time series by UK national institute for data science and
    artificial intelligence</p>
</li>
<li>slearn: package linking symbolic representation with scikit-learn machine learning</li>
<li>statsforecast: Lightning � fast forecasting with statistical and econometric models</li>
<li>Statsmodels: statistical modeling and econometrics in Python</li>
<li>tbats: BATS and TBATS time series forecasting methods</li>
<li>timemachines: Autonomous, univariate, k-step ahead time-series forecasting functions assigned Elo ratings</li>
<li>TIMEX: time series forecasting as a service</li>
<li>TSCV: Time Series CrossValidation</li>
<li>ts-eval: Time Series analysis and evaluation tools</li>
<li>tslearn: machine learning toolkit dedicated to time series data</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ACV: Optimal Out-of-Sample Forecast Evaluation and Testing under Stationarity</li>
<li>AIafter: Forecast Combination Using the AI-AFTER Algorithm</li>
<li>arfima: Fractional ARIMA (and Other Long Memory) Time Series Modeling</li>
<li>ATAforecasting: Automatic Time Series Analysis and Forecasting Using the Ata Method</li>
<li>autoTS: Automatic Model Selection and Prediction for Univariate Time Series</li>
<li>baguette: Efficient Model Functions for Bagging</li>
<li>bigtime: Sparse Estimation of Large Time Series Models</li>
<li>BINtools: Bayesian BIN (Bias, Information, Noise) Model of Forecasting</li>
<li>boot.pval: Bootstrap p-Values</li>
<li>caretForecast: Time Series Forecasting Using Caret Infrastructure</li>
<li>cvms: Cross-Validation for Model Selection</li>
<li>dsos: Dataset Shift with Outlier Scores</li>
<li>ensembleBMA: Probabilistic Forecasting using Ensembles and Bayesian Model Averaging</li>
<li>fable: Forecasting Models for Tidy Time Series</li>
<li>fable.ata: ATAforecasting Modelling Interface for fable Framework</li>
<li>fable.prophet: Prophet Modelling Interface for ’fable’</li>
<li>fabletools: Core Tools for Packages in the ’fable’ Framework</li>
<li>FinnTS: Microsoft Finance Time Series Forecasting Framework</li>
<li>flexmix: Flexible Mixture Modeling</li>
<li>
<p>ForeCA: Forecastable Component Analysis</p>
</li>
<li>
<p>ForecastComb: Forecast Combination Methods</p>
</li>
<li>forecastHybrid: Convenient Functions for Ensemble Time Series Forecasts</li>
<li>forecastML: Time Series Forecasting with Machine Learning Methods</li>
<li>forecastSNSTS: Forecasting for Stationary and Non-Stationary Time Series</li>
<li>ForecastTB: Test Bench for the Comparison of Forecast Methods</li>
<li>FoReco: Point Forecast Reconciliation</li>
<li>forecTheta: Forecasting Time Series by Theta Models</li>
<li>fpp3: Data for ”Forecasting: Principles and Practice” (3rd Edition)</li>
<li>fracdiff: Fractionally Differenced ARIMA aka ARFIMA(P,d,q) Models</li>
<li>fwildclusterboot: Fast Wild Cluster Bootstrap Inference for Linear Models</li>
<li>greybox: Toolbox for Model Building and Forecasting</li>
<li>Greymodels: Shiny App for Grey Forecasting Model</li>
<li>hts: Hierarchical and Grouped Time Series</li>
<li>ipred: Improved Predictors</li>
<li>legion: Forecasting Using Multivariate Models</li>
<li>MAPA: Multiple Aggregation Prediction Algorithm</li>
<li>mFLICA: Leadership-Inference Framework for Multivariate Time Series</li>
<li>modeltime: The Tidymodels Extension for Time Series Modeling</li>
<li>modeltime.ensemble: Ensemble Algorithms for Time Series Forecasting with Modeltime</li>
<li>modeltime.gluonts: ’GluonTS’ Deep Learning</li>
<li>modeltime.resample: Resampling Tools for Time Series Forecasting</li>
<li>ngboostForecast: Probabilistic Time Series Forecasting</li>
<li>OOS: Out-of-Sample Time Series Forecasting</li>
<li>origami: Generalized Framework for Cross-Validation</li>
<li>pre: Prediction Rule Ensembles</li>
<li>predtoolsTS: Time Series Prediction Tools</li>
<li>profoc: Probabilistic Forecast Combination Using CRPS Learning</li>
<li>prophet: Automatic Forecasting Procedure</li>
<li>PSF: Forecasting of Univariate Time Series Using the Pattern Sequence-Based Forecasting (PSF) Algorithm</li>
<li>PTSR: Positive Time Series Regression</li>
<li>RFpredInterval: Prediction Intervals with Random Forests and Boosted Forests</li>
<li>rigr: Regression, Inference, and General Data Analysis Tools in R</li>
<li>
<p>Rlgt: Bayesian Exponential Smoothing Models with Trend Modifications</p>
</li>
<li>
<p>robets: Forecasting Time Series with Robust Exponential Smoothing</p>
</li>
<li>robustarima: Robust ARIMA Modeling</li>
<li>scoringfunctions: A Collection of Scoring Functions for Assessing Point Forecasts</li>
<li>scoringRules: Scoring Rules for Parametric and Simulated Distribution Forecasts</li>
<li>scoringutils: Utilities for Scoring and Assessing Predictions</li>
<li>s2dverification: Set of Common Tools for Forecast Verification</li>
<li>see: Visualisation Toolbox for ’easystats’ and Extra Geoms, Themes and Color Palettes for ’ggplot2’</li>
<li>seer: Feature-Based Forecast Model Selection</li>
<li>segmented: Regression Models with Break-Points / Change-Points Estimation</li>
<li>sense: Automatic Stacked Ensemble for Regression Tasks</li>
<li>shrink: Global, Parameterwise and Joint Shrinkage Factor Estimation</li>
<li>SLBDD: Statistical Learning for Big Dependent Data</li>
<li>smooth: Forecasting Using State Space Models</li>
<li>spcr: Sparse Principal Component Regression</li>
<li>SPlit: Split a Dataset for Training and Testing</li>
<li>StabilizedRegression: Stabilizing Regression and Variable Selection</li>
<li>stacks: Tidy Model Stacking</li>
<li>subsemble: An Ensemble Method for Combining Subset-Specific Algorithm Fits</li>
<li>tensorTS: Factor and Autoregressive Models for Tensor Time Series</li>
<li>tfarima: Transfer Function and ARIMA Models</li>
<li>thief: Temporal Hierarchical Forecasting</li>
<li>tidymv: Tidy Model Visualisation for Generalised Additive Models</li>
<li>traineR: Predictive Models Homologator</li>
<li>TSdeeplearning: Deep Learning Model for Time Series Forecasting</li>
<li>tsDyn: Nonlinear Time Series Models with Regime Switching</li>
<li>tsensembler: Dynamic Ensembles for Time Series Forecasting</li>
<li>TSPred: Functions for Benchmarking Time Series Prediction</li>
<li>TSstudio: Functions for Time Series Analysis and Forecasting</li>
<li>tsutils: Time Series Exploration, Modelling and Forecasting</li>
<li>tswge: Time Series for Data Science.Accompanies the texts Time Series for Data Science and Applied Time
    Series Analysis with R,</li>
<li>vars: VAR Modelling</li>
<li>yardstick: Tidy Characterizations of Model Performance</li>
<li>yhat: Interpreting Regression Effects</li>
</ul>
<h3>7.26 Graphs and graphical modeling.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ogb: Benchmark datasets, data loaders, and evaluators for graph machine learning</li>
<li>pathpy: analysis of time series data on networks using higher-order and multi-order graphical models</li>
<li>PGM: Probabilistic Graphical Models</li>
<li>pgmpy: Probabilistic Graphical Models</li>
<li>PGM_PyLib: Inference and Learning of Probabilistic Graphical Models</li>
<li>pyaGrUM: Bayesian networks and other Probabilistic Graphical Models</li>
<li>scikit-network: nalysis of large graphs</li>
<li>skggm: Scikit-learn compatible estimation of general graphical models</li>
<li>vishwakarma: visualization library for Probabilistic Graphical Models, Discrete &amp; Continuous Distributions,
    and a lot more</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>backbone: Extracts the Backbone from Graphs</li>
<li>deepgp: Deep Gaussian Processes using MCMC</li>
<li>gmgm: Gaussian Mixture Graphical Model Learning and Inference</li>
<li>pcalg: Methods for Graphical Models and Causal Inference</li>
<li>Revticulate: Interaction with ”RevBayes” in R</li>
<li>tgp: Bayesian Treed Gaussian Process Models</li>
<li>tidygraph: A Tidy API for Graph Manipulation</li>
</ul>
<h3>7.27 Linear algebra.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>arctic: High performance datastore by Man Group for time series and tick data</li>
<li>PyArma: Linear algebra library for Python</li>
<li>PyArmadillo: an alternative approach to linear algebra in Python</li>
<li>PyPardiso: Python interface to the Intel MKL Pardiso library to solve large sparse linear systems of equations</li>
<li>Scipy: mathematics, science, and engineering</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>EigenR: Complex Matrix Algebra with ’Eigen’</li>
<li>fastmatrix: Fast Computation of some Matrices Useful in Statistics</li>
<li>freqdom: Frequency Domain Based Analysis: Dynamic PCA</li>
<li>ica: Independent Component Analysis</li>
<li>Matrix: Sparse and Dense Matrix Classes and Methods</li>
<li>MatrixExtra: Extra Methods for Sparse Matrices</li>
<li>matsbyname: An Implementation of Matrix Mathematics</li>
<li>proxyC: Computes Proximity in Large Sparse Matrices</li>
<li>rARPACK: Solvers for Large Scale Eigenvalue and SVD Problems</li>
<li>RcppArmadillo: ’Rcpp’ Integration for the ’Armadillo’ Templated Linear Algebra Library</li>
<li>RcppEigen: ’Rcpp’ Integration for the ’Eigen’ Templated Linear Algebra Library</li>
<li>Rlinsolve: Iterative Solvers for (Sparse) Linear System of Equations</li>
<li>RSpectra: Solvers for Large-Scale Eigenvalue and SVD Problems</li>
<li>sanic: Solving Ax = b Nimbly in C++</li>
<li>SparseChol: Sparse Cholesky LDL Decomposition of Symmetric Matrices</li>
<li>SparseM: Sparse Linear Algebra</li>
<li>svd: Interfaces to Various State-of-Art SVD and Eigensolvers</li>
</ul>
<h3>7.28 Machine Learning.</h3>
<p><strong>Collections of resources</strong></p>
<p>List of links:</p>
<ul>
<li>Curated list of open source libraries to deploy, monitor, version and scale machine learning</li>
<li>Dive into Machine Learning</li>
<li>Artificial Intelligence and Machine Learning For Quantum Technologies</li>
</ul>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>abess: Fast Best Subset Selection</li>
<li>AIF360: comprehensive set of fairness metrics for datasets and machine learning models, explanations for
    these metrics, and algorithms to mitigate bias in datasets and models</li>
<li>benchmark_VAE: Unifying Generative Autoencoder implementations in Python</li>
<li>bindsnet: Simulation of spiking neural networks (SNNs) using PyTorch</li>
<li>
<p>biosphere: Simple, fast random forests</p>
</li>
<li>
<p>Catalyst: PyTorch framework for Deep Learning Research and Development</p>
</li>
<li>catboost: Gradient Boosting on Decision Trees by Yandex</li>
<li>Chainer: flexible framework of neural networks for deep learning</li>
<li>combo: A Python Toolbox for Machine Learning Model Combination</li>
<li>compose: machine learning tool for automated prediction engineering</li>
<li>coremltools: convert machine learning models from third-party libraries to the Core ML format (by Apple)</li>
<li>CrypTen: framework for Privacy Preserving Machine Learning</li>
<li>DeepChecks: Testing and Validating ML Models and Data</li>
<li>DoubleML: Double Machine Learning in Python</li>
<li>Driblet - Google Cloud based ML pipeline by Google</li>
<li>geotorch: Constrained optimization toolkit for PyTorch</li>
<li>GPyTorch: Gaussian processes for modern machine learning systems.</li>
<li>Hub for Tensorflow: library for transfer learning by reusing parts of TensorFlow models</li>
<li>Hummingbird: library by Microsoft for compiling trained traditional ML models into tensor computations</li>
<li>InvarianceUnitTests: Linear unit-tests for invariance discovery</li>
<li>JAX: toolkit by Google for composable transformations of Python+NumPy programs: differentiate, vectorize,
    JIT to GPU/TPU, and more</li>
<li>jraph: Graph Neural Network Library in Jax</li>
<li>karateclub: Framework for Unsupervised Learning on Graphs</li>
<li>keras: deep learning API written in Python, running on top of the machine learning platform TensorFlow</li>
<li>Local Cascade Ensemble (LCE) is a high-performing, scalable and user-friendly machine learning method for
    the general tasks of Classification and Regression</li>
<li>LightGBM: fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART)
    framework by Microsoft</li>
<li>Merlion: A Machine Learning Framework for Time Series Intelligence by SalesForce</li>
<li>mlflow: Interface to ’MLflow’</li>
<li>MLForecast: Scalable machine learning based time series forecasting</li>
<li>mlinsights: Extends scikit-learn with new models, transformers, metrics, plotting.</li>
<li>MLJAR Automated Machine Learning for Humans</li>
<li>mlxtend: extension and helper modules for Python’s data analysis and machine learning libraries.</li>
<li>MMdnn: toolkit by Microsoft to convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch
    Onnx and CoreML.</li>
<li>Model Garden for TensorFlow</li>
<li>
<p>mvlearn is an open-source Python software package for multiview learning tools.</p>
</li>
<li>
<p>NannyM: estimate post-deployment model performance (without access to targets), detect data drift, and
    intelligently link data drift alerts back to changes in model performance</p>
</li>
<li>NeuralForecast: time series forecasting with deep learning models</li>
<li>NGBoost: Natural Gradient Boosting for Probabilistic Prediction</li>
<li>nimbusml: toolkit by Microsoft that provides Python bindings for ML.NET</li>
<li>nolearn: Combines the ease of use of scikit-learn with the power of Theano/Lasagne</li>
<li>norse: Deep learning with spiking neural networks (SNNs) in PyTorch.</li>
<li>OPACUS: Training PyTorch models with differential privacy</li>
<li>ptgnn: PyTorch Graph Neural Network Library</li>
<li>PyCaret : machine learning library</li>
<li>PyTorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration</li>
<li>PyTorch Lightning: lightweight PyTorch wrapper for ML researchers</li>
<li>Ray: packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter
    tuning librar</li>
<li>scikit-learn: machine learning in Python</li>
<li>scikit-learn-intelex: Intel Extension for Scikit-learn</li>
<li>sklearn-onnx converts scikit-learn models to ONNX</li>
<li>skorch: scikit-learn compatible neural network library that wraps PyTorch</li>
<li>SNNTORCH: Deep and online learning with spiking neural networks</li>
<li>tensorflow: end-to-end open source platform for machine learning</li>
<li>tf2onnx: Convert TensorFlow, Keras, Tensorflow.js and Tflite models to ONNX</li>
<li>Transfer Learning Library for Domain Adaptation, Task Adaptation, and Domain Generalization</li>
<li>transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX</li>
<li>Trax: Deep Learning by Google with Clear Code and Speed</li>
<li>tslearn: machine learning toolkit dedicated to time-series data</li>
<li>xformers: Hackable and optimized Transformers building blocks, supporting a composable construction</li>
<li>yellowbrick: Visual analysis and diagnostic tools to facilitate machine learning model selection</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>abess: Fast Best Subset Selection</li>
<li>agua: ’tidymodels’ Integration with ’h2o’</li>
<li>APML: An Approach for Machine-Learning Modelling</li>
<li>arenar: Arena for the Exploration and Comparison of any ML Models</li>
<li>
<p>brulee: High-Level Modeling Functions with ’torch’</p>
</li>
<li>
<p>distillML: Model Distillation and Interpretability Methods for Machine Learning Models</p>
</li>
<li>elmNNRcpp: The Extreme Learning Machine Algorithm</li>
<li>fairmodels: Flexible Tool for Bias Detection, Visualization, and Mitigation</li>
<li>familiar: End-to-End Automated Machine Learning and Model Evaluation</li>
<li>KernelKnn: Extends the simple k-nearest neighbors algorithm by incorporating numerous kernel functions
    and a variety of distance metrics</li>
<li>lightgbm: Light Gradient Boosting Machine by Microsoft</li>
<li>MachineShop: Machine Learning Models and Tools</li>
<li>mcboost: Multi-Calibration Boosting</li>
<li>MetricsWeighted: Weighted Metrics, Scoring Functions and Performance Measures for Machine Learning</li>
<li>mikropml: User-Friendly R Package for Supervised Machine Learning Pipelines</li>
<li>mlflow: Interface to ’MLflow’</li>
<li>mlquantify: Algorithms for Class Distribution Estimation</li>
<li>mlr3: Machine Learning in R - Next Generation</li>
<li>mlr3cluster: Cluster Extension for ’mlr3’</li>
<li>mlr3learners: Recommended Learners for ’mlr3’</li>
<li>mlr3tuning: hyperparameter tuning with ’mlr3’</li>
<li>mlr3verse: package family is a set of packages for machine-learning purposes built in a modular fashion</li>
<li>mlr3viz: Visualizations for’mlr3</li>
<li>mlrintermbo: Model-Based Optimization for ’mlr3’ Through ’mlrMBO’</li>
<li>mlrMBO: Bayesian Optimization and Model-Based Optimization of Expensive Black-Box Functions</li>
<li>multiview: Cooperative Learning for Multi-View Analysis</li>
<li>rTorch: R Bindings to ’PyTorch’</li>
<li>SPlit: Split a Dataset for Training and Testing</li>
<li>tensorflow: R Interface to ’TensorFlow’</li>
<li>tfdatasets: Interface to ’TensorFlow’ Datasets</li>
<li>tfprobability: Interface to ’TensorFlow Probability’</li>
<li>TSdeeplearning: Deep Learning Model for Time Series Forecasting</li>
<li>xgboost: Extreme Gradient Boosting</li>
</ul>
<h3>7.29 Machine Learning frameworks (includes Automated ML and hyperparameters tuning)</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>AI2 Tango: library for choreographing machine learning research</li>
<li>AutoGluon: toolkit by Amazon on AutoML for Text, Image, and Tabular Data</li>
<li>AutoKeras: An AutoML system based on Keras</li>
<li>AutoPyTorch: Automatic architecture search and hyperparameter optimization for PyTorch</li>
<li>auto-sklearn: Automated Machine Learning with scikit-learn</li>
<li>BayesianOptimization: global optimization with gaussian processes.</li>
<li>cesium: Machine Learning Time-Series Platform</li>
<li>Clairvoyance: Unified, End-to-End AutoML Pipeline for Medical Time Series</li>
<li>Colossal-AI: A Unified Deep Learning System for Big Model Era</li>
<li>EvalML is an AutoML library which builds, optimizes, and evaluates machine learning pipelines using domain-
    specific objective functions.</li>
<li>FLAML: accurate machine learning models automatically, efficiently and economically (by Microsoft)</li>
<li>flax: neural network library for JAX that is designed for flexibility</li>
<li>H2O is an Open Source, Distributed, Fast &amp; Scalable Machine Learning Platform</li>
<li>Hypernets: General Automated Machine Learning framework</li>
<li>HyperOpt: Distributed Asynchronous Hyperparameter Optimization</li>
<li>hyperopt-sklearn: Hyper-parameter optimization for sklearn</li>
<li>kedro: framework for creating reproducible, maintainable and modular data science code</li>
<li>kedro-viz: Visualise your Kedro data and machine-learning pipelines and track your experiments.</li>
<li>keras-tuner: hyperparameter optimization framework</li>
<li>MLBox: Automated Machine Learning library</li>
<li>mlpack: ’Rcpp’ Integration for the ’mlpack’ Library</li>
<li>mlr3tuning: Tuning for ’mlr3’</li>
<li>model_search: framework (by Google) that implements AutoML algorithms for model architecture search at
    scale</li>
<li>NannyM: estimate post-deployment model performance (without access to targets), detect data drift, and
    intelligently link data drift alerts back to changes in model performance</li>
<li>NNI: toolkit by Microsoft to help users automate Feature Engineering, Neural Architecture Search, Hyperpa-
    rameter Tuning and Model Compression</li>
<li>oneflow: OneFlow is a deep learning framework designed to be user-friendly, scalable and efficient.</li>
<li>
<p>ONNX: Open Neural Network Exchange is an Open standard for machine learning interoperability</p>
</li>
<li>
<p>Optuna: hyperparameter optimization framework</p>
</li>
<li>PyCaret : machine learning library</li>
<li>squirrel-core: library that enables ML teams to share, load, and transform data in a collaborative, flexible,
    and efficient way.</li>
<li>Relevance AI - The ML Platform for Unstructured Data Analysis</li>
<li>Talos: Hyperparameter Optimization for TensorFlow, Keras and PyTorch</li>
<li>trax: end-to-end library (by Google Brain) for deep learning that focuses on clear code and speed.</li>
<li>tune-sklearn: drop-in replacement for Scikit-Learn’s GridSearchCV / RandomizedSearchCV – but with cutting
    edge hyperparameter tuning techniques</li>
<li>vowpal_wabbit: machine learning system which pushes the frontier of machine learning with techniques such
    as online, hashing, allreduce, reductions, learning2search, active, and interactive learning</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>autokeras: R Interface to ’AutoKeras’</li>
<li>automl: Deep Learning with Metaheuristic</li>
<li>DriveML: Self-Drive Machine Learning Projects</li>
<li>familiar: End-to-End Automated Machine Learning and Model Evaluation</li>
<li>mlpack: ’Rcpp’ Integration for the ’mlpack’ Library</li>
<li>mlr3tuningspaces: Search Spaces for Hyperparameter Tuning</li>
<li>ParBayesianOptimization: Parallel Bayesian Optimization of Hyperparameters</li>
<li>rBayesianOptimization: Bayesian Optimization of Hyperparameters</li>
<li>RemixAutoML: automation of machine learning, forecasting, feature engineering, model evaluation, model
    interpretation, recommenders, and EDA.</li>
</ul>
<h3>7.30 Network and graph analysis.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>dantro: handle, transform, and visualize hierarchically structured data</li>
<li>deeptime: nalysis of time series data including dimensionality reduction, clustering, and Markov model esti-
    mation</li>
<li>ETNA Time Series Library by Tinkoff AI</li>
<li>fastpath: find the path through a network of nodes</li>
<li>GraKeL: scikit-learn compatible library for graph kernels</li>
<li>grapharray: handle network link/node attributes as Numpy arrays</li>
<li>GraphVite: A General and High-performance Graph Embedding System</li>
<li>
<p>karateclub: Framework for Unsupervised Learning on Graphs</p>
</li>
<li>
<p>netrd: etwork {reconstruction, distances, dynamics}</p>
</li>
<li>networkit: toolkit for large-scale network analysis</li>
<li>NetworkX: Network Analysis in Python</li>
<li>pandana: Pandas Network Analysis: fast accessibility metrics and shortest paths, using contraction hierarchies</li>
<li>pyvis: visualizing interactive network graphs</li>
<li>rustworkx: high performance Python graph library implemented in Rust</li>
<li>scikit-learn: machine learning in Python</li>
<li>tslearn: machine learning toolkit dedicated to time-series data</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>backbone: identify the most ‘important’ or ‘significant’ edges in a network</li>
<li>bnmonitor: An Implementation of Sensitivity Analysis in Bayesian Networks</li>
<li>bootnet: Bootstrap Methods for Various Network Estimation Routines</li>
<li>CINNA: Deciphering Central Informative Nodes in Network Analysis</li>
<li>dbnR: Dynamic Bayesian Network Learning and Inference</li>
<li>diceR: Diverse Cluster Ensemble in R</li>
<li>dtwclust: Time Series Clustering Along with Optimizations for the Dynamic Time Warping Distance</li>
<li>fabisearch: Change Point Detection in High-Dimensional Time Series Networks</li>
<li>fastkmedoids: Faster K-Medoids Clustering Algorithms: FastPAM, FastCLARA, FastCLARANS</li>
<li>gRain: Graphical Independence Networks</li>
<li>heatmaply: Interactive Cluster Heat Maps Using ’plotly’ and ’ggplot2’</li>
<li>influential: Identification and Classification of the Most Influential Nodes</li>
<li>MatTransMix: Clustering with Matrix Gaussian and Matrix Transformation Mixture Models</li>
<li>Mercator: Clustering and Visualizing Distance Matrices</li>
<li>MixSim: Simulating Data to Study Performance of Clustering Algorithms</li>
<li>mixture: Mixture Models for Clustering and Classification</li>
<li>MKMeans: A Modern K-Means (MKMeans) Clustering Algorithm</li>
<li>ndtv: Network Dynamic Temporal Visualizations</li>
<li>network: Classes for Relational Data</li>
<li>networkABC: Network Reverse Engineering with Approximate Bayesian Computation</li>
<li>networkDynamic: Dynamic Extensions for Network Objects</li>
<li>NetworKit: tool suite for high-performance network analysis</li>
<li>
<p>networktools: Tools for Identifying Important Nodes in Networks</p>
</li>
<li>
<p>statnet: Software Tools for the Statistical Analysis of Network Data</p>
</li>
<li>visNetwork: Network Visualization using ’vis.js’ Library</li>
<li>wdnet: Weighted and Directed Networks</li>
<li>WGCNA: Weighted Correlation Network Analysis</li>
</ul>
<h3>7.31 Numerical methods (includes numerical optimization)</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ADE: Asynchronous Differential Evolution, with efficient multiprocessing</li>
<li>autoray: Write numeric code that automatically works with any numpy-ish libraries</li>
<li>BayesianOptimization: global optimization with gaussian processes</li>
<li>CasADi is a symbolic framework for numeric optimization implementing automatic differentiation in forward
    and reverse modes on sparse matrix-valued computational graphs</li>
<li>cmaes: Covariance Matrix Adaptation Evolution Strategy (CMA-ES)</li>
<li>coco: Numerical Black-Box Optimization Benchmarking Framework</li>
<li>cp_solver: CP-SAT Solver by Google</li>
<li>cvxopt: convex optimization</li>
<li>cvxpy: convex optimization</li>
<li>DEAP: Distributed Evolutionary Algorithms in Python</li>
<li>derivative: Numerical differentiation of noisy time series data</li>
<li>Differential Evolution expensiveopt</li>
<li>eigenpy: Efficient Python bindings between Numpy/Eigen</li>
<li>ELA drframework: Dimensionality Reduction Framework for Exploratory Landscape Analysis</li>
<li>evol: grammar for evolutionary algorithms and heuristics</li>
<li>fcmaes complements scipy optimize by providing additional optimization methods, faster C++/Eigen based
    implementations and a coordinated parallel retry mechanism.</li>
<li>gemseo: Generic Engine for Multi-disciplinary Scenarios, Exploration and Optimization</li>
<li>General Purpose Optimization Library GPOL</li>
<li>HiGHS: Linear optimization</li>
<li>hyperactive: optimization and data collection toolbox for convenient and fast prototyping of computationally
    expensive models</li>
<li>ipopt: Cython interface for the interior point optimzer IPOPT</li>
<li>ipyopt: interface for the interior point optimizer COIN-OR IPOpt</li>
<li>mystic: highly-constrained non-convex optimization and uncertainty quantification</li>
<li>
<p>nevergrad: Python toolbox for performing gradient-free optimization by Facebook</p>
</li>
<li>
<p>nlopt: nonlinear optimization</p>
</li>
<li>Open MDAO: optimization framework</li>
<li>optima: library for numerical optimization calculations</li>
<li>OR-Tools: optimization toolkit by Google</li>
<li>osqp: Operator Splitting QP Solver</li>
<li>pybobyqa: Derivative-Free Optimization with Bound Constraints</li>
<li>pycma: Covariance Matrix Adaptation Evolution Strategy (CMA-ES)</li>
<li>pymoo: Multi-objective Optimization</li>
<li>pyomo: supports a diverse set of optimization capabilities for formulating and analyzing optimization models.</li>
<li>PyOptSparse: object-oriented framework for formulating and solving nonlinear constrained optimization prob-
    lems</li>
<li>PyPDE: solve partial differential equations using finite differences.</li>
<li>qpsolvers: Quadratic programming solvers in Python with a unified API</li>
<li>root_numpy: interface between ROOT and NumPy</li>
<li>scikit-opt: Swarm Optimization methods</li>
<li>scikit-optimize: Sequential model-based optimization with a ‘scipy.optimize‘ interface</li>
<li>Scipy: Fundamental algorithms for scientific computing</li>
<li>SHADE: Success-History Based Parameter Adaptation for Differential Evolution</li>
<li>stgaircase: data analysis package based on mathematical step functions</li>
<li>theseus: differentiable nonlinear optimization</li>
<li>torchquad: High-performance numerical integration on the GPU with PyTorch, JAX and Tensorflow</li>
<li>torchsde: Differentiable SDE solvers with GPU support and efficient sensitivity analysis</li>
<li>trust-region:trust-region subproblem solvers for nonlinear optimization</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ao: Alternating Optimization</li>
<li>bbotk: Black-Box Optimization Toolkit</li>
<li>CGNM: Cluster Gauss-Newton Method: Find multiple solutions of a nonlinear least squares problem</li>
<li>CVXR: Disciplined Convex Optimization</li>
<li>DEoptim: Global Optimization by Differential Evolution</li>
<li>DEoptimR: Differential Evolution Optimization in Pure R</li>
<li>ECOSolveR: Embedded Conic Solver in R</li>
<li>
<p>ggblanket: Simplify ’ggplot2’ Visualisation</p>
</li>
<li>
<p>graDiEnt: derivative-free, optim-style Stochastic Quasi-Gradient Differential Evolution optimization</p>
</li>
<li>itp: The Interpolate, Truncate, Project (ITP) Root-Finding Algorithm</li>
<li>LowRankQP: Low Rank Quadratic Programming</li>
<li>miesmuschel: Mixed Integer Evolution Strategies</li>
<li>minqa: Derivative-Free Optimization Algorithms by Quadratic Approximation</li>
<li>mlr3mbo: Flexible Bayesian Optimization</li>
<li>NMOF: Numerical Methods and Optimization in Finance</li>
<li>osqp: Quadratic Programming Solver using the ’OSQP’ Library</li>
<li>RcppEnsmallen: Header-Only C++ Mathematical Optimization Library for ’Armadillo’</li>
<li>rvinecopulib: High Performance Algorithms for Vine Copula Modeling</li>
<li>rgenoud: R Version of GENetic Optimization Using Derivatives</li>
<li>rmoo: Multi-Objective Optimization in R</li>
<li>scs: Splitting Conic Solver for linear programs (’LPs’), second-order cone programs (’SOCPs’), semidefinite
    programs (’SDPs’), exponential cone programs (’ECPs’), and power cone programs (’PCPs’), or problems
    with any combination of those cone</li>
<li>SimEngine: A Modular Framework for Statistical Simulations in R</li>
<li>trustOptim: Trust Region Optimization for Nonlinear Functions with Sparse Hessians</li>
</ul>
<h3>7.32 Probabilistic modeling (includes mixture models and Gaussian Processes)</h3>
<p>Links to resources</p>
<ul>
<li>Professionally curated list of awesome Conformal Prediction videos, tutorials, books, papers, PhD and MSc
    theses, articles and open-source libraries</li>
</ul>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>beanmachine: inference on probabilistic models</li>
<li>celerite2: fast and scalable Gaussian Process (GP) Regression</li>
<li>conformal-rnn: code for ”Conformal time-series forecasting”, NeurIPS 2021</li>
<li>crepes: Conformal Regressors and Conformal Predictive Systems</li>
<li>EnbPI: Ensemble batch prediction intervals</li>
<li>EnCQR: ensemble conformalized quantile regression (EnCQR)</li>
<li>GluonTS: toolkit by Amazon for Probabilistic time series modeling in Python</li>
<li>gptools: Gaussian processes with arbitrary derivative constraints and predictions.</li>
<li>GPy: Gaussian processes framework</li>
<li>GPyTorch: Gaussian processes for modern machine learning systems.</li>
<li>
<p>MAPIE: scikit-learn-compatible module for estimating prediction intervals</p>
</li>
<li>
<p>NGBoost: Natural Gradient Boosting for Probabilistic Prediction</p>
</li>
<li>orbit-ml: Bayesian forecasting package by Uber</li>
<li>pgmpy: Probabilistic Graphical Models – learning (Structure and Parameter), inference (Probabilistic and
    Causal), and simulations in Bayesian Networks</li>
<li>pplbench: Evaluation Framework for Probabilistic Programming Languages</li>
<li>PyMC: Bayesian Modeling and Probabilistic Machine Learning with Aesara</li>
<li>pyro: Deep universal probabilistic programming with Python and PyTorch</li>
<li>PySloth: Probabilistic Prediction</li>
<li>skpro: toolkit by UK national institute for data science and artificial intelligence for Supervised domain-
    agnostic prediction framework for probabilistic modelling</li>
<li>tinyGP: The tiniest of Gaussian Process libraries</li>
<li>zhusuan: probabilistic programming library for Bayesian deep learning, generative models, based on Tensor-
    flow</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>AdequacyModel: Adequacy of Probabilistic Models and General Purpose Optimization</li>
<li>AdMit: Adaptive Mixture of Student-t Distributions</li>
<li>aldvmm: Adjusted Limited Dependent Variable Mixture Models</li>
<li>bgmm: Gaussian Mixture Modeling Algorithms and the Belief-Based Mixture Modeling</li>
<li>bmixture: Bayesian Estimation for Finite Mixture of Distributions</li>
<li>BNPmix: Bayesian Nonparametric Mixture Models</li>
<li>bpgmm: Bayesian Model Selection Approach for Parsimonious Gaussian Mixture Models</li>
<li>ClusterR: Gaussian Mixture Models, K-Means, Mini-Batch-Kmeans, K-Medoids and Affinity Propagation
    Clustering</li>
<li>conformalInference.multi: Conformal Inference Tools for Regression with Multivariate Response</li>
<li>DistributionOptimization: Distribution Optimization</li>
<li>distributionsrd: Distribution Fitting and Evaluation</li>
<li>EMCluster: EM Algorithm for Model-Based Clustering of Finite Mixture Gaussian Distribution</li>
<li>evmix: Extreme Value Mixture Modelling, Threshold Estimation and Boundary Corrected Kernel Density
    Estimation</li>
<li>flexmix: Flexible Mixture Modeling</li>
<li>flexmixNL: Finite Mixture Modeling of Generalized Nonlinear Models</li>
<li>GauPro: Gaussian Process Fitting</li>
<li>gmgm: Gaussian Mixture Graphical Model Learning and Inference</li>
<li>
<p>greta.gp: Gaussian Process Modelling in ’greta’</p>
</li>
<li>
<p>hmmr: ”Mixture and Hidden Markov Models with R” Datasets and Example Code</p>
</li>
<li>ltmix: Left-Truncated Mixtures of Gamma, Weibull, and Lognormal Distributions</li>
<li>MatrixMixtures: Model-Based Clustering via Matrix-Variate Mixture Models</li>
<li>MGMM: Missingness Aware Gaussian Mixture Models</li>
<li>mistr: Mixture and Composite Distributions</li>
<li>mixComp: Estimation of Order of Mixture Distributions</li>
<li>MixMatrix: Classification with Matrix Variate Normal and t Distributions</li>
<li>MixSim: Simulating Data to Study Performance of Clustering Algorithms</li>
<li>mixsmsn: Fitting Finite Mixture of Scale Mixture of Skew-Normal Distributions</li>
<li>mixreg: Functions to Fit Mixtures of Regressions</li>
<li>mixSPE: Mixtures of Power Exponential and Skew Power Exponential Distributions for Use in Model-Based
    Clustering and Classification</li>
<li>mixsqp: Sequential Quadratic Programming for Fast Maximum-Likelihood Estimation of Mixture Proportions</li>
<li>mixtools: Tools for Analyzing Finite Mixture Models</li>
<li>mixture: Mixture Models for Clustering and Classification</li>
<li>mclust: Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation</li>
<li>mlr3proba: Probabilistic Supervised Learning for ’mlr3’</li>
<li>MoMPCA: Inference and Clustering for Mixture of Multinomial Principal Component Analysis</li>
<li>mvgb: Multivariate Probabilities of Scale Mixtures of Multivariate Normal Distributions via the Genz and
    Bretz (2002) QRSVN Method</li>
<li>ngboostForecast: Probabilistic Time Series Forecasting</li>
<li>nlsmsn: Fitting Nonlinear Models with Scale Mixture of Skew-Normal Distributions</li>
<li>Nmix: Bayesian Inference on Univariate Normal Mixtures</li>
<li>nvmix: Multivariate Normal Variance Mixtures</li>
<li>opGMMassessment: Optimized Automated Gaussian Mixture Assessment</li>
<li>pgmm: Parsimonious Gaussian Mixture Models</li>
<li>pGPx: Pseudo-Realizations for Gaussian Process Excursions</li>
<li>pks: Probabilistic Knowledge Structures</li>
<li>plgp: Particle Learning of Gaussian Processes</li>
<li>plotmm: Tidy Tools for Visualizing Mixture Models</li>
<li>QuantileGH: Quantile Least Mahalanobis Distance Estimator for Tukey g-&amp;-h Mixture</li>
<li>rebmix: Finite Mixture Modeling, Clustering &amp; Classification</li>
<li>Revticulate: Interaction with ”RevBayes” in R</li>
<li>
<p>RGMM: Robust Mixture Model</p>
</li>
<li>
<p>RMixtComp: Mixture Models with Heterogeneous and (Partially) Missing Data</p>
</li>
<li>robmixglm: Robust Generalized Linear Models (GLM) using Mixtures</li>
<li>Rmixmod: Classification with Mixture Modelling</li>
<li>RobMixReg: Robust Mixture Regression</li>
<li>rrMixture: Reduced-Rank Mixture Models</li>
<li>seqHMM: Mixture Hidden Markov Models for Social Sequence Data and Other Multivariate, Multichannel
    Categorical Time Series</li>
<li>skewlmm: Scale Mixture of Skew-Normal Linear Mixed Models</li>
<li>skewMLRM: Estimation for Scale-Shape Mixtures of Skew-Normal Distributions</li>
<li>uGMAR: Estimate Univariate Gaussian and Student’s t Mixture Autoregressive Models</li>
</ul>
<h3>7.33 Reinforcement learning.</h3>
<p><strong>Collections of resources</strong></p>
<p>List of links:</p>
<ul>
<li>Awesome Reinforcement Learning: Reinforcement learning resources curated</li>
<li>Awesome Deep RL: curated list of awesome Deep Reinforcement Learning resources</li>
</ul>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>Acme: a research framework by DeepMind for reinforcement learning</li>
<li>Baconian: Model-based Reinforcement Learning Framework</li>
<li>Open AI Baselines: high-quality implementations by OpenAI of reinforcement learning algorithms</li>
<li>Catalyst.RL: Distributed Framework for Reproducible RL Research</li>
<li>ChainerRL: deep reinforcement learning library built on top of Chainer</li>
<li>Coach: Reinforcement Learning by Intel AI Lab</li>
<li>d3rlpy: offline deep reinforcement learning library</li>
<li>Decision Transformer: Reinforcement Learning via Sequence Modeling</li>
<li>DRL with PyTorch: PyTorch implementations of deep reinforcement learning algorithms and environments</li>
<li>Deep Reinforcement Learning Hands-On</li>
<li>deer: DEEp Reinforcement learning framework</li>
<li>Dopamine: research framework by Google for fast prototyping of reinforcement learning algorithms</li>
<li>ElegantRL: Lightweight and scalable deep reinforcement learning using PyTorch</li>
<li>FinRL: Deep Reinforcement Learning for Quantitative Finance</li>
<li>FinRL-Meta: Universe of Near-Real Market Environments for Data-Driven Financial Reinforcement Learning</li>
<li>
<p>garage: toolkit for reproducible reinforcement learning research</p>
</li>
<li>
<p>Gym: ttolkit by openAI for toolkit for developing and comparing reinforcement learning algorithms</p>
</li>
<li>HRAC: Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning</li>
<li>keras-rl: Deep Reinforcement Learning for Keras</li>
<li>Mava: library of multi-agent reinforcement learning components and systems</li>
<li>Multi-Agent Resource Optimization (MARO) platform is an instance of Reinforcement Learning as a Service
    (RaaS) for real-world resource optimization problems.</li>
<li>MBRL-Lib: toolbox by Facebook for facilitating development of Model-Based Reinforcement Learning algo-
    rithms</li>
<li>Mushroom RL: modular toolkit able to use modularity allows to use libraries for tensor computation (e.g.
    PyTorch, Tensorflow) and RL benchmarks (e.g. OpenAI Gym, PyBullet, Deepmind Control Suite)</li>
<li>PettingZoo: Gym for multi-agent reinforcement learning</li>
<li>PFRL: PyTorch-based deep reinforcement learning library</li>
<li>PGPortfolio: Policy Gradient Portfolio</li>
<li>PyTorchRL: reinforcement learning library focused on modularity and simplicity</li>
<li>Rainbow: Combining Improvements in Deep Reinforcement Learning</li>
<li>ReAgent: platform by Facebook for Reasoning systems (Reinforcement Learning, Contextual Bandits, etc.)</li>
<li>rl: modular, primitive-first, python-first PyTorch library for Reinforcement Learning.</li>
<li>RLkit: Collection of reinforcement learning algorithms</li>
<li>RLlib: Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperpa-
    rameter tuning librar</li>
<li>RLMeta is a light-weight flexible framework for Distributed Reinforcement Learning Research</li>
<li>rlpyt: Reinforcement Learning in PyTorch</li>
<li>rlstructures: Facebook library to facilitate the implementation of new reinforcement learning algorithms</li>
<li>skrl: Modular reinforcement learning</li>
<li>Stable Baselines3: PyTorch version of Stable Baselines, reliable implementations of reinforcement learning
    algorithms</li>
<li>Tensorforce: TensorFlow library for applied reinforcement learning</li>
<li>TensorLayer: Deep Learning and Reinforcement Learning Library for Scientists and Engineers</li>
<li>TF-Agents: TensorFlow library for Contextual Bandits and Reinforcement Learning</li>
<li>Tianshou: PyTorch deep reinforcement learning library</li>
<li>Tonic RL: Tonic RL library</li>
<li>TorchBeast: A PyTorch Platform by Facebook for Distributed RL</li>
<li>TRFL: TensorFlow Reinforcement Learning by DeepMind</li>
<li>vowpal_wabbit: machine learning system which pushes the frontier of machine learning with techniques such
    as online, hashing, allreduce, reductions, learning2search, active, and interactive learning</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>Hands-On Reinforcement Learning</li>
<li>QLearning: Reinforcement Learning using the Q Learning Algorithm</li>
<li>reinforcelearn: reinforcement learning, including Q-Learning algorithm</li>
<li>ReinforcementLearning: Model-Free Reinforcement Learning</li>
<li>RLT: Reinforcement Learning Trees</li>
</ul>
<h3>7.34 Robust numerical methods.</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>derivative: Numerical differentiation of noisy time series data</li>
<li>hypothesize: hypothesis testing using robust statistics</li>
<li>robusta: interface to many common statistical analyses, performed using through R and RPY2.</li>
<li>Robustats is a Python library for high-performance computation of robust statistical estimators</li>
<li>robustbase: Statistical Estimators (Sn, Qn, MAD, IQR)</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>clubSandwich: Cluster-Robust (Sandwich) Variance Estimators with Small-Sample Corrections</li>
<li>l1spectral: An L1-Version of the Spectral Clustering</li>
<li>L2E: Robust Structured Regression via the L2 Criterion</li>
<li>pcaPP: Robust PCA by Projection Pursuit</li>
<li>RCTS: Clustering Time Series While Resisting Outliers</li>
<li>RDnp: Robust Test for Complete Independence in High-Dimensions</li>
<li>revss: Robust Estimation in Very Small Samples</li>
<li>RGMM: Robust Mixture Model</li>
<li>rigr: Regression, Inference, and General Data Analysis Tools in R</li>
<li>robcp: Robust Change-Point Tests</li>
<li>robcor: Robust Correlations</li>
<li>robfilter: Robust Time Series Filters</li>
<li>robmixglm: Robust Generalized Linear Models (GLM) using Mixtures</li>
<li>RobMixReg: Robust Mixture Regression</li>
<li>RobStatTM: Robust Statistics: Theory and Methods</li>
<li>
<p>robust: Port of the S+ ”Robust Library”</p>
</li>
<li>
<p>RobustANOVA: Robust One-Way ANOVA Tests under Heteroscedasticity and Nonnormality</p>
</li>
<li>robustbase: Basic Robust Statistics</li>
<li>RobustCalibration: Robust Calibration of Imperfect Mathematical Models</li>
<li>robustcov: Collection of Robust Covariance and (Sparse) Precision Matrix Estimators</li>
<li>robustHD: Robust Methods for High-Dimensional Data</li>
<li>rrcov: Scalable Robust Estimators with High Breakdown Point</li>
<li>RSC: Robust and Sparse Correlation Matrix</li>
<li>sandwich: Robust Covariance Matrix Estimators</li>
<li>StabilizedRegression: Stabilizing Regression and Variable Selection</li>
<li>tsrobprep: Robust Preprocessing of Time Series Data</li>
<li>walrus: Robust Statistical Methods</li>
</ul>
<h3>7.35 Selection of features, variables, models, data splits</h3>
<p><strong>Collections of resources</strong></p>
<p>List of links:</p>
<ul>
<li>Data Science Feature Engineering and Selection Tutorials</li>
<li>Feature Engineering and Selection: A Practical Approach for Predictive Models</li>
<li>Guide for Feature Engineering and Feature Selection</li>
</ul>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>abess: Fast Best-Subset Selection Library</li>
<li>boruta_py: Boruta all-relevant feature selection method</li>
<li>dython: Data analysis tools</li>
<li>featureclass: Feature engineering library to keep track of feature dependencies, documentation and schema</li>
<li>feature_engine: library with multiple transformers to engineer and select features for use in machine learning
    models</li>
<li>FeatureTools: automated feature engineering</li>
<li>Featurewiz: advanced feature engineering strategies</li>
<li>ITMO_FS: Feature selection library</li>
<li>KnockPy: Knockoffs for controlled variable selection</li>
<li>kydavra: feature selection</li>
<li>Py_FS: Feature Selection</li>
<li>pyHSICLasso: Versatile Nonlinear Feature Selection Algorithm for High-dimensional Data</li>
<li>
<p>python_stepwiseSelection: Automated Backward and Forward Selection</p>
</li>
<li>
<p>scikit-learn: machine learning in Python</p>
</li>
<li>scikit-rebate: scikit-learn-compatible Python implementation of ReBATE, a suite of Relief-based feature
    selection algorithms</li>
<li>Sklearn-genetic-opt: Hyperparameters tuning and feature selection, using evolutionary algorithms</li>
<li>sktime: unified framework for machine learning with time series by UK national institute for data science and
    artificial intelligence</li>
<li>tsfeatures: Calculates various features from time series data. Python implementation of the R package
    tsfeatures</li>
<li>UltraNest: Fit and compare complex models reliably and rapidly. Advanced nested sampling</li>
<li>zoofs: feature selection using a variety of nature-inspired wrapper algorithms</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>abess: Fast Best-Subset Selection Library</li>
<li>BAS: Bayesian Variable Selection and Model Averaging using Bayesian Adaptive Sampling</li>
<li>basad: Bayesian Variable Selection with Shrinking and Diffusing Priors</li>
<li>BayesVarSel: Bayes Factors, Model Choice and Variable Selection in Linear Models</li>
<li>bpgmm: Bayesian Model Selection Approach for Parsimonious Gaussian Mixture Models</li>
<li>bravo: Bayesian Screening and Variable Selection</li>
<li>care: High-Dimensional Regression and CAR Score Variable Selection</li>
<li>dials: Tools for Creating Tuning Parameter Values</li>
<li>EMVS: The Expectation-Maximization Approach to Bayesian Variable Selection</li>
<li>FeatureTerminatoR: Feature Selection Engine to Remove Features with Minimal Predictive Power</li>
<li>FSinR: Feature Selection</li>
<li>fsMTS: Feature Selection for Multivariate Time Series</li>
<li>gausscov: The Gaussian Covariate Method for Variable Selection</li>
<li>greybox: Toolbox for Model Building and Forecasting</li>
<li>hrqglas: Group Variable Selection for Quantile and Robust Mean Regression</li>
<li>knockoff: The Knockoff Filter for Controlled Variable Selection</li>
<li>mBvs: Bayesian Variable Selection Methods for Multivariate Data</li>
<li>MDFS: MultiDimensional Feature Selection</li>
<li>mlr3fselect: Feature Selection for ’mlr3’</li>
<li>mplot: Graphical Model Stability and Variable Selection Procedures</li>
<li>MXM: Feature Selection (Including Multiple Solutions) and Bayesian Networks</li>
<li>
<p>nestfs: Cross-Validated (Nested) Forward Selection</p>
</li>
<li>
<p>NonpModelCheck: Model Checking and Variable Selection in Nonparametric Regression</p>
</li>
<li>pcaPP: Robust PCA by Projection Pursuit</li>
<li>picR: Predictive Information Criteria for Model Selection</li>
<li>plsVarSel: Variable Selection in Partial Least Squares</li>
<li>praznik: Tools for Information-Based Feature Selection and Scoring</li>
<li>prinvars: Principal Variables (methods for reducing the number of features within a data set)</li>
<li>projpred: Projection Predictive Feature Selection</li>
<li>Rforestry: Random Forests, Linear Trees, and Gradient Boosting for Inference and Interpretability</li>
<li>rmcfs: The MCFS-ID Algorithm for Feature Selection and Interdependency Discovery</li>
<li>rSAFE: Surrogate-Assisted Feature Extraction</li>
<li>rstanarm: Bayesian Applied Regression Modeling via Stan</li>
<li>SelectBoost: A General Algorithm to Enhance the Performance of Variable Selection Methods in Correlated
    Datasets</li>
<li>SignifReg: Consistent Significance Controlled Variable Selection in Generalized Linear Regression</li>
<li>sivs: Stable Iterative Variable Selection</li>
<li>smoothic: Variable Selection Using a Smooth Information Criterion</li>
<li>SPlit: Split a Dataset for Training and Testing</li>
<li>splitTools: Tools for Data Splitting</li>
<li>stabiliser: Stabilising Variable Selection</li>
<li>stabm: Stability Measures for Feature Selection</li>
<li>stacks: Tidy Model Stacking</li>
<li>stepgbm: Stepwise Variable Selection for Generalized Boosted Regression Modeling</li>
<li>SWIM: Scenario Weights for Importance Measurement</li>
<li>theft: Tools for Handling Extraction of Features from Time Series</li>
<li>tornado: Plots for Model Sensitivity and Variable Importance</li>
<li>valse: Variable Selection with Mixture of Models</li>
<li>WLasso: Variable Selection for Highly Correlated Predictors</li>
</ul>
<h3>7.36 Sensitivity analysis and numerical derivatives</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>derivative: Numerical differentiation of noisy time series data</li>
<li>higher: obtain higher order gradients</li>
<li>jacobi: Numerical derivatives for Python</li>
<li>JAX: toolkit by Google for composable transformations of Python+NumPy programs: differentiate, vectorize,
    JIT to GPU/TPU, and more</li>
<li>OMSens: OpenModelica sensitivity analysis and optimization module</li>
<li>PyApprox: high-dimensional approximation and uncertainty quantification by Sandia Labs</li>
<li>SALib: Sensitivity Analysis Library (Contains Sobol, Morris, FAST, and other methods)</li>
<li>sensitivity: Sensitivity Analysis</li>
<li>tangent: library (by Google) for automatic differentiation providing Source-to-Source Debuggable Derivatives
    in Pure Python</li>
<li>torchsde: Differentiable SDE solvers with GPU support and efficient sensitivity analysis</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>bnmonitor: An Implementation of Sensitivity Analysis in Bayesian Networks</li>
<li>GSA.UN: Global Sensitivity Analysis Tool</li>
<li>reval: Argument Table Generation for Sensitivity Analysis</li>
<li>samon: Sensitivity Analysis for Missing Data</li>
<li>sensemakr: Sensitivity Analysis Tools for Regression Models</li>
<li>sensitivity: Global Sensitivity Analysis of Model Outputs</li>
<li>sensobol: Computation of Variance-Based Sensitivity Indices</li>
<li>SWIM: Scenario Weights for Importance Measurement</li>
<li>tornado: Plots for Model Sensitivity and Variable Importance</li>
</ul>
<h3>7.37 Statistics and Probability</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>distfit: probability density function fitting and hypothesis testing</li>
<li>empiricaldist: empirical distribution functions</li>
<li>momentum: Running mean, variance, skew, and kurtosis</li>
<li>pingouin: Statistical package in Python based on Pandas</li>
<li>
<p>probs: Probability library</p>
</li>
<li>
<p>PyProbables: Probabilistic data structures in python</p>
</li>
<li>PyStats: statistical analysis and distributions</li>
<li>RunStats: Computing Statistics and Regression in One Pass</li>
<li>statsmodels: statistical modeling and econometrics</li>
<li>tensorflow-probability: Probabilistic reasoning and statistical analysis in TensorFlow</li>
<li>wquantiles: Weighted quantiles</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>arsenal: An Arsenal of ’R’ Functions for Large-Scale Statistical Summaries</li>
<li>ashr: Methods for Adaptive Shrinkage, using Empirical Bayes</li>
<li>confintr: Confidence Intervals</li>
<li>DEM: The Distributed EM Algorithms in Multivariate Gaussian Mixture Models</li>
<li>DescTools: Tools for Descriptive Statistics</li>
<li>distr6: The Complete R6 Probability Distributions Interface</li>
<li>distr: Object Oriented Implementation of Distributions</li>
<li>distrEx: Extensions of Package ’distr’</li>
<li>distributionsrd: Distribution Fitting and Evaluation</li>
<li>DPQ: Density, Probability, Quantile (’DPQ’) Computations</li>
<li>EasyDescribe: A Convenient Way of Descriptive Statistics</li>
<li>entropy: Estimation of Entropy, Mutual Information and Related Quantities</li>
<li>estimatr: Fast Estimators for Design-Based Inference</li>
<li>evd: Functions for Extreme Value Distributions</li>
<li>expectreg: Expectile and Quantile Regression</li>
<li>fitur: Fit Univariate Distributions</li>
<li>fromo: Fast Robust Moments</li>
<li>Gmedian: Geometric Median, k-Medians Clustering and Robust Median PCA</li>
<li>HSAUR3: A Handbook of Statistical Analyses Using R (3rd Edition)</li>
<li>lmom: L-Moments</li>
<li>lmomco: L-Moments, Censored L-Moments, Trimmed L-Moments, L-Comoments, and Many Distributions</li>
<li>matrixdist: Statistics for Matrix Distributions</li>
<li>MatrixModels: Modelling with Sparse and Dense Matrices</li>
<li>matrixStats: Functions that Apply to Rows and Columns of Matrices (and to Vectors)</li>
<li>
<p>minsample2: The Minimum Sample Size</p>
</li>
<li>
<p>mlquantify: Algorithms for Class Distribution Estimation</p>
</li>
<li>mvtnorm: Multivariate Normal and t Distributions</li>
<li>NNS: Nonlinear nonparametric statistics using partial moments</li>
<li>overlapping: Estimation of Overlapping in Empirical Distributions</li>
<li>PCDimension: Finding the Number of Significant Principal Components</li>
<li>philentropy: Similarity and Distance Quantification Between Probability Functions</li>
<li>pls: Partial Least Squares and Principal Component Regression</li>
<li>psre: Presenting Statistical Results Effectively</li>
<li>Qest: Quantile-Based Estimator</li>
<li>qp: Quantile parametrization for probability distribution functions</li>
<li>RcppRoll: Efficient Rolling / Windowed Operations</li>
<li>revss: Robust Estimation in Very Small Samples</li>
<li>RobStatTM: Robust Statistics: Theory and Methods</li>
<li>robustbase: Basic Robust Statistics</li>
<li>roll: Rolling and Expanding Statistics</li>
<li>statsExpressions: Tidy Dataframes and Expressions with Statistical Details</li>
<li>walrus: Robust Statistical Methods</li>
<li>weights: Weighting and Weighted Statistics</li>
</ul>
<h3>7.38 Stress testing, rare events, extreme values and scenarios, survival analysis</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>pyextremes: Extreme Value Analysis</li>
<li>pycox is a python package for survival analysis and time-to-event prediction with PyTorch</li>
<li>scikit-extremes: univariate extreme value calculations</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>BMAmevt: Multivariate Extremes: Bayesian Estimation of the Spectral Measure</li>
<li>climextRemes: Tools for Analyzing Climate Extremes</li>
<li>extRemes: Extreme Value Analysis</li>
<li>extremeStat: Extreme Value Statistics and Quantile Estimation</li>
<li>evd: Functions for Extreme Value Distributions</li>
<li>
<p>evmix: Extreme Value Mixture Modelling, Threshold Estimation and Boundary Corrected Kernel Density
    Estimation</p>
</li>
<li>
<p>ExtremalDep: Extremal Dependence Models</p>
</li>
<li>ExtremeRisks: Extreme Risk Measures</li>
<li>lax: Loglikelihood Adjustment for Extreme Value Models</li>
<li>lite: Likelihood-Based Inference for Time Series Extremes</li>
<li>mev: Modelling of Extreme Values</li>
<li>survivalmodels: Models for Survival Analysis</li>
</ul>
<h3>7.39 Symbolic regression &amp; data-driven model discovery and machine learning</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>2SEGP: Simple Simultaneous Ensemble Learning in Genetic Programming</li>
<li>AIFeynman: Physics-Inspired Method for Symbolic Regression</li>
<li>BindingGP: Symbolic Regression with Dimension Calculation</li>
<li>Data Driven Symbolic Regression</li>
<li>DEAP: Distributed Evolutionary Algorithms</li>
<li>DeepSymReg: Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery</li>
<li>DeepSymRegTorch: PyTorch implementation of the EQL network, a neural network for symbolic regression</li>
<li>Deep symbolic optimization</li>
<li>diffeqpy: Solving differential equations in Python using DifferentialEquations.jl and the SciML Scientific
    Machine Learning organization</li>
<li>ellyn: python-wrapped version of ellen, a linear genetic programming system for symbolic regression and
    classification</li>
<li>EQLearner: A Seq2Seq approach to Symbolic Regression</li>
<li>ffx: Fast Function Extraction for symbolic regressio</li>
<li>geppy: framework for gene expression programming</li>
<li>gplearn: Genetic Programming in Python, with a scikit-learn inspired API</li>
<li>hal-cgp: Cartesian genetic programming</li>
<li>Neural Symbolic Regression That Scales</li>
<li>pyglyph: library based on deap providing abstraction layers for symbolic regression problems</li>
<li>pymbolic: Easy Expression Trees and Term Rewriting</li>
<li>PySR: High-Performance Symbolic Regression in Python</li>
<li>PySINDy: sparse identification of nonlinear dynamical systems from data</li>
<li>pySRURGS: Symbolic regression by uniform random global search</li>
<li>salmon-lm: symbolic algebra of linear regression and modeling</li>
<li>
<p>slearn: package linking symbolic representation with scikit-learn machine learning</p>
</li>
<li>
<p>SR Bench: benchmark framework for symbolic regression</p>
</li>
<li>SymEngine is a fast symbolic manipulation library</li>
<li>symfit: Symbolic Fitting; fitting as it should be.</li>
<li>symbolic experiments: Repository for symbolic regression/classification experiments</li>
<li>Symbolic Regression Boosting</li>
<li>Simpy: symbolic mathematics</li>
<li>symreg: A Symbolic Regression engine</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>DiffEqR: Solving differential equations in R using DifferentialEquations.jl and the SciML Scientific Machine
    Learning ecosystem</li>
<li>gramEvol: Grammatical Evolution for R</li>
<li>symbolicDA: Analysis of Symbolic Data</li>
<li>symengine: Interface to the ’SymEngine’ Library</li>
</ul>
<h3>7.40 Testing (numerical, statistical, etc.), comparison and ranking</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>AutoTS: Automated Time Series Forecasting</li>
<li>darts: toolkit by Unit8 for easy manipulation and forecasting of time series</li>
<li>goftests: Generic goodness of fit tests for random plain old data</li>
<li>hypothesize: hypothesis testing using robust statistics</li>
<li>hypothetical: Hypothesis and statistical testing</li>
<li>hyppo: multivariate hypothesis testing</li>
<li>InvarianceUnitTests: Linear unit-tests for invariance discovery</li>
<li>MAPIE: scikit-learn-compatible module for estimating prediction intervals.</li>
<li>Merlion: A Machine Learning Framework for Time Series Intelligence by SalesForce</li>
<li>permute: permutation tests and confidence sets</li>
<li>PhiK: practical correlation constant that works consistently between categorical, ordinal and interval variables</li>
<li>pingouin: Statistical package in Python based on Pandas</li>
<li>responsible-ai-toolbox: Error Analysis dashboard, for identifying model errors and discovering cohorts of data
    for which the model underperforms.</li>
<li>RunStats: Computing Statistics and Regression in One Pass</li>
<li>scikit-learn: machine learning in Python</li>
<li>statsmodels: statistical modeling and econometrics</li>
<li>UltraNest: Fit and compare complex models reliably and rapidly. Advanced nested sampling.</li>
<li>xskillscore: Metrics for verifying forecasts</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ACV: Optimal Out-of-Sample Forecast Evaluation and Testing under Stationarity</li>
<li>amp: Statistical Test for the Multivariate Point Null Hypotheses</li>
<li>ashr: Methods for Adaptive Shrinkage, using Empirical Bayes</li>
<li>bayefdr: Bayesian Estimation and Optimisation of Expected False Discovery Rate</li>
<li>BEST: Bayesian Estimation Supersedes the t-Test</li>
<li>BFpack: Flexible Bayes Factor Testing of Scientific Expectations</li>
<li>blocklength: Select an Optimal Block-Length to Bootstrap Dependent Data (Block Bootstrap)</li>
<li>boot: Bootstrap Functions</li>
<li>boot.pval: Bootstrap p-Values</li>
<li>bootUR: Bootstrap Unit Root Tests</li>
<li>CADFtest: A Package to Perform Covariate Augmented Dickey-Fuller Unit Root Tests</li>
<li>ChangepointTesting: Change Point Estimation for Clustered Signals</li>
<li>clusrank: Wilcoxon Rank Tests for Clustered Data</li>
<li>cocor: Comparing Correlations</li>
<li>corTESTsrd: Significance Testing of Rank Cross-Correlations under SRD</li>
<li>CovTools: Statistical Tools for Covariance Analysis</li>
<li>crossvalidationCP: Cross-Validation for Change-Point Regression</li>
<li>crseEventStudy: A Robust and Powerful Test of Abnormal Stock Returns in Long-Horizon Event Studies</li>
<li>cvCovEst: Cross-Validated Covariance Matrix Estimation</li>
<li>cvms: Cross-Validation for Model Selection</li>
<li>CVST: Fast Cross-Validation via Sequential Testing</li>
<li>dgof: Discrete Goodness-of-Fit Tests</li>
<li>digitTests: Tests for Detecting Irregular Digit Patterns</li>
<li>DiscreteFDR: Multiple Testing Procedures with Adaptation for Discrete Tests</li>
<li>dsos: Dataset Shift with Outlier Scores</li>
<li>elo: Ranking Teams by Elo Rating and Comparable Methods</li>
<li>energy: E-Statistics: Multivariate Inference via the Energy of Data</li>
<li>exactRankTests: Exact Distributions for Rank and Permutation Tests</li>
<li>FactorAssumptions: Set of Assumptions for Factor and Principal Component Analysis</li>
<li>FAMT: Factor Analysis for Multiple Testing (FAMT) : Simultaneous Tests under Dependence in High-
    Dimensional Data</li>
<li>
<p>fbst: The Full Bayesian Evidence Test, Full Bayesian Significance Test and the e-Value</p>
</li>
<li>
<p>fdrci: Permutation-Based FDR Point and Confidence Interval Estimation</p>
</li>
<li>FDRestimation: Estimate, Plot, and Summarize False Discovery Rates</li>
<li>funtimes: Nonparametric estimators and tests for time series analysis</li>
<li>fwb: Fractional Weighted Bootstrap</li>
<li>fwildclusterboot: Fast Wild Cluster Bootstrap Inference for Linear Models</li>
<li>gvlma: Global Validation of Linear Models Assumptions</li>
<li>gt: Easily Create Presentation-Ready Display Tables</li>
<li>gtExtras: Extending ’gt’ for Beautiful HTML Tables</li>
<li>heplots: Visualizing Hypothesis Tests in Multivariate Linear Models</li>
<li>HSAUR3: A Handbook of Statistical Analyses Using R (3rd Edition)</li>
<li>htestClust: Reweighted Marginal Hypothesis Tests for Clustered Data</li>
<li>ICtest: Estimating and Testing the Number of Interesting Components in Linear Dimension Reduction</li>
<li>inferr: Inferential Statistics (parametric and non-parametric statistical tests)</li>
<li>L2DensityGoFtest: Density Goodness-of-Fit Test</li>
<li>locits: Test of Stationarity and Localized Autocovariance</li>
<li>mashr: Multivariate Adaptive Shrinkage</li>
<li>mcStats: Visualize Results of Statistical Hypothesis Tests</li>
<li>melt: Multiple Empirical Likelihood Tests</li>
<li>metrica: evaluate prediction performance of point-forecast models</li>
<li>MixedIndTests: Tests of Randomness and Tests of Independence</li>
<li>modeltime.resample: Resampling Tools for Time Series Forecasting</li>
<li>MSTest: Hypothesis Testing for Markov Switching Models</li>
<li>multDM: Multivariate Version of the Diebold-Mariano Test</li>
<li>MultiFit: Multiscale Fisher’s Independence Test for Multivariate Dependence</li>
<li>MultiHorizonSPA: Multi Horizon Superior Predictive Ability</li>
<li>multiverse: ’Explorable Multiverse’ Data Analysis and Reports to show the robustness of statistical inference</li>
<li>MVTests: Multivariate Hypothesis Tests and the confidence intervals</li>
<li>nestedcv: Nested Cross-Validation with ’glmnet’ and ’caret’</li>
<li>NonParRolCor: a Non-Parametric Statistical Significance Test for Rolling Window Correlation</li>
<li>OOS: Out-of-Sample Time Series Forecasting</li>
<li>origami: Generalized Framework for Cross-Validation</li>
<li>OptSig: Optimal Level of Significance for Regression and Other Statistical Tests</li>
<li>
<p>OPTtesting: Optimal Testing</p>
</li>
<li>
<p>OutliersO3: Draws Overview of Outliers (O3) Plots</p>
</li>
<li>pbo: Probability of Backtest Overfitting</li>
<li>performance: Assessment of Regression Models Performance</li>
<li>permutes: Permutation Tests for Time Series Data</li>
<li>poolr: Methods for Pooling P-Values from (Dependent) Tests</li>
<li>portes: Portmanteau Tests for Univariate and Multivariate Time Series Models</li>
<li>randtoolbox: Toolbox for Pseudo and Quasi Random Number Generation and Random Generator Tests</li>
<li>RDieHarder: R Interface to the ’DieHarder’ RNG Test Suite</li>
<li>RDnp: Robust Test for Complete Independence in High-Dimensions</li>
<li>rigr: Regression, Inference, and General Data Analysis Tools in R</li>
<li>Rita: Automated Transformations, Normality Testing, and Reporting</li>
<li>rmcorr: Repeated Measures Correlation</li>
<li>RobustANOVA: Robust One-Way ANOVA Tests under Heteroscedasticity and Nonnormality</li>
<li>robusTest: Calibrated Correlation, Two-Sample Tests</li>
<li>rsample: General Resampling Infrastructure</li>
<li>rstatix: Pipe-Friendly Framework for Basic Statistical Tests</li>
<li>s2dverification: Set of Common Tools for Forecast Verification</li>
<li>scoringfunctions: A Collection of Scoring Functions for Assessing Point Forecasts</li>
<li>scoringRules: Scoring Rules for Parametric and Simulated Distribution Forecasts</li>
<li>scoringutils: Utilities for Scoring and Assessing Predictions</li>
<li>sdafilter: distribution free multiple testing rules for false discovery rate (FDR) control under general depen-
    dence</li>
<li>sgof: Multiple Hypothesis Testing</li>
<li>SHT: Statistical Hypothesis Testing Toolbox</li>
<li>slider: Sliding Window Functions</li>
<li>SlidingWindows: Methods for Time Series Analysis</li>
<li>SPlit: Split a Dataset for Training and Testing</li>
<li>splitTools: Tools for Data Splitting</li>
<li>statsExpressions: Tidy Dataframes and tests (parametric, nonparametric, robust, etc)</li>
<li>tidyposterior: Bayesian Analysis to Compare Models using Resampling Statistics</li>
<li>tidystats: Save Output of Statistical Tests</li>
<li>UnitStat: Performs Unit Root Test Statistics</li>
<li>urca: Unit Root and Cointegration Tests for Time Series Data</li>
<li>USP: U-Statistic Permutation Tests of Independence for all Data Types</li>
<li>walrus: Robust Statistical Methods</li>
<li>yardstick: Tidy Characterizations of Model Performance</li>
</ul>
<h3>7.41 Testing software codes</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>benchmark: microbenchmark support library</li>
<li>bugsnag error monitoring and error reporting</li>
<li>case: Python unittest Utilities</li>
<li>cxxtest: CxxTest Unit Testing Framework</li>
<li>dirty-equals: make python code (generally unit tests) more declarative and therefore easier to read and write.</li>
<li>expectest: implements expect tests (also known as ”golden” tests)</li>
<li>formencode: validation and form generation</li>
<li>freezegun: allows your Python tests to travel through time by mocking the datetime module</li>
<li>green: clean, colorful, fast python test runner</li>
<li>Hypothesis: family of testing libraries which let you write tests parametrized by a source of examples</li>
<li>Mamba Test Runner: definitive testing tool for Python</li>
<li>mutattest: Safely run mutation trials without source code modifications and see what will get past your test
    suite.</li>
<li>nose2: unittest with plugins.</li>
<li>nox: Flexible test automation for Python</li>
<li>partialtesting: toolkit by Man Group to run only the tests relevant for code changes</li>
<li>playwright-python: Python version of the Playwright testing and automation library</li>
<li>Pynguin: PYthoN General UnIt Test geNerator</li>
<li>pyperformance: intended to be an authoritative source of benchmarks for all Python implementations</li>
<li>pytest: easy to write small tests, yet scales to support complex functional testing</li>
<li>pytest-benchmark: py.test fixture for benchmarking code</li>
<li>pytest-check: pytest plugin that allows multiple failures per test.</li>
<li>pytest-html: Plugin for generating HTML reports for pytest results</li>
<li>pytest-parallel: pytest plugin for parallel and concurrent testing</li>
<li>pytest-regressions: Pytest plugin for regression testing</li>
<li>stestr: parallel Python test runner built around subunit</li>
<li>TestSlide: test framework by Facebook</li>
<li>testtools: extensions to the Python standard library’s unit testing framework.</li>
<li>tox: Command line driven CI frontend and development task automation tool</li>
<li>ward: modern test framework for Python with a focus on productivity and readability.</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>exampletestr: Help for Writing Unit Tests Based on Function Examples</li>
<li>melt: Multiple Empirical Likelihood Tests</li>
<li>mockthat: Function Mocking for Unit Testing</li>
<li>patrick: Parameterized Unit Testing by Google</li>
<li>realtest: When Expectations Meet Reality: Realistic Unit Testing</li>
<li>shinytest2: Testing for Shiny Applications</li>
<li>testdat: Data Unit Testing for R</li>
<li>testthat: Unit Testing for R</li>
<li>testthis: Utils and ’RStudio’ Addins to Make Testing Even More Fun</li>
<li>ttdo: Extend ’tinytest’ with ’diffobj’</li>
<li>unitizer: Interactive R Unit Tests</li>
<li>unittest: TAP-Compliant Unit Testing</li>
<li>xpectr: Generates Expectations for ’testthat’ Unit Testing</li>
</ul>
<h3>7.42 Time series analysis and modeling</h3>
<p><strong>Collections of resources</strong></p>
<p>List of links:</p>
<ul>
<li>Curated list with python packages for time series analysis</li>
<li>Popular Python Time Series Packages</li>
<li>Resources for working with time series and sequence data</li>
</ul>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>Clairvoyance: Unified, End-to-End AutoML Pipeline for Medical Time Series</li>
<li>darts: toolkit by Unit8 for easy manipulation and forecasting of time series</li>
<li>DataGene: Identify How Similar TS Datasets Are to One Another</li>
<li>deeptime: analysis of time series data including dimensionality reduction, clustering, and Markov model
    estimation</li>
<li>EntropyHub: open-source toolkit for entropic time-series analysis.</li>
<li>ETNA Time Series Library by Tinkoff AI</li>
<li>fastreg: Fast sparse regressions with advanced formula syntax. OLS, GLM, Poisson, Maxlike, and more.
    High-dimensional fixed effects</li>
<li>Featuretools: automated feature engineering</li>
<li>
<p>glum: Generalized linear models</p>
</li>
<li>
<p>hcrystalball: unifies the API for most commonly used libraries and modeling techniques for time-series fore-
    casting in the Python ecosystem</p>
</li>
<li>HyperTools: toolbox for gaining geometric insights into high-dimensional data</li>
<li>HyperTS: Full-Pipeline Automated Time Series (AutoTS) Analysis Toolkit</li>
<li>kats: tookit by Facebook for time series analysis and forecasting</li>
<li>KFAS: Kalman Filter and Smoother for Exponential Family State Space Models</li>
<li>khiva-python: Python binding for Khiva library for time series analytics</li>
<li>Loud ML: inference engine for metrics and events</li>
<li>luminaire: ML driven solutions for monitoring time series data</li>
<li>matrixprofile-ts: detect patterns and anomalies in massive datasets using Matrix Profile</li>
<li>MatrixStats: Methods that Apply to Rows and Columns of Matrices (and to Vectors)</li>
<li>mkl_fft: NumPy-based Python interface to Intel (R) MKL FFT functionality</li>
<li>nixtla: Automated time series processing and forecasting</li>
<li>pandas: data structures for data analysis, time series, and statistics</li>
<li>pyFFTW is a pythonic wrapper around FFTW 3, the speedy FFT library</li>
<li>pyFIt-SNE: FFT-accelerated Interpolation-based t-SNE (FIt-SNE)</li>
<li>pyts: time series classification</li>
<li>pytsal: Time Series analysis, visualization, forecasting along with AutoTS</li>
<li>seglearn: machine learning for time series</li>
<li>sktime: unified framework for machine learning with time series by UK national institute for data science and
    artificial intelligence</li>
<li>slearn: package linking symbolic representation with scikit-learn machine learning</li>
<li>statsmodels: statistical modeling and econometrics</li>
<li>stumpy: variety of time series data mining tasks</li>
<li>theft: Tools for Handling Extraction of Features from Time Series</li>
<li>timemachines: Evaluation and standardization of popular time series packages</li>
<li>timetk: A Tool Kit for Working with Time Series in R</li>
<li>Traces: library for unevenly-spaced time series analysis</li>
<li>tsai: time series tasks like classification, regression, forecasting, imputation</li>
<li>tsam: time series aggregation module (tsam)</li>
<li>ts-eval: Time Series analysis and evaluation tools</li>
<li>tsfresh: extracts relevant characteristics from time series</li>
<li>tslearn: machine learning toolkit dedicated to time-series data</li>
<li>tspreprocess: package to preprocess time series</li>
<li>tsmoothie: time-series smoothing and outlier detection in a vectorized way</li>
<li>vectorbt: library for backtesting and analyzing trading strategies at scale</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ASSA: Applied Singular Spectrum Analysis</li>
<li>astsa: Applied Statistical Time Series Analysis</li>
<li>autostsm: Automatic Structural Time Series Models</li>
<li>bdots: Bootstrapped Differences of Time Series</li>
<li>bfast: Breaks for Additive Season and Trend</li>
<li>bimets: Time Series and Econometric Modeling</li>
<li>bootUR: Bootstrap Unit Root Tests</li>
<li>ctbi: A Procedure to Clean, Decompose and Aggregate Timeseries</li>
<li>energy: E-Statistics: Multivariate Inference via the Energy of Data</li>
<li>entropy: Estimation of Entropy, Mutual Information and Related Quantities</li>
<li>freqdom: Frequency Domain Based Analysis: Dynamic PCA</li>
<li>funtimes: Functions for Time Series Analysis</li>
<li>garchx: Flexible and Robust GARCH-X Modelling</li>
<li>LMD: A Self-Adaptive Approach for Demodulating Multi-Component Signal</li>
<li>LSTS: Locally Stationary Time Series</li>
<li>lubridate: Make Dealing with Dates a Little Easier</li>
<li>mcvis: Multi-Collinearity Visualization</li>
<li>MixedIndTests: Tests of Randomness and Tests of Independence</li>
<li>MTS: All-Purpose Toolkit for Analyzing Multivariate Time Series (MTS) and Estimating Multivariate Volatil-
    ity Models</li>
<li>NonlinearTSA: Nonlinear Time Series Analysis</li>
<li>nonlinearTseries: Nonlinear Time Series Analysis</li>
<li>nortsTest: Assessing Normality of Stationary Process</li>
<li>NTS: Nonlinear Time Series Analysis</li>
<li>Rfssa: Functional Singular Spectrum Analysis</li>
<li>rhosa: Higher-Order Spectral Analysis</li>
<li>rrcov: Scalable Robust Estimators with High Breakdown Point</li>
<li>rrMixture: Reduced-Rank Mixture Models</li>
<li>Rssa: A Collection of Methods for Singular Spectrum Analysis</li>
<li>rtrend: Trend Estimating Tools</li>
<li>Rwave: Time-Frequency Analysis of 1-D Signals</li>
<li>
<p>seastests: Seasonality Tests</p>
</li>
<li>
<p>shrink: Global, Parameterwise and Joint Shrinkage Factor Estimation</p>
</li>
<li>simts: Time Series Analysis Tools</li>
<li>SLBDD: Statistical Learning for Big Dependent Data</li>
<li>svars: Data-Driven Identification of SVAR Models</li>
<li>tempdisagg: Temporal Disaggregation and Interpolation of Time Series</li>
<li>theft: Tools for Handling Extraction of Features from Time Series</li>
<li>TidyDensity: Functions for Tidy Analysis and Generation of Random Data</li>
<li>timetk: A Tool Kit for Working with Time Series in R</li>
<li>TSA: Time Series Analysis</li>
<li>tsbox: Class-Agnostic Time Series</li>
<li>tscopula: Time Series Copula Models</li>
<li>tseries: Time Series Analysis and Computational Finance</li>
<li>TSrepr: Time Series Representations</li>
<li>tsrobprep: Robust Preprocessing of Time Series Data</li>
<li>TSstudio: Functions for Time Series Analysis and Forecasting</li>
<li>tsutils: Time Series Exploration, Modelling and Forecasting</li>
<li>tsviz: Easy and Interactive Time Series Visualization</li>
<li>vars: VAR Modelling</li>
<li>xts: eXtensible Time Series</li>
</ul>
<h3>7.43 Text, sentiment and topic analytics (including NLP)</h3>
<p><strong>Python software implementations</strong></p>
<ul>
<li>AllenNLP: toolkit by Allen Institute of Articial Intelligence for NLP research</li>
<li>EmTract: Extracting Emotions from Social Media Text Tailored for Financial Contexts</li>
<li>EvoMSA: Sentiment Analysis System based on B4MSA and EvoDAG</li>
<li>fairseq: Facebook AI Research Sequence-to-Sequence Toolkit</li>
<li>FastFormers: toolkit by Microsoft to achieve inference of Transformer models for Natural Language Under-
    standing</li>
<li>gensim: topic modelling, document indexing and similarity retrieval with large corpora</li>
<li>GPT-3: Language Models are Few-Shot Learners</li>
<li>LIT: Language Interpretability Tool: Interactively analyze NLP models for model understanding</li>
<li>LangTech Text Library (LTTL) is an open-source python package for text processing and analysis.</li>
<li>Natural Language Processing Best Practices and Examples by Microsoft</li>
<li>
<p>netts: toolkit by UK national institute for data science and artificial intelligence for creating networks cap-
    turing semantic content of speech transcripts</p>
</li>
<li>
<p>nlpaug: Data augmentation for NLP</p>
</li>
<li>nltk: Natural Language Toolkit</li>
<li>pytext: A natural language modeling framework based on PyTorch</li>
<li>PyTorch-NLP: Basic Utilities for PyTorch Natural Language Processing (NLP)</li>
<li>Senta: Baidu’s open-source Sentiment Analysis System.</li>
<li>spaCy: Industrial-strength Natural Language Processing (NLP) in Python</li>
<li>stocksight: Stock market analyzer and predictor using Elasticsearch, Twitter, News headlines, NLP and
    sentiment analysis</li>
<li>sumy: automatic summarization of text documents and HTML pages</li>
<li>textacy: NLP, before and after spaCy</li>
<li>vaderSentiment: VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based
    sentiment analysis tool</li>
<li>wordfreq: Access a database of word frequencies, in various natural languages.</li>
</ul>
<p><strong>R software implementations</strong></p>
<ul>
<li>cleanNLP: Tidy Data Model for Natural Language Processing</li>
<li>doc2concrete: Measuring Concreteness in Natural Language</li>
<li>fastTextR: An Interface to the ’fastText’ Library</li>
<li>globaltrends: Google Trends portal.</li>
<li>lsa: Latent Semantic Analysis</li>
<li>LSX: Model for Semisupervised Text Analysis Based on Word Embeddings</li>
<li>meanr: Sentiment Analysis Scorer</li>
<li>NLP: Natural Language Processing Infrastructure</li>
<li>opitools: Analyzing the Opinions in a Big Text Document</li>
<li>quanteda: Quantitative Analysis of Textual Data</li>
<li>saotd: Sentiment Analysis of Twitter Data</li>
<li>sentiment.ai: Simple Sentiment Analysis Using Deep Learning</li>
<li>SentimentAnalysis: Dictionary-Based Sentiment Analysis</li>
<li>sentimentr: Calculate Text Polarity Sentiment</li>
<li>sentometrics: Integrated Framework for Textual Sentiment Time Series Aggregation and Prediction</li>
<li>spacyr: Wrapper to the ’spaCy’ ’NLP’ Library</li>
<li>sweater: Speedy Word Embedding Association Test and Extras Using R</li>
<li>syuzhet: Extracts Sentiment and Sentiment-Derived Plot Arcs from Text</li>
<li>tau: Text Analysis Utilities</li>
<li>
<p>text2map: R Tools for Text Matrices, Embeddings, and Networks</p>
</li>
<li>
<p>text2sdg: Detecting UN Sustainable Development Goals in Text</p>
</li>
<li>text2vec: Modern Text Mining Framework for R</li>
<li>texter: An Easy Text and Sentiment Analysis Library</li>
<li>TextForecast: Regression Analysis and Forecasting Using Textual Data from a Time-Varying Dictionary</li>
<li>textTinyR: Text Processing for Small or Big Data Files</li>
<li>tidytext: Text Mining using ’dplyr’, ’ggplot2’, and Other Tidy Tools</li>
<li>transforEmotion: Sentiment Analysis for Text and Qualitative Data</li>
<li>tsentiment: Fetching Tweet Data for Sentiment Analysis</li>
<li>Xplortext: Statistical Analysis of Textual Data</li>
</ul>
<h3>7.44 Uncertainty: analysis and modeling.</h3>
<p>Links to resources</p>
<ul>
<li>Professionally curated list of awesome Conformal Prediction videos, tutorials, books, papers, PhD and MSc
    theses, articles and open-source libraries</li>
</ul>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>Bumps: data fitting and uncertainty estimation</li>
<li>conformal-rnn: code for ”Conformal time-series forecasting”, NeurIPS 2021</li>
<li>crepes: Conformal Regressors and Conformal Predictive Systems</li>
<li>EasyVVUQ: verification, validation and uncertainty quantification in high performance computing</li>
<li>EnbPI: Ensemble batch prediction intervals</li>
<li>EnCQR: ensemble conformalized quantile regression (EnCQR)</li>
<li>MAPIE: scikit-learn-compatible module for estimating prediction intervals</li>
<li>mystic: highly-constrained non-convex optimization and uncertainty quantification</li>
<li>OpenTURNS (Open source initiative to Treat Uncertainties, Risks’N Statistics)</li>
<li>PySloth: Probabilistic Prediction</li>
<li>UncertaintyToolbox: predictive uncertainty quantification, calibration, metrics, and visualization</li>
<li>UQpy: UQpy (Uncertainty Quantification with python) is a general purpose Python toolbox for modeling
    uncertainty in physical and mathematical systems</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>bootComb: Combine Parameter Estimates via Parametric Bootstrap</li>
</ul>
<h3>7.45 Visualization and reporting</h3>
<p><strong>Python software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>Algviz is an algorithm visualization tool for your Python code</li>
<li>appmode: Jupyter extension that turns notebooks into web applications</li>
<li>Best of Streamlit</li>
<li>clustergram: Visualization and diagnostics for cluster analysis in Python</li>
<li>dash: framework for building ML and data science web apps</li>
<li>dash-extensions:extensions to the Plotly Dash framework</li>
<li>D-tale:Visualizer by Man Group for pandas data structures</li>
<li>FlameScope: visualization ny Netflix for exploring different time ranges as Flame Graphs.</li>
<li>HyperTools: toolbox for gaining geometric insights into high-dimensional data</li>
<li>ipyslides: Create Interactive Slides in Jupyter Notebook with all kind of rich content</li>
<li>itables: Pandas DataFrames as Interactive DataTables</li>
<li>Lux: automate the visualization and data analysis process</li>
<li>Markdown: Python implementation of markdown</li>
<li>matplotlib: omprehensive library for creating static, animated, and interactive visualizations</li>
<li>mpl-animators: interative animation framework for matplotlib</li>
<li>Orange: Interactive data analysis</li>
<li>plotly: graphing library makes interactive, publication-quality graphs</li>
<li>Plotly Resampler: Visualize large time-series data in plotly</li>
<li>plotnine: A grammar of graphics for Python</li>
<li>plottable: Beautifully customized tables with matplotlib</li>
<li>psyplot: interactive data visualization</li>
<li>PyGraphistry: quickly load, shape, embed, and explore big graphs with the GPU-accelerated Graphistry
    visual graph analyzer</li>
<li>PyMetis: Python wrapper around Metis, a graph partitioning package</li>
<li>PyShiny: Shiny for Python</li>
<li>pyvis: visualizing interactive network graphs</li>
<li>seaborn: statistical data visualization</li>
<li>seaborn analyzer: data analysis and visualization tool using Seaborn library</li>
<li>streamlit: fastest way to build and share data apps</li>
<li>tensorboard: TensorFlow’s Visualization Toolkit</li>
<li>
<p>torchsde: Differentiable SDE solvers with GPU support and efficient sensitivity analysis</p>
</li>
<li>
<p>tourr: Tour Methods for Multivariate Data Visualisation</p>
</li>
<li>Vega-Altair is a declarative statistical visualization library for Pytho</li>
<li>VisPy: interactive scientific visualization in Python</li>
<li>visdom: lexible tool for creating, organizing, and sharing visualizations of live, rich data</li>
</ul>
<p><strong>R software implementations</strong></p>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>apexcharter: Create Interactive Chart with the JavaScript ’ApexCharts’ Library</li>
<li>autoplotly: Automatic Generation of Interactive Visualizations for Statistical Results</li>
<li>classmap: Visualizing Classification Results</li>
<li>cleanrmd: Clean Class-Less ’R Markdown’ HTML Documents</li>
<li>clustree: Visualise Clusterings at Different Resolutions</li>
<li>ComplexUpset: Create Complex UpSet Plots Using ’ggplot2’ Components</li>
<li>condformat: Conditional Formatting in Data Frames</li>
<li>conductor: Create Tours in ’Shiny’ Apps Using ’Shepherd.js’</li>
<li>d3po: Fast and Beautiful Interactive Visualization for ’Markdown’ and ’Shiny’</li>
<li>DataVisualizations: Visualizations of High-Dimensional Data</li>
<li>descriptr: Generate Descriptive Statistics</li>
<li>DT: A Wrapper of the JavaScript Library ’DataTables’</li>
<li>echarty: Minimal R/Shiny Interface to JavaScript Library ’ECharts’</li>
<li>esquisse: Explore and Visualize Your Data Interactively</li>
<li>fmtr: Easily Apply Formats to Data</li>
<li>ggalluvial: Alluvial Plots in ’ggplot2’</li>
<li>GGally: Extension to ’ggplot2’</li>
<li>gganimate: A Grammar of Animated Graphics</li>
<li>ggbreak: Set Axis Break for ’ggplot2’</li>
<li>ggcharts: Shorten the Distance from Data Visualization Idea to Actual Plot</li>
<li>ggcorrplot: Visualization of a Correlation Matrix using ’ggplot2’</li>
<li>ggcorset: The Corset Plot</li>
<li>ggdag: Analyze and Create Elegant Directed Acyclic Graphs</li>
<li>ggdist: Visualizations of Distributions and Uncertainty</li>
<li>ggDoubleHeat: A Heatmap-Like Visualization Tool</li>
<li>ggeffects: Create Tidy Data Frames of Marginal Effects for ’ggplot’ from Model Outputs</li>
<li>
<p>ggESDA: Exploratory Symbolic Data Analysis with ’ggplot2’</p>
</li>
<li>
<p>ggfocus: Scales that Focus Specific Levels in your ggplot</p>
</li>
<li>ggforce: Accelerating ’ggplot2’</li>
<li>ggformula: Formula Interface to the Grammar of Graphics</li>
<li>ggfortify: Data Visualization Tools for Statistical Analysis Results</li>
<li>gghdr: Visualisation of Highest Density Regions in ’ggplot2’</li>
<li>ggheatmap: Plot Heatmap</li>
<li>gghighlight: Highlight Lines and Points in ’ggplot2’</li>
<li>ggh4x: Hacks for ’ggplot2’</li>
<li>ggiraph: Make ’ggplot2’ Graphics Interactive</li>
<li>ggmatplot: Plot Columns of Two Matrices Against Each Other Using ’ggplot2’</li>
<li>ggmice: Visualizations for ’mice’ with ’ggplot2’</li>
<li>ggmosaic: Mosaic Plots in the ’ggplot2’ Framework</li>
<li>ggmulti: High Dimensional Data Visualization</li>
<li>ggnetwork: Geometries to Plot Networks with ’ggplot2’</li>
<li>ggpattern: ’ggplot2’ Pattern Geoms</li>
<li>ggpie: pie, donut and rose pie plots with ggplot2</li>
<li>ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics</li>
<li>ggplotify: Convert Plot to ’grob’ or ’ggplot’ Object</li>
<li>ggpmisc: Miscellaneous Extensions to ’ggplot2’</li>
<li>ggpubr: ’ggplot2’ Based Publication Ready Plots</li>
<li>ggpval: Annotate Statistical Tests for ’ggplot2’</li>
<li>ggquickeda: Quickly Explore Your Data Using ’ggplot2’ and ’table1’ Summary Tables</li>
<li>ggside extends ’ggplot2’ by allowing users to add graphical information about one of the main panel’s axis
    using a familiar ’ggplot2’ style API with tidy data</li>
<li>ggsignif: Significance Brackets for ’ggplot2’</li>
<li>ggstance: Horizontal ’ggplot2’ Components</li>
<li>ggstar: Multiple Geometric Shape Point Layer for ’ggplot2’</li>
<li>ggstatsplot: ’ggplot2’ Based Plots with Statistical Details</li>
<li>ggthemes: Extra Themes, Scales and Geoms for ’ggplot2’</li>
<li>ggtrace: Provides ggplot2 geoms that allow groups of data points to be outlined or highlighted for emphasis</li>
<li>gluedown: Wrap Vectors in Markdown Formatting</li>
<li>gridstackeR: easy way to create responsive layouts with just a few lines of code using gridstack.js</li>
<li>gt: Easily Create Presentation-Ready Display Tables</li>
<li>
<p>gtExtras: additional functions for creating tables with gt</p>
</li>
<li>
<p>gtsummary: Presentation-Ready Data Summary and Analytic Result Tables</p>
</li>
<li>heatmaply: Interactive Cluster Heat Maps Using ’plotly’ and ’ggplot2’</li>
<li>heplots: Visualizing Hypothesis Tests in Multivariate Linear Models</li>
<li>htmlTable: Advanced Tables for Markdown/HTML</li>
<li>huxtable: Easily Create and Style Tables for LaTeX, HTML and Other Formats</li>
<li>jjAnno: An Annotation Package for ’ggplot2’ Output</li>
<li>kableExtra: Construct Complex Table with ’kable’ and Pipe Syntax</li>
<li>listdown: Create R Markdown from Lists</li>
<li>loon: Interactive Statistical Data Visualization</li>
<li>loon.ggplot: A Grammar of Interactive Graphics</li>
<li>magick: Advanced Graphics and Image-Processing in R</li>
<li>memoiR: R Markdown and Bookdown Templates to Publish Documents</li>
<li>ndtv: Network Dynamic Temporal Visualizations</li>
<li>numform: Tools to Format Numbers for Publication</li>
<li>performance: Assessment of Regression Models Performance</li>
<li>plot.matrix: Visualizes a Matrix as Heatmap</li>
<li>presenter: Present Data with Style</li>
<li>prompter: Add Tooltips in ’Shiny’ Apps with ’Hint.css’</li>
<li>psre: Presenting Statistical Results Effectively</li>
<li>quarto: R Interface to ’Quarto’ Markdown Publishing System</li>
<li>r2resize: In-Text Resizing for Containers, Images and Data Tables in ’Shiny’, ’Markdown’ and ’Quarto’
    Documents</li>
<li>r3js: allow WebGL-based 3D plotting using the three.js library</li>
<li>reactR: Make it easy to use ’React’ in R with ’htmlwidget’ scaffolds</li>
<li>reporter: Creates Statistical Reports</li>
<li>rheroicons: A Zero Dependency ’SVG’ Icon Library for ’Shiny’</li>
<li>rhino: A Framework for Enterprise Shiny Applications</li>
<li>rintrojs: Wrapper for the ’Intro.js’ Library</li>
<li>rmarkdown: Dynamic Documents for R</li>
<li>rsvg: Render SVG Images into PDF, PNG, (Encapsulated) PostScript, or Bitmap Arrays</li>
<li>semantic.dashboard: Dashboard with Fomantic UI Support for Shiny</li>
<li>shapviz: visualize SHapley Additive exPlanations (SHAP) - waterfall, force, importance, dependence plots</li>
<li>shiny: Web Application Framework for R</li>
<li>
<p>shinyChakraUI: A Wrapper of the ’React’ Library ’Chakra UI’ for ’Shiny’</p>
</li>
<li>
<p>shinydlplot: Add a Download Button to a ’shiny’ Plot or ’plotly’</p>
</li>
<li>shinyHugePlot: Efficient Plotting of Large-Sized Data</li>
<li>shinyMobile: Mobile Ready ’shiny’ Apps with Standalone Capabilities</li>
<li>shinySelect: A Wrapper of the ’react-select’ Library</li>
<li>shiny.semantic: Semantic UI Support for Shiny</li>
<li>shinytest: Test Shiny Apps</li>
<li>shinyWidgets: Custom Inputs Widgets for Shiny</li>
<li>starry: Explore Data with Plots and Tables</li>
<li>statsExpressions: Tidy Dataframes and Expressions with Statistical Details</li>
<li>sugrrants: Supporting Graphs for Analysing Time Series</li>
<li>tidybayes: Tidy Data and ’Geoms’ for Bayesian Models</li>
<li>tidycharts: Generate Tidy Charts Inspired by ’IBCS’</li>
<li>tidyHeatmap: A Tidy Implementation of Heatmap</li>
<li>tourr: Tour Methods for Multivariate Data Visualisation</li>
<li>tornado: Plots for Model Sensitivity and Variable Importance</li>
<li>trelliscopejs: Create Interactive Trelliscope Displays</li>
<li>tsviz: Easy and Interactive Time Series Visualization</li>
<li>UpSetR: A More Scalable Alternative to Venn and Euler Diagrams for Visualizing Intersecting Sets</li>
<li>visNetwork: Network Visualization using ’vis.js’ Library</li>
<li>visStatistics: Automated Visualization of Statistical Tests</li>
<li>vtable: Variable Table for Variable Documentation</li>
<li>xaringan: Presentation Ninja</li>
<li>yardstick: Tidy Characterizations of Model Performance</li>
</ul>
<h2>8 Codes for QWIM (Quantitative Wealth and Investment Management)</h2>
<h2>ment)</h2>
<h3>8.1 Collections of resources</h3>
<p>List of links:</p>
<ul>
<li>Curated list of practical financial machine learning tools and applications</li>
<li>EliteQuant: online resources for quantitative modeling, trading, portfolio management</li>
</ul>
<h3>8.2 Research studies with code</h3>
<p>Ardia et al. (“RiskPortfolios: Computation of Risk-Based Portfolios in R,” 2017)
Boileau et al. (“cvCovEst: Cross-validated covariance matrix estimator selection and evaluation in R,” 2021)
Brugiere ( <em>Quantitative Portfolio Management with Applications in Python</em> , 2020)
Bryzgalova et al. (“Forest Through the Trees: Building Cross-Sections of Stock Returns,” 2021)
Cajas (“Entropic Portfolio Optimization: a Disciplined Convex Programming Framework,” 2021)
Cajas (“OWA Portfolio Optimization: a Disciplined Convex Programming Framework,” 2021)
Chen and Zimmermann (“Open Source Cross-Sectional Asset Pricing,” 2022)
Chib ( <em>R package czfactor</em> , 2020)
Chib and Zhao ( <em>R package czzg</em> , 2020)
Coqueret and Guida ( <em>Machine Learning for Factor Investing: R Version</em> , 2020)
Coqueret ( <em>Perspectives in sustainable equity investing (website version)</em> , 2022)
de Carvalho and Rua (“Real-time nowcasting the US output gap: Singular spectrum analysis at work,” 2017)
Ding et al. (“A Python package for multi-stage stochastic programming,” 2020)
Dixon et al. ( <em>Machine Learning in Finance: from theory to practice</em> , 2020)
Dixon and Polson (“Deep Fundamental Factor Models,” 2020)
Dong et al. (“Anomalies and the expected market return,” 2022)
Guijarro-Ordonez et al. (“Deep Learning Statistical Arbitrage,” 2021)
Gurdogan and Kercheval (“Multi Anchor Point Shrinkage for the Sample Covariance Matrix (Extended Ver-
sion),” 2021)
Ho et al. (“Moving beyond P values: data analysis with estimation graphics,” 2019)
Irlam (“Multi Scenario Financial Planning via Deep Reinforcement Learning AI,” 2020)
Irlam ( <em>AI Planner</em> , 2020)
Irlam (“Machine learning for retirement planning,” 2020)
Jansen ( <em>Machine Learning for Algorithmic Trading (Second Edition)</em> , 2020)
Kakushadze and Yu (“Statistical Risk Models,” 2016)
Kakushadze and Yu (“Open Source Fundamental Industry Classification,” 2017)
Kakushadze and Yu (“Betas, Benchmarks, and Beating the Market,” 2018)
Kakushadze and Yu (“Decoding stock market with quant alphas,” 2018)
Kakushadze and Yu (“Machine learning risk models,” 2019)
Kakushadze and Yu (“Machine learning treasury yields,” 2020)
Lai et al. (“TODS: An Automated Time Series Outlier Detection System,” 2021)
Lettau and Pelger (“Factors That Fit the Time Series and Cross-Section of Stock Returns,” 2020)
Li et al. (“FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative
Finance,” 2021)
Liu et al. (“FinRL: Deep Reinforcement Learning Framework to Automate Trading in Quantitative Finance,”
2021)
Liu et al. (“FinRL-Meta: A Universe of Near-Real Market Environments for Data-Driven Deep Reinforcement
Learning in Quantitative Finance,” 2022)
Marinescu (“Risk-Based Optimal Portfolio Strategies: A Compendium,” 2022)
Martin (“PyPortfolioOpt: portfolio optimization in Python,” 2021)
Marwood and Minnen (“Safely Boosting Retirement Income by Harmonizing Drawdown Paths,” 2020)
McIndoe (“A Data Driven Approach to Market Regime Classification,” 2020)
Micheli and Neuman (“Evidence of Crowding on Russell 3000 Reconstitution Events,” 2022)
Milevsky ( <em>Retirement Income Recipes in R: From Ruin Probabilities to Intelligent Drawdowns</em> , 2020)
Qian et al. (“Combining forecasts for universally optimal performance,” 2022)
Rao and Jelvis ( <em>Foundations of Reinforcement Learning with Applications in Finance</em> , 2022)
Sarmas et al. ( <em>Multicriteria Portfolio Construction with Python</em> , 2020)
Sharma et al. (“DoWhy: Addressing Challenges in Expressing and Validating Causal Assumptions,” 2021)
Shi et al. (“Deep Learning Algorithms for Hedging with Frictions,” 2022)
Siebert et al. (“A systematic review of Python packages for time series analysis,” 2021)
Simos et al. (“Time-varying Black–Litterman portfolio optimization using a bio-inspired approach and neu-
ronets,” 2021)
Snow (“Machine learning in asset management,” 2019)</p>
<p>Snow (“Machine Learning in Asset Management Part 1: Portfolio Construction Trading Strategies,” 2020)
Snow (“Machine Learning in Asset Management - Part 2: Portfolio Construction - Weight Optimization,” 2020)
Tatsat et al. ( <em>Machine Learning and Data Science Blueprints for Finance: From Building Trading Strategies to
Robo-Advisors Using Python</em> , 2020)
Tuck et al. (“Portfolio Construction Using Stratified Models,” 2022)
Ungolo et al. (“affine_mortality: A Github repository for estimation, analysis, and projection of affine mortality
models,” 2021)
Vamossy and Skog (“EmTract: Investor Emotions and Market Behavior,” 2021)
Vinod (“R Package GeneralCorr Functions for Portfolio Choice,” 2021)
Yang et al. (“FinBERT: A Pretrained Language Model for Financial Communications,” 2020)
Yu et al. (“An AI approach to measuring financial risk,” 2020)</p>
<h3>8.3 Python software implementations.</h3>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>alive-progress: new kind of Progress Bar, with real-time throughput, ETA, and very cool animations</li>
<li>alphalens: Performance analysis of predictive (alpha) stock factors</li>
<li>AlphaPy: Automated Machine Learning [AutoML] with Python, scikit-learn, Keras, XGBoost, LightGBM,
    and CatBoost</li>
<li>auquantoolbox: Backtesting toolbox for trading strategies</li>
<li>azapy: Financial Portfolio Optimization Algorithms</li>
<li>bt: flexible backtesting framework</li>
<li>btgym: Scalable, event-driven, deep-learning-friendly backtesting library</li>
<li>Clairvoyant: identify and monitor social/historical cues for short term stock movement</li>
<li>CVXPortfolio: Portfolio optimization and simulation</li>
<li>cyanure: Toolbox for Empirical Risk Minimization</li>
<li>Elegant-FinRL: algorithmic strategies using Reinforcement Learning</li>
<li>Empyrial: AI and data-driven quantitative portfolio management for risk and performance analytics</li>
<li>empyrical: Common financial risk and performance metrics</li>
<li>EmTract: Extracting Emotions from Social Media Text Tailored for Financial Contexts</li>
<li>ffn: Financial functions for Python</li>
<li>FinDataPy: download market data via Bloomberg, Eikon, Quandl, Yahoo etc.</li>
<li>FinMarketPy: backtesting trading strategies and analyzing financial markets</li>
<li>finnhub-python: Finnhub Python API Client to provide financial data(real-time stock price, global funda-
    mentals, global ETFs holdings and alternative data)</li>
<li>FinRL: Deep Reinforcement Learning for Quantitative Finance</li>
<li>FinRL-Meta: Universe of Near-Real Market Environments for Data-Driven Financial Reinforcement Learning</li>
<li>fredapi is a Python interface to the Federal Reserve Economic Data (FRED) and ALFRED databases</li>
<li>lifelib: Actuarial models in Python</li>
<li>
<p>lifelines: Survival analysis in Python, including Kaplan Meier, Nelson Aalen and regression</p>
</li>
<li>
<p>lrsm_portfolio: Portfolio Construction using Stratified Models</p>
</li>
<li>Machine Learning and Data Science Blueprints for Finance (codes for the book)</li>
<li>Machine Learning fior asset management</li>
<li>Machine Learning for Algorithmic Trading (codes for the book)</li>
<li>MLFinLab: Machine Learning Financial Laboratory</li>
<li>momentum: Running mean, variance, skew, and kurtosis</li>
<li>Multicriteria Portfolio Construction with Python</li>
<li>okama: investment portfolio analyzing and optimization tools</li>
<li>OpenBBTerminal: modern Python-based integrated environment for investment research</li>
<li>OptimalPortfolio: portfolio optimization</li>
<li>QuantAxis: Quantitative Financial FrameWork</li>
<li>QuantEcon: quantitative economics</li>
<li>Pandas TA: Technical Analysis Indicators</li>
<li>portfolio-backtest: backtest portfolio asset allocation</li>
<li>precise: online covariance and precision forecasting, portfolios, and model ensembles</li>
<li>predictionrevisited: implements the core statistical concepts from the book ”Prediction Revisited: The Im-
    portance of Observation”</li>
<li>pyfinance: general financial and security returns analysis</li>
<li>pyfolio: Portfolio and risk analytics in Python</li>
<li>pyhrp: hierarchical risk parity algorithms</li>
<li>PyPortfolioOpt: Financial portfolio optimisation</li>
<li>Qlib: Microsoft AI-oriented quantitative investment platform</li>
<li>QuantEcon.py: quantitative economics</li>
<li>QuantLib: Python bindings for the QuantLib library</li>
<li>QuantResearch: Quantitative analysis, strategies and backtests</li>
<li>Quantropy: Financial pipeline for the data-driven investor to research, develop and deploy robust strategie</li>
<li>QuantStats: Portfolio analytics for quants</li>
<li>Riskfolio-Lib: Portfolio Optimization and Quantitative Strategic Asset Allocation</li>
<li>Robust Risk-aware reinforcement learning</li>
<li>stocksight: Stock market analyzer and predictor using Elasticsearch, Twitter, News headlines, NLP and
    sentiment analysis</li>
<li>ta: Technical Analysis Library using Pandas and Numpy</li>
<li>TA-lib: Python wrapper for TA-Lib Technical Analysis Library</li>
<li>Tax-Calculator: USA Federal Individual Income and Payroll Tax Microsimulation Model</li>
<li>tf-quant-finance: High-performance TensorFlow library by Google for quantitative finance.</li>
<li>vectorbt: Supercharged backtesting and technical analysis for quants</li>
<li>zipline: Algorithmic Trading Library</li>
</ul>
<h3>8.4 R software implementations</h3>
<p>List of packages and/or codes and/or frameworks and/or links:</p>
<ul>
<li>ASSA: Applied Singular Spectrum Analysis</li>
<li>AssetAllocation: Backtesting Simple Asset Allocation Strategies</li>
<li>BEKKs: Multivariate Conditional Volatility Modelling and Forecasting</li>
<li>crseEventStudy: A Robust and Powerful Test of Abnormal Stock Returns in Long-Horizon Event Studies</li>
<li>DOSPortfolio: Dynamic Optimal Shrinkage Portfolio</li>
<li>ExtremeRisks: Extreme Risk Measures</li>
<li>FFdownload: Download Data from Kenneth French’s Website</li>
<li>FinnTS: Microsoft Finance Time Series Forecasting Framework</li>
<li>finreportr: Financial Data from U.S. Securities and Exchange Commission</li>
<li>fHMM: Fitting Hidden Markov Models to Financial Data</li>
<li>FinnTS: Microsoft Finance Time Series Forecasting Framework</li>
<li>fitHeavyTail: Mean and Covariance Matrix Estimation under Heavy Tails</li>
<li>fixedincome: Fixed Income Models, Calculations, Data Structures and Instruments</li>
<li>generalCorr: Generalized Correlations, Causal Paths and Portfolio Selection</li>
<li>greeks: Sensitivities of Prices of Financial Options</li>
<li>HDShOP: High-Dimensional Shrinkage Optimal Portfolios</li>
<li>HierPortfolios: Hierarchical Clustering-Based Portfolio Allocation Strategies</li>
<li>highOrderPortfolios: Design of High-Order Portfolios via Mean, Variance, Skewness, and Kurtosis</li>
<li>imputeFin: Imputation of Financial Time Series with Missing Values and/or Outliers</li>
<li>MarkowitzR: Statistical Significance of the Markowitz Portfolio</li>
<li>MortCast: Estimation and Projection of Age-Specific Mortality Rates</li>
<li>parma: Portfolio Allocation and Risk Management Applications</li>
<li>pbo: Probability of Backtest Overfitting</li>
<li>pec: Prediction Error Curves for Risk Prediction Models in Survival Analysis</li>
<li>pedquant: Public Economic Data and Quantitative Analysis</li>
<li>PerformanceAnalytics: Econometric Tools for Performance and Risk Analysis</li>
<li>PortfolioAnalytics: Portfolio Analysis, Including Numerical Methods for Optimization of Portfolios</li>
<li>portfolioBacktest: Automated Backtesting of Portfolios over Multiple Datasets</li>
<li>portvine: portfolio level risk estimates using ARMA-GARCH and vine copulas</li>
<li>priceR: Economics and Pricing Tools</li>
<li>
<p>qlcal: R Bindings to the Calendaring Functionality of ’QuantLib’</p>
</li>
<li>
<p>qrmtools: Tools for Quantitative Risk Management</p>
</li>
<li>quantmod: Quantitative Financial Modelling Framework</li>
<li>RcppQuantuccia: R Bindings to the Calendaring Functionality of ’QuantLib’</li>
<li>riskParityPortfolio: Design of Risk Parity Portfolios</li>
<li>RiskPortfolios: Computation of Risk-Based Portfolios</li>
<li>riskRegression: Risk Regression Models and Prediction Scores for Survival Analysis with Competing Risks</li>
<li>RPESE: Estimates of Standard Errors for Risk and Performance Measures</li>
<li>RQuantLib: R Interface to the ’QuantLib’ Library</li>
<li>SharpeR: Statistical Significance of the Sharpe Ratio</li>
<li>sharpeRratio: Moment-Free Estimation of Sharpe Ratios</li>
<li>sparseIndexTracking: Design of Portfolio of Stocks to Track an Index</li>
<li>SWIM: Scenario Weights for Importance Measurement</li>
<li>TextForecast: Regression Analysis and Forecasting Using Textual Data from a Time-Varying Dictionary</li>
<li>tidyquant: Tidy Quantitative Financial Analysis</li>
<li>Trading: CCR, Advanced Correlation &amp; Beta Estimates, Betting Strategies</li>
<li>tseries: Time Series Analysis and Computational Finance</li>
<li>ufRisk: Risk Measure Calculation in Financial TS</li>
<li>usincometaxes: wrapper to the NBER’s TAXSIM 35 tax simulator for federal and state income taxes</li>
<li>ycevo: Nonparametric Estimation of the Yield Curve Evolution</li>
<li>yfR: Downloads and Organizes Financial Data from Yahoo Finance</li>
</ul>
<h2>References</h2>
<p>Abhyankar, A. and Wu, Y. (2020).“Circus Ring to Zoo to Museum: The Fragility of Factors in Characteristic-based
Asset Pricing Models.” In: <em>SSRN e-Print</em>.
Ahmed, S., Bu, Z., and Tsvetanov, D. (2019).“Best of the Best: A Comparison of Factor Models.” In: <em>Journal of
Financial and Quantitative Analysis</em> 54(4), pp. 1713–1758.
Alexander, C., Coulon, M., Han, Y., and Meng, X. (2021).“Evaluating the Discrimination Ability of Proper Mul-
tivariate Scoring Rules.” In: <em>arXiv e-Print</em>.
Alexandrov, A., Benidis, K., Bohlke-Schneider, M., Flunkert, V., Gasthaus, J., Januschowski, T., Maddix, D. C.,
Rangapuram, S., Salinas, D., Schulz, J., Stella, L., Turkmen, A. C., and Wang, Y. (2020).“GluonTS: Probabilistic
and Neural Time Series Modeling in Python.” In: <em>Journal of Machine Learning Research</em> 21(116), pp. 1–6.
Alhnaity, B. and Abbod, M. (2020).“A new hybrid financial time series prediction model.” In: <em>Engineering Appli-
cations of Artificial Intelligence</em> 95, p. 103873.
Allende, H. and Valle, C. (2017).“Ensemble methods for time series forecasting.” In: <em>Claudio Moraga: A Passion for
Multi-Valued Logic and Soft Computing</em>. Ed. by R. Seising and H. Allende-Cid. Vol. 349. Springer International
Publishing, pp. 217–232.
Anghel, D. G. (2021).“Data Snooping Bias in Tests of the Relative Performance of Multiple Forecasting Models.”
In: <em>Journal of Banking &amp; Finance</em> 126, p. 106113.
Ankile, L. L. and Krange, K. (2022). “The DONUT Approach to Ensemble Combination Forecasting.” In: <em>arXiv
e-Print</em>.
Ansari, A. F., Benidis, K., Kurle, R., Turkmen, A. C., Soh, H., Smola, A. J., Wang, Y., and Januschowski, T.
(2021). “Deep Explicit Duration Switching Models for Time Series.” In: <em>arXiv e-Print</em>.
Aparicio, D. and Lopez de Prado, M. (2018).“How Hard Is It to Pick the Right Model? MCS and backtest
overfitting.” In: <em>Algorithmic Finance</em> 7, pp. 53–61.
Ardia, D., Boudt, K., and Gagnon-Fleury, J.-P. (2017).“RiskPortfolios: Computation of Risk-Based Portfolios in
R.” In: <em>The Journal of Open Source Software</em> 2(10), pp. 171+.
Ardia, D. and Dufays, A. (2021).“Measuring uncertainty and uncertainty dispersion from a large set of model
predictions.” In: <em>SSRN e-Print</em>.
Arias-Calluari, K., Alonso-Marroquin, F., Najafi, M. N., and Harré, M. (2021).“Methods for forecasting the effect
of exogenous risks on stock markets.” In: <em>Physica A: Statistical Mechanics and its Applications</em> 568, p. 125587.
Arnold, S., Henzi, A., and Ziegel, J. F. (2022). “Sequentially valid tests for forecast calibration.” In: <em>arXiv e-Print</em>.
Arvanitis, S., Post, T., Potı̀, V., and Karabati, S. (2021).“Nonparametric tests for Optimal Predictive Ability.” In:
<em>International Journal of Forecasting</em> 37(2), pp. 881–898.
Ashouri, M., Hyndman, R. J., and Shmueli, G. (2022).“Fast Forecast Reconciliation Using Linear Models.” In:
<em>Journal of Computational and Graphical Statistics</em>.
Athanasopoulos, G., Gamakumara, P., Panagiotelis, A., Hyndman, R. J., and Affan, M. (2019).“Hierarchical
Forecasting.” In: <em>Macroeconomic Forecasting in the Era of Big Data</em>. Springer International Publishing, pp. 689–
719.
Athanasopoulos, G. and Kourentzes, N. (2020). <em>On the evaluation of hierarchical forecasts</em>. Tech. rep. Monash
University.
Atiya, A. F. (2020).“Why does forecast combination work so well?” In: <em>International Journal of Forecasting</em> 36(1),
pp. 197–200.
Babiak, M. and Barunik, J. (2020).“Deep Learning, Predictability, and Optimal Portfolio Returns.” In: <em>SSRN
e-Print</em>.
Babii, A., Ball, R. T., Ghysels, E., and Striaukas, J. (2021).“Machine Learning Panel Data Regressions with
Heavy-tailed Dependent Data: Theory and Application.” In: <em>SSRN e-Print</em>.
Bacher, P., Bergsteinsson, H. G., Frolke, L., Sorensen, M. L., Lemos-Vinasco, J., Liisberg, J., Moller, J. K., Nielsen,
H. A., and Madsen, H. (2022). “onlineforecast: An R package for adaptive and recursive forecasting.” In: <em>arXiv
e-Print</em>.
Bacon, C. R. (2019).“Performance Attribution: History and Progress.” In: <em>CFA Institute Research Foundation
Publications</em>.
Bahrami, A., Shamsuddin, A., and Uylangco, K. (2019).“Are advanced emerging market stock returns predictable?
A regime-switching forecast combination approach.” In: <em>Pacific-Basin Finance Journal</em> 55, pp. 142–160.</p>
<p>Bailey, N., Kapetanios, G., and Pesaran, M. H. (2020).“Measurement of Factor Strength: Theory and Practice.”
In: <em>SSRN e-Print</em>.
Baitinger, E. and Flegel, S. (2021).“New Concepts in Financial Forecasting: Network-Based Information, Topolog-
ical Data Analysis and their Combination.” In: <em>SSRN e-Print</em>.
Bali, T. G., Goyal, A., Huang, D., Jiang, F., and Wen, Q. (2021).“Different Strokes: Return Predictability Across
Stocks and Bonds with Machine Learning and Big Data.” In: <em>SSRN e-Print</em>.
Baltas, N. and Karyampas, D. (2020).“Forecasting the Equity Risk Premium: The Importance of Regime-Dependent
Evaluation.” In: <em>SSRN e-Print</em>.
Baltussen, G., Martens, M., and Penninga, O. (2020).“Predicting Bond Returns: 70 Years of International Evidence.”
In: <em>SSRN e-Print</em>.
Baltussen, G., Martens, M., and Penninga, O. (2021).“Predicting Bond Returns: 70 Years of International Evidence.”
In: <em>Financial Analysts Journal</em> 77(3), pp. 133–155.
Bandara, K., Bergmeir, C., and Smyl, S. (2020).“Forecasting across time series databases using recurrent neu-
ral networks on groups of similar series: A clustering approach.” In: <em>Expert Systems with Applications</em> 140,
pp. 112896+.
Bandara, K., Hewamalage, H., Liu, Y.-H., Kang, Y., and Bergmeir, C. (2021).“Improving the accuracy of global
forecasting models using time series data augmentation.” In: <em>Pattern Recognition</em> 120, p. 108148.
Barillas, F. and Shanken, J. (2019).“Real-time Portfolio Choice Implications of Asset Pricing Models.” In: <em>Consor-
tium on Factor Investing Conference</em>. Vol. 73.
Barras, L. (2019).“A large-scale approach for evaluating asset pricing models.” In: <em>Journal of Financial Economics</em>
134(3), pp. 549–569.
Barrow, D. K. and Crone, S. F. (2016).“A comparison of AdaBoost algorithms for time series forecast combination.”
In: <em>International Journal of Forecasting</em> 32(4), pp. 1103–1119.
Bates, S., Hastie, T., and Tibshirani, R. (2021).“Cross-validation: what does it estimate and how well does it do
it?” In: <em>arXiv e-Print</em>.
Bauer, A., Zufle, M., Herbst, N., Kounev, S., and Curtef, V. (2020).“Telescope: An Automatic Feature Extraction
and Transformation Approach for Time Series Forecasting on a Level-Playing Field.” In: <em>Proceedings of the 36th
International Conference on Data Engineering (ICDE)</em> , pp. 1902–1905.
Bektic, D., Hachenberg, B., and Schiereck, D. (2020).“Factor-based investing in government bond markets: a survey
of the current state of research.” In: <em>Journal of Asset Management</em> 21, pp. 94–105.
Ben Baccar, Y. (2019).“Comparative Study on Time Series Forecasting Models.” en. MA thesis. ParisTech.
Benidis, K., Rangapuram, S. S., Flunkert, V., Wang, B., Maddix, D., Turkmen, C., Gasthaus, J., Bohlke-Schneider,
M., Salinas, D., Stella, L., Callot, L., and Januschowski, T. (2020).“Neural forecasting: Introduction and liter-
ature overview.” In: <em>arXiv e-Print</em>.
Berardi, A., Markovich, M., Plazzi, A., and Tamoni, A. (2021).“Mind the (Convergence) Gap: Bond Predictability
Strikes Back!” In: <em>Management Science</em>.
Bergmeir, C., Triguero, I., Molina, D., Aznarte, J. L., and Benitez, J. M. (2012).“Time Series Modeling and Fore-
casting Using Memetic Algorithms for Regime-Switching Models.” In: <em>IEEE Transactions on Neural Networks
and Learning Systems</em> 23(11), pp. 1841–1847.
Bergmeir, C., Hyndman, R. J., and Koo, B. (2018).“A note on the validity of cross-validation for evaluating
autoregressive time series prediction.” In: <em>Computational Statistics &amp; Data Analysis</em> 120, pp. 70–83.
Bernardi, M. and Catania, L. (2014).“The Model Confidence Set package for R.” In: <em>arXiv e-Print</em>.
Berrisch, J. and Ziel, F. (2021).“CRPS Learning.” In: <em>arXiv e-Print</em>.
Bessembinder, H. (, Burt, A. P., and Hrdlicka, C. M. (2022).“Time Series Variation in the Factor Zoo.” In: <em>SSRN
e-Print</em>.
Bhatnagar, A., Kassianik, P., Liu, C., Lan, T., Yang, W., Cassius, R., Sahoo, D., Arpit, D., Subramanian, S., Woo,
G., Saha, A., Jagota, A. K., Gopalakrishnan, G., Singh, M., Krithika, K. C., Maddineni, S., Cho, D., Zong, B.,
Zhou, Y., Xiong, C., Savarese, S., Hoi, S., and Wang, H. (2021). “Merlion: A Machine Learning Library for Time
Series.” In: <em>arXiv e-Print</em>.
Bianchi, D., Buchner, M., and Tamoni, A. (2021).“Bond Risk Premiums with Machine Learning.” In: <em>The Review
of Financial Studies</em> 34(2), pp. 1046–1089.
Bianchi, D. and McAlinn, K. (2021).“Divide and Conquer: Financial Ratios and Industry Returns Predictability.”
In: <em>SSRN e-Print</em>.</p>
<p>Bianchi, D. and Tamoni, A. (2020).“Sparse Predictive Regressions: Statistical Performance and Economic Signif-
icance.” In: <em>Machine Learning for Asset Management: New Developments and Financial Applications</em>. Ed. by
E. Jurczenko. Wiley, pp. 75–113.
Bielinski, A. and Broby, D. (2021).“Machine Learning Methods in Asset Pricing.” In: <em>SSRN e-Print</em>.
Billio, M., Casarin, R., Ravazzolo, F., and Dijk, H. K. van (2013).“Time-varying combinations of predictive densities
using nonlinear filtering.” In: <em>Journal of Econometrics</em> 177(2), pp. 213–232.
Bisaglia, L. and Grigoletto, M. (2021).“A new time-varying model for forecasting long-memory series.” In: <em>Statistical
Methods &amp; Applications</em> 30, pp. 139–155.
Bjerregård, M. B., Møller, J. K., and Madsen, H. (2021).“An introduction to multivariate probabilistic forecast
evaluation.” In: <em>Energy and AI</em> 4, p. 100058.
Blitz, D., Hanauer, M. X., Vidojevic, M., and Vliet, P. v. (2018).“Five Concerns with the Five-Factor Model.” In:
<em>The Journal of Portfolio Management</em> 44(4), pp. 71–78.
Bloemheuvel, S., van den Hoogen, J., Jozinovic, D., Michelini, A., and Atzmueller, M. (2022). “Multivariate Time
Series Regression with Graph Neural Networks.” In: <em>arXiv e-Print</em>.
Boileau, P., Hejazi, N., Collica, B., Laan, M. van der, and Dudoit, S. (2021).“cvCovEst: Cross-validated covariance
matrix estimator selection and evaluation in R.” In: <em>Journal of Open Source Software</em> 6(63), p. 3273.
Bokde, N. D., Yaseen, Z. M., and Andersen, G. B. (2020).“ForecastTB - An R Package as a Test-Bench for Time
Series Forecasting, with Application of Wind Speed and Solar Radiation Modeling.” In: <em>Energies</em> 13(10), p. 2578.
Botchkarev, A. (2019).“A new typology design of performance metrics to measure errors in machine learning
regression algorithms.” In: <em>Interdisciplinary Journal of Information, Knowledge, and Management</em> 14, pp. 045–
076.
Bouallegue, Z. B., Haiden, T., and Richardson, D. S. (2018).“The diagonal score: Definition, properties, and
interpretations.” In: <em>Quarterly Journal of the Royal Meteorological Society</em> 144(714), pp. 1463–1473.
Brehmer, J., Gneiting, T., Schlather, M., and Strokorb, K. (2021). “Using scoring functions to evaluate point process
forecasts.” In: <em>arXiv e-Print</em>.
Brehmer, J. R. and Gneiting, T. (2020).“Properization: constructing proper scoring rules via Bayes acts.” In: <em>Annals
of the Institute of Statistical Mathematics</em> 72(3), pp. 659–673.
Breitung, J. and Knuppel, M. (2021).“How far can we forecast? Statistical tests of the predictive content.” In:
<em>Journal of Applied Econometrics</em> 36(4), pp. 369–392.
Brugiere, P. (2020). <em>Quantitative Portfolio Management with Applications in Python</em>. Springer International Pub-
lishing. 189 pp.
Bryzgalova, S., Huang, J., and Julliard, C. (2021a).“Bayesian solutions for the factor zoo: we just ran two quadrillion
models.” In: <em>SSRN e-Print</em>.
Bryzgalova, S., Pelger, M., and Zhu, J. (2021b).“Forest Through the Trees: Building Cross-Sections of Stock
Returns.” In: <em>SSRN e-Print</em>.
Bulut, L. (2019).“Does Statistical Significance Help to Evaluate Predictive Performance of Competing Models?”
In: <em>SSRN e-Print</em>.
Burns, D. M. and Whyne, C. M. (2018).“Seglearn: A Python Package for Learning Sequences and Time Series.”
In: <em>arXiv e-Print</em>.
Cai, Z., Fang, Y., and Xu, Q. (2022).“Testing capital asset pricing models using functional-coefficient panel data
models with cross-sectional dependence.” In: <em>Journal of Econometrics</em> 227(1), pp. 114–133.
Cajas, D. (2021a).“Entropic Portfolio Optimization: a Disciplined Convex Programming Framework.” In: <em>SSRN
e-Print</em>.
Cajas, D. (2021b).“OWA Portfolio Optimization: a Disciplined Convex Programming Framework.” In: <em>SSRN e-
Print</em>.
Caldeira, J. F., Moura, G. V., and Santos, A. A. P. (2018).“Yield curve forecast combinations based on bond
portfolio performance.” In: <em>Journal of Forecasting</em> 37(1), pp. 64–82.
Capolongo, A. and Pacella, C. (2021).“Forecasting inflation in the euro area: countries matter!” In: <em>Empirical
Economics</em> 61, pp. 2477–2499.
Carr, P. P. and Wu, L. (2021). <em>Decomposing Long Bond Returns: A Decentralized Theory</em>. Tech. rep. NYU.
Castilho, D., Souza, T. T. P., Kang, S. M., Gama, J., and Carvalho, A. C. P. L. F. de (2021). “Forecasting Financial
Market Structure from Network Features using Machine Learning.” In: <em>arXiv e-Print</em>.
Castle, J. L., Doornik, J. A., and Hendry, D. F. (2021a).“Forecasting Principles from Experience with Forecasting
Competitions.” In: <em>Forecasting</em> 3(1), pp. 138–165.</p>
<p>Castle, J. L., Doornik, J. A., and Hendry, D. F. (2021b).“Selecting a Model for Forecasting.” In: <em>Econometrics</em> 9(3),
p. 26.
Cerqueira, V., Torgo, L., and Mozetič, I. (2020).“Evaluating time series forecasting models: an empirical study on
performance estimation methods.” In: <em>Machine Learning</em> 109, pp. 1997–2028.
Cerqueira, V., Torgo, L., and Soares, C. (2021a).“Model Selection for Time Series Forecasting: Empirical Analysis
of Different Estimators.” In: <em>arXiv e-Print</em>.
Cerqueira, V., Torgo, L., Soares, C., and Bifet, A. (2021b).“Model Compression for Dynamic Forecast Combination.”
In: <em>arXiv e-Print</em>.
Cetin, B. and Yavuz, I. (2021).“Comparison of forecast accuracy of Ata and exponential smoothing.” In: <em>Journal
of Applied Statistics</em> 48(13-15), pp. 2580–2590.
Chai, D., Chiah, M., and Gharghori, P. (2019).“Which model best explains the returns of large Australian stocks?”
In: <em>Pacific-Basin Finance Journal</em> 55, pp. 182–191.
Challu, C., Olivares, K. G., Welter, G., and Dubrawski, A. (2021). “DMIDAS: Deep Mixed Data Sampling Regression
for Long Multi-Horizon Time Series Forecasting.” In: <em>arXiv e-Print</em>.
Chan, F. and Pauwels, L. L. (2018).“Some theoretical results on forecast combinations.” In: <em>International Journal
of Forecasting</em> 34(1), pp. 64–74.
Charles, A., Darné, O., and Kim, J. H. (2022).“Stock return predictability: Evaluation based on interval forecasts.”
In: <em>Bulletin of Economic Research</em> 74(2), pp. 363–385.
Charte, F., Vico, A., Perez-Godoy, M. D., and Rivera, A. J. (2019).“predtoolsTS: R package for streamlining time
series forecasting.” In: <em>Progress in Artificial Intelligence</em> 8(4), pp. 505–510.
Chatigny, P., Wang, S., Patenaude, J.-M., and Oreshkin, B. N. (2021). “Neural forecasting at scale.” In: <em>arXiv
e-Print</em>.
Chatterjee, A., Bhowmick, H., and Sen, J. (2021). “Stock Price Prediction Using Time Series, Econometric, Machine
Learning, and Deep Learning Models.” In: <em>arXiv e-Print</em>.
Chen, A. Y. and Zimmermann, T. (2021).“Open Source Cross-Sectional Asset Pricing.” In: <em>SSRN e-Print</em>.
Chen, A. Y. and Zimmermann, T. (2022).“Open Source Cross-Sectional Asset Pricing.” In: <em>American Finance
Association Annual Meeting</em>.
Chen, H., Jiang, L., and Liu, W. (2020).“Predicting returns out of sample: A naive model averaging approach.” In:
<em>SSRN e-Print</em>.
Chen, L., Chen, D., Shang, Z., Zhang, Y., Wen, B., and Yang, C. (2022). “Multi-Scale Adaptive Graph Neural
Network for Multivariate Time Series Forecasting.” In: <em>arXiv e-Print</em>.
Chen, L., Chen, W., Wu, B., Zhang, Y., Wen, B., and Yang, C. (2021a). “Learning from Multiple Time Series: A
Deep Disentangled Approach to Diversified Time Series Forecasting.” In: <em>arXiv e-Print</em>.
Chen, L., Pelger, M., and Zhu, J. (2021b).“Deep learning in asset pricing.” In: <em>SSRN e-Print</em>.
Cheng, D., Yang, F., Xiang, S., and Liu, J. (2022).“Financial time series forecasting with multi-modality graph
neural network.” In: <em>Pattern Recognition</em> 121, p. 108218.
Cheng, M., Swanson, N. R., and Yao, C. (2019).“Forecast Evaluation.” In: <em>SSRN e-Print</em>.
Chevallier, J., Guégan, D., and Goutte, S. (2021).“Is It Possible to Forecast the Price of Bitcoin?” In: <em>Forecasting</em>
3(2), pp. 377–420.
Chiah, M., Chai, D., Zhong, A., and Li, S. (2016).“A Better Model? An Empirical Investigation of the Fama-French
Five-factor Model in Australia.” In: <em>International Review of Finance</em> 16(4), pp. 595–638.
Chiang, I.-H. E., Liao, Y., and Zhou, Q. (2021).“Modeling the cross-section of stock returns using sensible models
in a model pool.” In: <em>Journal of Empirical Finance</em> 60, pp. 56–73.
Chib, S. (2020). <em>R package czfactor</em>. Tech. rep. Washington University.
Chib, S. and Zeng, X. (2020).“Which factors are risk factors in asset pricing? A model scan framework.” In: <em>Journal
of Business &amp; Economic Statistics</em> 38(4), pp. 771–783.
Chib, S., Zeng, X., and Zhao, L. (2020).“On Comparing Asset Pricing Models.” In: <em>Journal of Finance</em> 75(11),
pp. 551–577.
Chib, S. and Zhao, L. (2020). <em>R package czzg</em>. Tech. rep. Washington University.
Chib, S., Zhao, L., Huang, D., and Zhou, G. (2022).“Winners from Winners: A Tale of Risk Factors.” In: <em>SSRN
e-Print</em>.
Chiu, C.-W., Hayes, S., Kapetanios, G., and Theodoridis, K. (2019).“A new approach for detecting shifts in forecast
accuracy.” In: <em>International Journal of Forecasting</em> 35(4), pp. 1596–1612.
Choe, Y. J. and Ramdas, A. (2022). “Comparing Sequential Forecasters.” In: <em>arXiv e-Print</em>.</p>
<p>Cholakov, R. and Kolev, T. (2021). “Transformers predicting the future. Applying attention in next-frame and time
series forecasting.” In: <em>arXiv e-Print</em>.
Chordia, T., Goyal, A., and Saretto, A. (2020).“Anomalies and false rejections.” In: <em>The Review of Financial Studies</em>
33(5), pp. 2134–2179.
Chu, P. K. (2021).“Forecasting Recessions with Financial Variables and Temporal Dependence.” In: <em>Economies</em>
9(3), p. 118.
Chudik, A., Pesaran, M. H., and Sharifvaghefi, M. (2021).“Variable Selection and Forecasting in High Dimensional
Linear Regressions with Structural Breaks.” In: <em>SSRN e-Print</em> 2020(394).
Cohen, N., Sood, S., Zeng, Z., Balch, T., and Veloso, M. (2020). “Visual Forecasting of Time Series with Image-to-
Image Regression.” In: <em>arXiv e-Print</em>.
Cohen, N., Sood, S., Zeng, Z., Balch, T., and Veloso, M. (2021).“Visual Time Series Forecasting: An Image-driven
Approach.” In: <em>MiLeTS’21: 7th KDD Workshop on Mining and Learning from Time Series</em>.
Collot, S. and Hemauer, T. (2021).“A literature review of new methods in empirical asset pricing: omitted-variable
and errors-in-variable bias.” In: <em>Financial Markets and Portfolio Management</em> 35, pp. 77–100.
Cong, L. W., Tang, K., Wang, J., and Zhang, Y. (2021).“Deep Sequence Modeling: Development and Applications
in Asset Pricing.” In: <em>The Journal of Financial Data Science</em> 3(1), pp. 28–42.
Coqueret, G. (2022). <em>Perspectives in sustainable equity investing (website version)</em>.
Coqueret, G. and Guida, T. (2020). <em>Machine Learning for Factor Investing: R Version</em>. Chapman and Hall/CRC.
341 pp.
Cornell, B. (2020).“Stock characteristics and stock returns: a skeptic’s look at the cross section of expected returns.”
In: <em>The Journal of Portfolio Management</em> 46(5), pp. 131–142.
Coroneo, L., Iacone, F., Paccagnini, A., and Monteiro, P. S. (2021).“Testing the Predictive Accuracy of COVID-19
Forecasts.” In: <em>SSRN e-Print</em>.
Costantini, M. and Kunst, R. M. (2021).“On using predictive-ability tests in the selection of time-series prediction
models: A Monte Carlo evaluation.” In: <em>International Journal of Forecasting</em> 37(2), pp. 445–460.
Czasonis, M., Kritzman, M., and Turkington, D. (2020).“Addition by Subtraction: A Better Way to Forecast Factor
Returns (and Everything Else).” In: <em>The Journal of Portfolio Management</em> 46(8), pp. 98–107.
Czasonis, M., Kritzman, M., and Turkington, D. (2021a).“Relevance.” In: <em>SSRN e-Print</em>.
Czasonis, M., Kritzman, M., and Turkington, D. (2021b).“The Past as Prologue: How to Forecast Presidential
Elections.” In: <em>SSRN e-Print</em>.
Dai, Z., Kang, J., and Wen, F. (2021).“Predicting stock returns: A risk measurement perspective.” In: <em>International
Review of Financial Analysis</em> 74, p. 101676.
Dai, Z., Li, T., and Yang, M. (2022).“Forecasting stock return volatility: The role of shrinkage approaches in a
data-rich environment.” In: <em>Journal of Forecasting</em>.
Dama, F. and Sinoquet, C. (2021).“Analysis and modeling to forecast in time series: a systematic review.” In: <em>arXiv
e-Print</em>.
Davydenko, A. and Goodwin, P. (2021).“Assessing Point Forecast Bias Across Multiple Time Series: Measures and
Visual Tools.” In: <em>International Journal of Statistics and Probability</em> 10(5), p. 46.
De Baets, S. and Harvey, N. (2020).“Using judgment to select and adjust forecasts from statistical models.” In:
<em>European Journal of Operational Research</em> 284(3), pp. 882–895.
de Carvalho, M. and Rua, A. (2017).“Real-time nowcasting the US output gap: Singular spectrum analysis at
work .” In: <em>International Journal of Forecasting</em> 33(1), pp. 185–198.
de Valk, S., Mattos, D. de, and Ferreira, P. (2019).“Nowcasting: An R Package for Predicting Economic Variables
Using Dynamic Factor Models.” In: <em>The R Journal</em>.
Debnath, A., Waghmare, G., Wadhwa, H., Asthana, S., and Arora, A. (2021).“Exploring Generative Data Aug-
mentation in Multivariate Time Series Forecasting : Opportunities and Challenges.” In: <em>MiLeTS’21: 7th KDD
Workshop on Mining and Learning from Time Series</em>.
Dendramis, Y., Kapetanios, G., and Marcellino, M. (2020).“A similarity-based approach for macroeconomic fore-
casting.” In: <em>Journal of the Royal Statistical Society Series A</em> 183(3), pp. 801–827.
Deshpande, P., Marathe, K., De, A., and Sarawagi, S. (2021).“Long Horizon Forecasting With Temporal Point
Processes.” In: <em>arXiv e-Print</em>.
Deshpande, P. and Sarawagi, S. (2021). “Long Range Probabilistic Forecasting in Time-Series using High Order
Statistics.” In: <em>arXiv e-Print</em>.</p>
<p>Di Fonzo, T. and Girolimetto, D. (2020).“Cross-temporal forecast reconciliation: Optimal combination method and
heuristic alternatives.” In: <em>arXiv e-Print</em>.
Di Fonzo, T. and Girolimetto, D. (2021). “Forecast combination based forecast reconciliation: insights and exten-
sions.” In: <em>arXiv e-Print</em>.
Di Fonzo, T. and Girolimetto, D. (2022).“Cross-temporal forecast reconciliation: Optimal combination method and
heuristic alternatives.” In: <em>International Journal of Forecasting</em>.
Diebold, F. X. (2015).“Comparing Predictive Accuracy, Twenty Years Later: A Personal Perspective on the Use
and Abuse of Diebold-Mariano Tests.” In: <em>Journal of Business and Economic Statistics</em> 33(1), p. 1.
Ding, L., Ahmed, S., and Shapiro, A. (2020).“A Python package for multi-stage stochastic programming.” In:
<em>Optimization Online e-Print</em>.
Dixon, M. and Polson, N. (2020).“Deep Fundamental Factor Models.” In: <em>SIAM Journal on Financial Mathematics</em>
11(3), SC–26–SC–37.
Dixon, M. F., Halperin, I., and Bilokon, P. (2020). <em>Machine Learning in Finance: from theory to practice</em>. Springer
International Publishing. 548 pp.
Dong, H., Guo, X., Reichgelt, H., and Hu, R. (2020).“Predictive power of ARIMA models in forecasting equity
returns: a sliding window method.” In: <em>Journal of Asset Management</em> (21), pp. 549–566.
Dong, X., Li, Y., Rapach, D., and Zhou, G. (2022).“Anomalies and the expected market return.” In: <em>Journal of
Finance</em> 27(1), pp. 639–681.
Drobetz, W. and Otto, T. (2020).“Empirical Asset Pricing via Machine Learning: Evidence from the European
Stock Market.” In: <em>SSRN e-Print</em>.
Du, Y., Wang, J., Feng, W., Pan, S., Qin, T., Xu, R., and Wang, C. (2021). “AdaRNN: Adaptive Learning and
Forecasting of Time Series.” In: <em>arXiv e-Print</em>.
Elkamhi, R., Lee, J. S. H., and Salerno, M. (2021).“Factor Investing Using Capital Market Assumptions.” In: <em>The
Journal of Portfolio Management</em>.
Ellingsen, J., Larsen, V. H., and Thorsrud, L. A. (2022).“News media vs. FRED-MD for macroeconomic forecasting.”
In: <em>Journal of Applied Econometrics</em>.
Fabozzi, F. J., Fabozzi, F. A., Lopez de Prado, M., and Stoyanov, S. (2021). <em>Asset Management: Tools and Issues</em>.
World Scientific. 516 pp.
Faloutsos, C., Flunkert, V., Gasthaus, J., Januschowski, T., and Wang, Y. (2019).“Forecasting Big Time Series:
Theory and Practice.” In: <em>Tutorial for the 25TH ACM SIGKDD Conference on Knowledge Discovery and Data
Mining</em>. ACM.
Fama, E. F. and French, K. R. (2018).“Choosing factors.” In: <em>Journal of Financial Economics</em> 128(2), pp. 234–252.
Fama, E. F. and French, K. R. (2020).“Comparing Cross-Section and Time-Series Factor Models.” In: <em>The Review
of Financial Studies</em> 33(5), pp. 1891–1926.
Fameliti, S. P. and Skintzi, V. D. (2020).“Predictive ability and economic gains from volatility forecast combina-
tions.” In: <em>Journal of Forecasting</em> 39(2), pp. 200–219.
Fan, J., Ke, Y., Sun, Q., and Zhou, W.-X. (2019).“FarmTest: Factor-adjusted robust multiple testing with approx-
imate false discovery control.” In: <em>Journal of the American Statistical Association</em> 114(528), pp. 1880–1893.
Fang, Y., Guan, B., Wu, S., and Heravi, S. (2020).“Optimal forecast combination based on ensemble empirical
mode decomposition for agricultural commodity futures prices.” In: <em>Journal of Forecasting</em> 39(6), pp. 877–886.
Faria, G. and Verona, F. (2021).“Time-frequency forecast of the equity premium.” In: <em>Quantitative Finance</em>.
Fauvel, K., Masson, V., and Fromont, elisa (2021). “A Performance-Explainability Framework to Benchmark Ma-
chine Learning Methods: Application to Multivariate Time Series Classifiers.” In: <em>arXiv e-Print</em>.
Feldkircher, M., Huber, F., and Pfarrhofer, M. (2020).“Factor Augmented Vector Autoregressions, Panel VARs,
and Global VARs.” In: <em>Macroeconomic Forecasting in the Era of Big Data</em>. Springer International Publishing,
pp. 65–93.
Feng, G., Giglio, S., and Xiu, D. (2020).“Taming the factor zoo: A test of new factors.” In: <em>The Journal of Finance</em>
75(3), pp. 1327–1370.
Filipovic, D. and Khalilzadeh, A. (2021).“Machine Learning for Predicting Stock Return Volatility.” In: <em>SSRN
e-Print</em>.
Fjellstrom, C. (2022). “Long Short-Term Memory Neural Network for Financial Time Series.” In: <em>arXiv e-Print</em>.
Fleiss, A. and Cui, H. (2021).“Forecasting Stock Price Changes Using Natural Language Processing.” In: <em>SSRN
e-Print</em>.
Fosten, J. and Gutknecht, D. (2021).“Horizon confidence sets.” In: <em>Empirical Economics</em>.</p>
<p>Frenkel, M., Jung, J.-K., and Rulke, J.-C. (2021).“Testing for the rationality of central bank interest rate forecasts.”
In: <em>Empirical Economics</em>.
Freyberger, J., Neuhierl, A., and Weber, M. (2020).“Dissecting Characteristics Nonparametrically.” In: <em>The Review
of Financial Studies</em> 33(5), pp. 2326–2377.
Fulton, C. and Hubrich, K. (2021).“Forecasting US Inflation in Real Time.” In: <em>SSRN e-Print</em> (014).
Gafka, B., Savor, P. G., and Wilson, M. I. (2021).“Sources of Return Predictability.” In: <em>SSRN e-Print</em>.
Gasthaus, J., Benidis, K., Wang, Y., Rangapuram, S. S., Salinas, D., Flunkert, V., and Januschowski, T. (2019).
“Probabilistic Forecasting with Spline Quantile Function RNNs.” In: <em>Proceedings of Machine Learning Research</em>
89, pp. 1901–1910.
Gastinger, J., Nicolas, S., Stepic, D., Schmidt, M., and Schulke, A. (2021).“A study on Ensemble Learning for
Time Series Forecasting and the need for Meta-Learning.” In: <em>arXiv e-Print</em>.
Geertsema, P. G. and Lu, H. (2021).“Long-horizon predictability and information decay in equity markets.” In:
<em>SSRN e-Print</em>.
Geweke, J. and Amisano, G. (2010).“Comparing and evaluating Bayesian predictive distributions of asset returns.”
In: <em>International Journal of Forecasting</em> 26(2), pp. 216–230.
Ghorbani, M. and Chong, E. K. P. (2021).“A dimension reduction method for stock-price prediction using multiple
predictors.” In: <em>Operational Research</em>.
Gilleland, E., Hering, A. S., Fowler, T. L., and Brown, B. G. (2018).“Testing the Tests: What Are the Impacts
of Incorrect Assumptions When Applying Confidence Intervals or Hypothesis Tests to Compare Competing
Forecasts?” In: <em>Monthly Weather Review</em> 146(6), pp. 1685–1703.
Gilliland, M. (2020).“The value added by machine learning approaches in forecasting.” In: <em>International Journal
of Forecasting</em> 36(1), pp. 161–166.
Giovannelli, A., Massacci, D., and Soccorsi, S. (2021a).“Forecasting Stock Returns with Large Dimensional Factor
Models.” In: <em>SSRN e-Print</em>.
Giovannelli, A., Massacci, D., and Soccorsi, S. (2021b).“Forecasting stock returns with large dimensional factor
models.” In: <em>Journal of Empirical Finance</em> 63, pp. 252–269.
Gneiting, T. and Resin, J. (2022). “Regression Diagnostics meets Forecast Evaluation: Conditional Calibration,
Reliability Diagrams, and Coefficient of Determination.” In: <em>arXiv e-Print</em>.
Godahewa, R., Bandara, K., Webb, G. I., Smyl, S., and Bergmeir, C. (2021a).“Ensembles of localised models for
time series forecasting.” In: <em>Knowledge-Based Systems</em> 233, p. 107518.
Godahewa, R., Bergmeir, C., Webb, G. I., Hyndman, R. J., and Montero-Manso, P. (2021b).“Monash Time Series
Forecasting Archive.” In: <em>arXiv E-Print</em>.
Goliński, A. and Spencer, P. (2021).“Estimating the Term Structure with Linear Regressions: Getting to the Roots
of the Problem.” In: <em>Journal of Financial Econometrics</em> 19(5), pp. 960–984.
Golyandina, N., Korobeynikov, A., and Zhigljavsky, A. (2018). <em>Singular Spectrum Analysis with R</em>. Springer Berlin
Heidelberg. 272 pp.
Gonzalez-Rivera, G., Luo, Y., and Ruiz, E. (2020).“Prediction regions for interval-valued time series.” In: <em>Journal
of Applied Econometrics</em> 35(4), pp. 373–390.
Gospodinov, N. and Maasoumi, E. (2021).“Generalized aggregation of misspecified models: with an application to
asset pricing.” In: <em>Journal of Econometrics</em> 222(1), pp. 451–467.
Gospodinov, N. and Robotti, C. (2021).“Common pricing across asset classes: Empirical evidence revisited.” In:
<em>Journal of Financial Economics</em> 132(2), pp. 292–324.
Goyal, A. and Jegadeesh, N. (2018).“Cross-Sectional and Time-Series Tests of Return Predictability: What Is the
Difference?” In: <em>The Review of Financial Studies</em> 31(5), pp. 1784–1824.
Gramespacher, T. and Banziger, A. (2019).“The Bias in Two-Pass Regression Tests of Asset-Pricing Models in Pres-
ence of Idiosyncratic Errors with Cross-Sectional Dependence.” In: <em>Review of Pacific Basin Financial Markets
and Policies</em> 22(02), p. 1950012.
Graziani, C., Rosner, R., Adams, J. M., and Machete, R. L. (2021).“Probabilistic recalibration of forecasts.” In:
<em>International Journal of Forecasting</em> 37(1), pp. 1–27.
Grazzi, R., Flunkert, V., Salinas, D., Januschowski, T., Seeger, M., and Archambeau, C. (2021). “Meta-Forecasting
by combining Global Deep Representations with Local Adaptation.” In: <em>arXiv e-Print</em>.
Grealish, A. and Kolm, P. N. (2021).“Robo-Advisory: From Investing Principles and Algorithms to Future Devel-
opments.” In: <em>SSRN e-Print</em>.
Greenberg, S. (2020).“Calibration Scoring Rules for Practical Prediction Training.” In: <em>arXiv e-Print</em>.</p>
<p>Gu, S., Kelly, B., and Xiu, D. (2021).“Autoencoder asset pricing models.” In: <em>Journal of Econometrics</em> 222(1),
pp. 429–450.
Gu, S., Kelly, B. T., and Xiu, D. (2020).“Empirical asset pricing via machine learning.” In: <em>The Review of Financial
Studies</em> 33 (5), pp. 2223–2273.
Guijarro-Ordonez, J., Pelger, M., and Zanotti, G. (2021).“Deep Learning Statistical Arbitrage.” In: <em>SSRN e-Print</em>.
Gurdogan, H. and Kercheval, A. (2021). “Multi Anchor Point Shrinkage for the Sample Covariance Matrix (Extended
Version).” In: <em>arXiv e-Print</em>.
Haase, F. and Neuenkirch, M. (2021).“Forecasting Stock Market Recessions in the US: Predictive Modeling using
Different Identification Approaches.” In: <em>SSRN e-Print</em>.
Habibnia, A. (2016).“Essays in high-dimensional nonlinear time series analysis.” PhD thesis. London School of
Economics and Political Science.
Hammerschmid, R. and Lohre, H. (2018).“Regime Shifts and Stock Return Predictability.” In: <em>International Review
of Economics and Finance</em> 56, pp. 138–160.
Hannadige, S. B., Gao, J., Silvapulle, M. J., and Silvapulle, P. (2021).“Forecasting a Nonstationary Time Series
Using a Mixture of Stationary and Nonstationary Predictors.” In: <em>SSRN e-Print</em>.
Harris, D., Martin, G. M., Perera, I., and Poskitt, D. S. (2019).“Construction and visualization of confidence sets
for frequentist distributional forecasts.” In: <em>Journal of Computational and Graphical Statistics</em> 28(2), pp. 92–104.
Harvey, C. R. and Liu, Y. (2020).“Detecting Repeatable Performance.” In: <em>SSRN e-Print</em>.
Harvey, C. R., Liu, Y., and Saretto, A. (2020).“An Evaluation of Alternative Multiple Testing Methods for Finance
Applications.” In: <em>The Review of Asset Pricing Studies</em> 10(2), pp. 199–248.
Harvey, D. I., Leybourne, S. J., Sollis, R., and Taylor, A. M. R. (2021).“Real-Time Detection of Regimes of
Predictability in the U.S. Equity Premium.” In: <em>Journal of Applied Econometrics</em> 36, pp. 45–70.
Hassler, U. and Pohle, M.-O. (2021).“Forecasting under Long Memory.” In: <em>Journal of Financial Econometrics</em>.
Hauzenberger, N., Huber, F., and Klieber, K. (2021). “Real-time Inflation Forecasting Using Non-linear Dimension
Reduction Techniques.” In: <em>arXiv e-Print</em>.
He, S. and Gu, S. (2022). “Multi-modal Attention Network for Stock Movements Prediction.” In: <em>arXiv e-Print</em>.
Herzen, J., Lassig, F., Piazzetta, S. G., Neuer, T., Tafti, L., Raille, G., Pottelbergh, T. V., Pasieka, M., Skrodzki,
A., Huguenin, N., Dumonal, M., Koscisz, J., Bader, D., Gusset, F., Benheddi, M., Williamson, C., Kosinski,
M., Petrik, M., and Grosch, G. (2022). “Darts: User-Friendly Modern Machine Learning for Time Series.” In:
<em>Journal of Machine Learning Research</em> 23(124), pp. 1–6.
Hewamalage, H., Bergmeir, C., and Bandara, K. (2020).“Global Models for Time Series Forecasting: A Simulation
Study.” In: <em>arXiv e-Print</em>.
Hewamalage, H., Bergmeir, C., and Bandara, K. (2021).“Recurrent Neural Networks for Time Series Forecasting:
Current status and future directions.” In: <em>International Journal of Forecasting</em> 37(1), pp. 388–427.
Ho, J., Tumkaya, T., Aryal, S., Choi, H., and Claridge-Chang, A. (2019).“Moving beyond P values: data analysis
with estimation graphics.” In: <em>Nature Methods</em> 16(7), pp. 565–566.
Ho, T.-W. and Lin, Y.-c. (2021).“Training by Rolling: Machine Learning and Stock Returns Forecasting.” In: <em>SSRN
e-Print</em>.
Hofmarcher, P. and Grun, B. (2020).“Bayesian Model Averaging.” In: <em>Macroeconomic Forecasting in the Era of Big
Data</em>. Springer International Publishing, pp. 359–388.
Hoga, Y. and Dimitriadis, T. (2021).“On Testing Equal Conditional Predictive Ability Under Measurement Error.”
In: <em>arXiv e-Print</em>.
Hollyman, R., Petropoulos, F., and Tipping, M. E. (2021).“Understanding forecast reconciliation.” In: <em>European
Journal of Operational Research</em> 294(1), pp. 149–160.
Holzmann, H. and Klar, B. (2021). “Using Proxies to Improve Forecast Evaluation.” In: <em>arXiv e-Print</em>.
Homescu, C. (2014).“Many risks, one (optimal) portfolio.” In: <em>SSRN e-Print</em>.
Homescu, C. (2015).“Better Investing Through Factors, Regimes and Sensitivity Analysis.” In: <em>SSRN e-Print</em>.
Hounyo, U. and Lahiri, K. (2022).“Estimating the variance of a combined forecast: Bootstrap-based approach.” In:
<em>Journal of Econometrics</em>.
Hsiao, C. and Wan, S. K. (2014).“Is there an optimal forecast combination?” In: <em>Journal of Econometrics</em> 178,
pp. 294–309.
Hull, B. and Qiao, X. (2017).“A Practitioner’s Defense of Return Predictability.” In: <em>The Journal of Portfolio
Management</em> 43(3), pp. 60–76.</p>
<p>Hunt, I. (2022).“In-sample tests of predictability are superior to pseudo-out-of-sample tests, even when data mining.”
In: <em>International Journal of Forecasting</em>.
Hyndman, R. J. (2020).“Tidy Time Series and Forecasting in R.” In: <em>RStudio conf2020</em>.
Hyndman, R. J. and Athanasopoulos, G. (2020). <em>Forecasting: Principles and Practice (Third Edition)</em>. OTexts.
380 pp.
Ilic, I., Gorgulu, B., and Cevik, M. (2020).“Augmented Out-of-Sample Comparison Method for Time Series Fore-
casting Techniques.” In: <em>Advances in Artificial Intelligence</em>. Springer International Publishing, pp. 302–308.
Ilmanen, A., Chandra, S., and McQuinn, N. (2020).“Demystifying illiquid assets: expected returns for private
equity .” In: <em>The Journal of Alternative Investments</em> 22(3), pp. 8–22.
Inoue, A., Jin, L., and Rossi, B. (2017).“Rolling window selection for out-of-sample forecasting with time-varying
parameters.” In: <em>Journal of Econometrics</em> 196(1), pp. 55–67.
Irlam, G. (2020a). <em>AI Planner</em> .url:https://www.aiplanner.com/.
Irlam, G. (2020b).“Machine learning for retirement planning.” In: <em>The Journal of Retirement</em> 8(1), pp. 32–29.
Irlam, G. (2020c).“Multi Scenario Financial Planning via Deep Reinforcement Learning AI.” In: <em>SSRN e-Print</em>.
Iworiso, J. and Vrontos, S. (2021).“On the Predictability of the Equity Premium Using Deep Learning Techniques.”
In: <em>The Journal of Financial Data Science</em> 3(1), pp. 74–92.
Jaganathan, S. and Prakash, P. K. S. (2020).“A combination-based forecasting method for the M4-competition.”
In: <em>International Journal of Forecasting</em> 36(1), pp. 98–104.
Jansen, S. (2020). <em>Machine Learning for Algorithmic Trading (Second Edition)</em>. Packt Publishing. 820 pp.
Janssen, R. V. (2019).“Multi-horizon comparison of multivariate inflation forecasting.” MA thesis. Erasmus School
of Economics.
Januschowski, T., Wang, Y., Torkkola, K., Erkkila, T., Hasson, H., and Gasthaus, J. (2022).“Forecasting with
trees.” In: <em>International Journal of Forecasting</em>.
Jegadeesh, N., Noh, J., Pukthuanthong, K., Roll, R., and Wang, J. (2019).“Empirical tests of asset pricing models
with individual assets: Resolving the errors-in-variables bias in risk premium estimation.” In: <em>Journal of Financial
Economics</em> 113(2), pp. 273–298.
Jin, S., Corradi, V., and Swanson, N. R. (2017).“Robust Forecast Comparison.” In: <em>Econometric Theory</em> 33(06),
pp. 1306–1351.
Jordan, A., Kruger, F., and Lerch, S. (2019).“Evaluating probabilistic forecasts with scoringRules.” In: <em>Journal of
Statistical Software</em> 90(12).
Joshi, S. (2019).“Time Series Analysis and Forecasting of the US Housing Starts using Econometric and Machine
Learning Model.” In: <em>arXiv e-Print</em>.
Jurczenko et al. (2020). <em>Machine Learning for Asset Management</em>. Ed. by E. Jurczenko. Wiley. 445 pp.
Kakushadze, Z. and Yu, W. (2016).“Statistical Risk Models.” In: <em>SSRN e-Print</em>.
Kakushadze, Z. and Yu, W. (2017).“Open Source Fundamental Industry Classification.” In: <em>MDPI Data</em> 22 (2).
Kakushadze, Z. and Yu, W. (2018a).“Betas, Benchmarks, and Beating the Market.” In: <em>The Journal of Trading</em>.
Kakushadze, Z. and Yu, W. (2018b).“Decoding stock market with quant alphas.” In: <em>Journal of Asset Management</em> ,
pp. 1–11.
Kakushadze, Z. and Yu, W. (2019).“Machine learning risk models.” In: <em>SSRN e-Print</em>.
Kakushadze, Z. and Yu, W. (2020).“Machine learning treasury yields.” In: <em>SSRN e-Print</em>.
Kalfa, S. Y. and Marquez, J. (2021).“Forecasting FOMC Forecasts.” In: <em>Econometrics</em> 9(3), p. 34.
Kamarthi, H., Kong, L., Rodriguez, A., Zhang, C., and Prakash, B. A. (2021). “CAMul: Calibrated and Accurate
Multi-view Time-Series Forecasting.” In: <em>arXiv e-Print</em>.
Kang, T.-H., Sharma, A., and Marshall, L. (2021a).“Assessing Goodness of Fit for Verifying Probabilistic Forecasts.”
In: <em>Forecasting</em> 3(4), pp. 763–773.
Kang, Y., Spiliotis, E., Petropoulos, F., Athiniotis, N., Li, F., and Assimakopoulos, V. (2021b).“Déjà vu: A data-
centric forecasting approach through time series cross-similarity.” In: <em>Journal of Business Research</em>.
Karathanasopoulos, A., Mitra, S., Skindilias, K., and Lo, C. C. (2017).“Modelling and Trading the English and
German Stock Markets with Novelty Optimization Techniques.” In: <em>Journal of Forecasting</em> 36(8) (8), pp. 974–
988.
Karolyi, A. and Van Nieuwerburgh, S. (2020).“New Methods for the Cross-Section of Returns.” In: <em>The Review of
Financial Studies</em> 33(5), pp. 1879–1890.
Kelly, B. T., Pruitt, S., and Su, Y. (2019).“Characteristics are covariances: A unified model of risk and return.” In:
<em>Journal of Financial Economics</em> 134(3), pp. 501–524.</p>
<p>Kiefer, D., Bauer, M., and Grimm, F. (2021).“Univariate Time Series Forecasting: Machine Learning Prediction of
the Best Suitable Forecast Model Based on Time Series Characteristics.” In: <em>Human Centred Intelligent Systems</em>.
Springer Singapore, pp. 152–162.
Klingberg Malmer, O. and Pettersson, G. (2020).“Tidying up the factor zoo: Using machine learning to find sparse
factor models that predict asset returns.” MA thesis. University of Goteborg.
Kosman, E. and Castro, D. D. (2021). “Vision-Guided Forecasting – Visual Context for Multi-Horizon Time Series
Forecasting.” In: <em>arXiv e-Print</em>.
Koutsandreas, D., Spiliotis, E., Petropoulos, F., and Assimakopoulos, V. (2021).“On the selection of forecasting
accuracy measures.” In: <em>Journal of the Operational Research Society</em>.
Kozak, S., Nagel, S., and Santosh, S. (2020).“Shrinking the cross-section.” In: <em>Journal of Financial Economics</em> 135
(2), pp. 271–292.
Kritzman, M., Kinlaw, W., and Turkington, D. (2017). <em>A Practitioner’s Guide to Asset Allocation</em>. Wiley. 256 pp.
Kruse, R., Leschinski, C., and Will, M. (2019).“Comparing Predictive Accuracy under Long Memory, With an
Application to Volatility Forecasting.” In: <em>Journal of Financial Econometrics</em> 17(2), pp. 180–228.
Kuznetsov, V. and Mohri, M. (2016).“Time series prediction and online learning.” In: <em>29th Annual Conference on
Learning Theory</em>. Ed. by V. Feldman, A. Rakhlin, and O. Shamir. Vol. 49. Proceedings of Machine Learning
Research. Columbia University, New York, New York, USA: PMLR, pp. 1190–1213.
Kynigakis, I. and Panopoulou, E. (2021).“Does Model Complexity add Value to Asset Allocation? Evidence from
Machine Learning Forecasting Models.” In: <em>Journal of Applied Econometrics</em>.
Kyriakou, I., Mousavi, P., Nielsen, J. P., and Scholz, M. (2020).“Longer-Term Forecasting of Excess Stock Returns</p>
<ul>
<li>The Five-Year Case.” In: <em>Mathematics</em> 8(6), p. 927.
Lai, K.-H., Zha, D., Wang, G., Xu, J., Zhao, Y., Kumar, D., Chen, Y., Zumkhawaka, P., Wan, M., Martinez, D.,
and Hu, X. (2021). “TODS: An Automated Time Series Outlier Detection System.” In: <em>arXiv e-Print</em>.
Lara-Benitez, P., Carranza-Garcia, M., and Riquelme, J. C. (2021).“An Experimental Review on Deep Learning
Architectures for Time Series Forecasting.” In: <em>arXiv e-Print</em>.
Lara-Benı́tez, P., Gallego-Ledesma, L., Carranza-Garcı́a, M., and Luna-Romera, J. M. (2021).“Evaluation of the
Transformer Architecture for Univariate Time Series Forecasting.” In: <em>Conference of the Spanish Association
for Artificial IntelligenceCAEPIA 2021: Advances in Artificial Intelligence</em>. Springer International Publishing,
pp. 106–115.
Le Guen, V. and Thome, N. (2020).“Probabilistic Time Series Forecasting with Structured Shape and Temporal
Diversity.” In: <em>arXiv e-Print</em>.
Le Guen, V. and Thome, N. (2021).“Deep Time Series Forecasting with Shape and Temporal Criteria.” In: <em>arXiv
e-Print</em>.
Ledoit, O., Wolf, M., and Zhao, Z. (2019).“Efficient Sorting: A More Powerful Test for Cross-Sectional Anomalies.”
In: <em>Journal of Financial Econometrics</em> 17(4), pp. 645–686.
Lee, T.-H. and Seregina, E. (2022). “Optimal Portfolio Using Factor Graphical Lasso.” In: <em>arXiv e-Print</em>.
Lerch, S., Thorarinsdottir, T. L., Ravazzolo, F., and Gneiting, T. (2015).“Forecaster’s Dilemma: Extreme Events
and Forecast Evaluation.” In: <em>arXiv e-Print</em>.
Lerch, S., Thorarinsdottir, T. L., Ravazzolo, F., and Gneiting, T. (2017).“Forecaster’s Dilemma: Extreme Events
and Forecast Evaluation.” In: <em>Statistical Science</em> 32(1), pp. 106–127.
Leroy, A., Latouche, P., Guedj, B., and Gey, S. (2020). “MAGMA: Inference and Prediction with Multi-Task
Gaussian Processes.” In: <em>arXiv e-Print</em>.
Lettau, M. and Pelger, M. (2020).“Factors That Fit the Time Series and Cross-Section of Stock Returns.” In: <em>The
Review of Financial Studies</em> 33(5), pp. 2274–2325.
Leung, E., Lohre, H., Mischlich, D., Shea, Y., and Stroh, M. (2020).“The Promises and Pitfalls of Machine Learning
for Predicting Cross-Sectional Stock Returns.” In: <em>SSRN e-Print</em>.
Leung, E., Lohre, H., Mischlich, D., Shea, Y., and Stroh, M. (2021).“The Promises and Pitfalls of Machine Learning
for Predicting Stock Returns.” In: <em>The Journal of Financial Data Science</em> 3(2), pp. 21–50.
Li, A. W. and Bastos, G. S. (2020).“Stock Market Forecasting Using Deep Learning and Technical Analysis: A
Systematic Review.” In: <em>IEEE Access</em> 8, pp. 185232–185242.
Li, L., Kang, Y., and Li, F. (2021a). “Bayesian forecast combination using time-varying features.” In: <em>arXiv e-Print</em>.
Li, Z., Liu, X.-Y., Zheng, J., Wang, Z., Walid, A., and Guo, J. (2021b).“FinRL-Podracer: High Performance and
Scalable Deep Reinforcement Learning for Quantitative Finance.” In: <em>ACM International Conference on AI in
Finance</em>.</li>
</ul>
<p>Lichtendahl, K. C. and Winkler, R. L. (2020).“Why do some combinations perform better than others?” In:
<em>International Journal of Forecasting</em> 36(1), pp. 142–149.
Lim, B. and Zohren, S. (2021).“Time-series forecasting with deep learning: a survey.” In: <em>Philosophical Transactions
of the Royal Society A: Mathematical, Physical and Engineering Sciences</em> 379(2194), p. 20200209.
Liu, M., Zeng, A., Xu, Z., Lai, Q., and Xu, Q. (2021a). “Time Series is a Special Sequence: Forecasting with Sample
Convolution and Interaction.” In: <em>arXiv e-Print</em>.
Liu, X.-Y., Rui, J., Gao, J., Yang, L., Yang, H., Wang, Z., Wang, C. D., and Guo, J. (2022). “FinRL-Meta: A
Universe of Near-Real Market Environments for Data-Driven Deep Reinforcement Learning in Quantitative
Finance.” In: <em>arXiv e-Print</em>.
Liu, X.-Y., Yang, H., Gao, J., and Wang, C. (2021b).“FinRL: Deep Reinforcement Learning Framework to Automate
Trading in Quantitative Finance.” In: <em>SSRN e-Print</em>.
Liu, Z., Zhu, Z., Gao, J., and Xu, C. (2021c).“Forecast Methods for Time Series Data: A Survey.” In: <em>IEEE Access</em>
9, pp. 91896–91912.
Loning, M., Bagnall, A., Ganesh, S., Kazakov, V., Lines, J., and Kiraly, F. J. (2019).“sktime: A Unified Interface
for Machine Learning with Time Series.” In: <em>arXiv e-Print</em>.
Loning, M. and Kiraly, F. (2020).“Forecasting with sktime: Designing sktime’s New Forecasting API and Applying
It to Replicate and Extend the M4 Study.” In: <em>arXiv e-Print</em>.
Lopez de Prado, M. (2020). <em>Machine learning for asset managers</em>. Cambridge University Press. 190 pp.
Makridakis, S., Hyndman, R. J., and Petropoulos, F. (2020).“Forecasting in social settings: The state of the art.”
In: <em>International Journal of Forecasting</em> 36 (1), pp. 15–28.
Makridakis, S. and Petropoulos, F. (2020).“The M4 competition: Conclusions.” In: <em>International Journal of Fore-
casting</em> 36(1), pp. 224–227.
Makridakis, S., Spiliotis, E., and Assimakopoulos, V. (2019).“The M4 Competition: 100,000 time series and 61
forecasting methods.” In: <em>International Journal of Forecasting</em> 36 (1), pp. 54–74.
Mancuso, P., Piccialli, V., and Sudoso, A. M. (2021).“A machine learning approach for forecasting hierarchical
time series.” In: <em>Expert Systems with Applications</em> 182, p. 115102.
Marinescu, M. (2022).“Risk-Based Optimal Portfolio Strategies: A Compendium.” In: <em>SSRN e-Print</em>.
Martin, G. M., Loaiza-Maya, R., Frazier, D. T., Maneesoonthorn, W., and Hassan, A. R. (2020).“Optimal proba-
bilistic forecasts: When do they work?” In: <em>arXiv e-Print</em>.
Martin, G. M., Loaiza-Maya, R., Maneesoonthorn, W., Frazier, D. T., and Ramı́rez-Hassan, A. (2022).“Optimal
probabilistic forecasts: When do they work?” In: <em>International Journal of Forecasting</em>.
Martin, R. (2021).“PyPortfolioOpt: portfolio optimization in Python.” In: <em>Journal of Open Source Software</em> 6(61),
p. 3066.
Martinez, A. B., Castle, J. L., and Hendry, D. F. (2020). <em>Smooth Robust Multi-Horizon Forecasts</em>. Tech. rep. George
Washington University.
Marwood, D. and Minnen, D. (2020).“Safely Boosting Retirement Income by Harmonizing Drawdown Paths.” In:
<em>Journal of Financial Planning</em> 33(11), pp. 46–60.
Maschner, C., Moritz, B., and Schmitz, M. (2021).“Modern Asset Management.” In: <em>SSRN e-Print</em>.
Masini, R. P., Medeiros, M. C., and Mendes, E. F. (2021).“Machine Learning Advances for Time Series Forecasting.”
In: <em>arXiv e-Print</em>.
McCracken, M. W. (2020).“Tests of Conditional Predictive Ability: Existence, Size, and Power.” In: <em>SSRN e-Print</em>.
McDonald, S., Coleman, S., McGinnity, T. M., Li, Y., and Belatreche, A. (2014).“A comparison of forecasting
approaches for capital markets.” In: <em>IEEE Conference on Computational Intelligence for Financial Engineering
and Economics (CIFEr)</em>. London, UK: IEEE, pp. 32–39.
McIndoe, C. (2020).“A Data Driven Approach to Market Regime Classification.” MA thesis. Imperial College.
McMillan, D. G. (2021a).“Forecasting sector stock market returns.” In: <em>Journal of Asset Management</em> 22(4), pp. 291–
300.
McMillan, D. G. (2021b).“Forecasting U.S. stock returns.” In: <em>The European Journal of Finance</em> 27(1-2), pp. 86–
109.
Meligkotsidou, L., Panopoulou, E., Vrontos, I. D., and Vrontos, S. D. (2021).“Out-of-sample equity premium
prediction: a complete subset quantile regression approach.” In: <em>The European Journal of Finance</em> 27(1-2),
pp. 110–135.
Menezes, A. G. and Mastelini, S. M. (2021).“MegazordNet: combining statistical and machine learning standpoints
for time series forecasting.” In: <em>arXiv e-Print</em>.</p>
<p>Messmer, M. and Audrino, F. (2020).“The Lasso and the Factor Zoo - Expected Returns in the Cross-Section.” In:
<em>SSRN e-Print</em>.
Micheli, A. and Neuman, E. (2022). “Evidence of Crowding on Russell 3000 Reconstitution Events.” In: <em>arXiv
e-Print</em>.
Mikeliani, R. and Kavlashvili, N. (2020).“Evaluation and comparison of machine learning and classical econometric
AR model on financial time series data.” MA thesis. University of Tartu.
Milevsky, M. A. (2020). <em>Retirement Income Recipes in R: From Ruin Probabilities to Intelligent Drawdowns</em>. Springer
International Publishing. 302 pp.
Montero-Manso, P., Athanasopoulos, G., Hyndman, R. J., and Talagala, T. S. (2020).“FFORMA: Feature-based
Forecast Model Averaging.” In: <em>International Jurnal of Forecasting</em> 36 (1), pp. 86–92.
Montero-Manso, P. and Hyndman, R. J. (2020).“Principles and Algorithms for Forecasting Groups of Time Series:
Locality and Globality.” In: <em>arXiv e-Print</em>.
Montero-Manso, P. and Hyndman, R. J. (2021).“Principles and Algorithms for Forecasting Groups of Time Series:
Locality and Globality.” In: <em>International Journal of Forecasting</em> 37(4), pp. 1632–1653.
Murray, M. H. and Blume, J. D. (2020).“False Discovery Rate Computation: Illustrations and Modifications.” In:
<em>arXiv e-Print</em>.
Neri, F. (2021).“Domain Specific Concept Drift Detectors for Predicting Financial Time Series.” In: <em>arXiv e-Print</em>.
Neto, A. E. D., Gonzalo, J., and Pitarakis, J.-Y. (2021).“Uncovering regimes in out of sample forecast errors.” In:
<em>Oxford Bulletin of Economics and Statistics</em> 83(3), pp. 713–741.
Nevasalmi, L. (2020).“Forecasting multinomial stock returns using machine learning methods.” In: <em>The Journal of
Finance and Data Science</em> 6, pp. 86–106.
Nevasalmi, L. (2022).“Recession forecasting with high-dimensional data.” In: <em>Journal of Forecasting</em>.
Nietert, B. and Otto, T. (2020).“Empirical asset pricing: economic significance and economic model evaluation.”
In: <em>SSRN e-Print</em>.
Noguer i Alonso, M., Batres-Estrada, G., and Moulin, A. (2020).“Deep Learning for Equity Time Series Prediction.”
In: <em>SSRN e-Print</em>.
Noguer i Alonso, M. and Srivastava, S. (2021).“The Shape of Performance Curve in Financial Time Series.” In:
<em>SSRN e-Print</em>.
Nonejad, N. (2021).“Bayesian model averaging and the conditional volatility process: an application to predicting
aggregate equity returns by conditioning on economic variables.” In: <em>Quantitative Finance</em> 21(8), pp. 1387–1411.
Nybrant, A. (2021).“On Robust Forecast Combinations With Applications to Automated Forecasting.” MA thesis.
University of Uppsala.
Nystrup, P., Lindstrom, E., Møller, J. K., and Madsen, H. (2021).“Dimensionality reduction in forecasting with
temporal hierarchies.” In: <em>International Journal of Forecasting</em> 37(3), pp. 1127–1146.
Odendahl, F., Rossi, B., and Sekhposyan, T. (2020).“Comparing Forecast Performance with State Dependence.”
In: <em>SSRN e-Print</em>.
Oh, D. H. and Patton, A. J. (2021).“Better the Devil You Know: Improved Forecasts from Imperfect Models.” In:
<em>Finance and Economics Discussion Series</em> 2021(070), pp. 1–45.
Okuno, S., Aihara, K., and Hirata, Y. (2019).“Combining multiple forecasts for multivariate time series via state-
dependent weighting.” In: <em>Chaos</em> 29(3), p. 033128.
Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. (2019).“N-BEATS: Neural basis expansion analysis for
interpretable time series forecasting.” In: <em>arXiv e-Print</em>.
Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. (2020).“N-BEATS: Neural basis expansion analysis for
interpretable time series forecasting.” In: <em>ICLR 2020 Conference</em>.
Papaioannou, P., Talmon, R., Serafino, D. di, and Siettos, C. (2021). “Time Series Forecasting Using Manifold
Learning.” In: <em>arXiv e-Print</em>.
Paranhos, L. (2021). “Predicting Inflation with Neural Networks.” In: <em>arXiv e-Print</em>.
Patton, A. J. (2020).“Comparing Possibly Misspecified Forecasts.” In: <em>Journal of Business &amp; Economic Statistics</em>
38(4), pp. 796–809.
Perrin, S. and Roncalli, T. (2020).“Machine Learning Optimization Algorithms &amp; Portfolio Allocation.” In: <em>Machine
Learning for Asset Management: New Developments and Financial Applications</em>. Ed. by E. Jurczenko. Wiley,
pp. 261–328.
Perron, P. and Yamamoto, Y. (2021).“Testing for Changes in Forecasting Performance.” In: <em>Journal of Business &amp;
Economic Statistics</em> 39(1), pp. 148–165.</p>
<p>Pesaran, M. H. and Smith, R. (2019).“The Role of Factor Strength and Pricing Errors for Estimation and Inference
in Asset Pricing Models.” In: <em>SSRN e-Print</em>.
Petropoulos, F., Apiletti, D., Assimakopoulos, V., Babai, M. Z., Barrow, D. K., Ben Taieb, S., Bergmeir, C., Bessa,
R. J., Bijak, J., Boylan, J. E., Browell, J., Carnevale, C., Castle, J. L., Cirillo, P., Clements, M. P., Cordeiro,
C., Oliveira, F. L. C., De Baets, S., Dokumentov, A., Ellison, J., Fiszeder, P., Franses, P. H., Frazier, D. T.,
Gilliland, M., Gonul, M. S., Goodwin, P., Grossi, L., Grushka-Cockayne, Y., Guidolin, M., Guidolin, M., Gunter,
U., Guo, X., Guseo, R., Harvey, N., Hendry, D. F., Hollyman, R., Januschowski, T., Jeon, J., Jose, V. R. R.,
Kang, Y., Koehler, A. B., Kolassa, S., Kourentzes, N., Leva, S., Li, F., Litsiou, K., Makridakis, S., Martin,
G. M., Martinez, A. B., Meeran, S., Modis, T., Nikolopoulos, K., Onkal, D., Paccagnini, A., Panagiotelis, A.,
Panapakidis, I., Pavia, J. M., Pedio, M., Pedregal, D. J., Pinson, P., Ramos, P., Rapach, D. E., Reade, J. J.,
Rostami-Tabar, B., Rubaszek, M., Sermpinis, G., Shang, H. L., Spiliotis, E., Syntetos, A. A., Talagala, P. D.,
Talagala, T. S., Tashman, L., Thomakos, D., Thorarinsdottir, T., Todini, E., Arenas, J. R. T., Wang, X.,
Winkler, R. L., Yusupova, A., and Ziel, F. (2022).“Forecasting: theory and practice.” In: <em>International Journal
of Forecasting</em>.
Petropoulos, F. and Grushka-Cockayne, Y. (2021).“Fast and Frugal Time Series Forecasting.” In: <em>SSRN e-Print</em>.
Petropoulos, F. and Spiliotis, E. (2021).“The Wisdom of the Data: Getting the Most Out of Univariate Time Series
Forecasting.” In: <em>Forecasting</em> 3(3), pp. 478–497.
Petropoulos, F., Spiliotis, E., and Panagiotelis, A. (2021).“Model combinations through revised base-rates.” In:
<em>arXiv e-Print</em>.
Petropoulos, F. and Svetunkov, I. (2020).“A simple combination of univariate models.” In: <em>International Journal
of Forecasting</em> 36(1), pp. 110–115.
Petropoulos, F., Wang, X., and Disney, S. M. (2019).“The inventory performance of forecasting methods: Evidence
from the M3 competition data.” In: <em>International Journal of Forecasting</em> 35(1), pp. 251–265.
Pinho, D. M. (2020).“Forecast comparison of volatility models and their combinations (FTSE100): a tied race.”
MA thesis. Universidade do Minho.
Pinto, J. M. and Castle, J. (2021). <em>A machine learning dynamic switching approach to forecasting when there are
structural breaks</em>. Tech. rep. University of Oxford.
Pinto, J. M. and Marçal, E. F. (2019).“Cross-Validation Based Forecasting Method: A Machine Learning Approach.”
In: <em>SSRN e-Print</em>.
Pinto, J. M. and Marçal, E. F. (2020).“Inflation Rate Forecasting: Extreme Learning Machine as a Model Combi-
nation Method.” In: <em>Contributions to Statistics</em>. Springer International Publishing, pp. 365–385.
Pitarakis, J.-Y. (2020).“A Novel Approach to Predictive Accuracy Testing in Nested Environments.” In: <em>arXiv
e-Print</em>.
Post, T., Karabati, S., and Arvanitis, S. (2019).“Robust optimization of forecast combinations.” In: <em>International
Journal of Forecasting</em> 35(3), pp. 910–926.
Prasad, V. V., Gumparthi, S., Venkataramana, L. Y., Srinethe, S., Sree, R. M. S., and Nishanthi, K. (2021).
“Prediction of Stock Prices Using Statistical and Machine Learning Models: A Comparative Analysis.” In: <em>The
Computer Journal</em>.
Prayogo, N., Cevik, M., and Bodur, M. (2020). “Time Series Sampling for Probabilistic Forecasting.” In: <em>Proceedings
of the 30th Annual International Conference on Computer Science and Software Engineering</em>. USA: IBM Corp.,
pp. 153–162.
Qian, W., Rolling, C. A., Cheng, G., and Yang, Y. (2019).“On the forecast combination puzzle.” In: <em>Econometrics</em>
7(3), p. 39.
Qian, W., Rolling, C. A., Cheng, G., and Yang, Y. (2022).“Combining forecasts for universally optimal perfor-
mance.” In: <em>International Journal of Forecasting</em>.
Qu, R., Timmermann, A., and Zhu, Y. (2021).“Comparing forecasting performance in cross-sections.” In: <em>Journal
of Econometrics</em>.
Quaedvlieg, R. (2021).“Multi-Horizon Forecast Comparison.” In: <em>Journal of Business &amp; Economic Statistics</em>.
Radchenko, P., Vasnev, A. L., and Wang, W. (2022).“Too similar to combine? On negative weights in forecast
combination.” In: <em>International Journal of Forecasting</em>.
Rahimikia, E. and Poon, S.-H. (2021).“Machine Learning for Realised Volatility Forecasting.” In: <em>SSRN e-Print</em>.
Rajapaksha, D., Bergmeir, C., and Hyndman, R. J. (2021). “LoMEF: A Framework to Produce Local Explanations
for Global Model Time Series Forecasts.” In: <em>arXiv e-Print</em>.
Rao, A. and Jelvis, T. (2022). <em>Foundations of Reinforcement Learning with Applications in Finance</em>.</p>
<p>Rapach, D. and Zhou, G. (2022).“Asset Pricing: Time-Series Predictability.” In: <em>SSRN e-Print</em>.
Rapach, D. E., Strauss, J. K., Tu, J., and Zhou, G. (2019).“Industry return predictability: A machine learning
approach.” In: <em>The Journal of Financial Data Science</em> 1(3), pp. 9–28.
Rapach, D. E. and Zhou, G. (2020).“Time-series and Cross-sectional Stock Return Forecasting: New Machine
Learning Methods.” In: <em>Machine Learning for Asset Management: New Developments and Financial Applications</em>.
Ed. by E. Jurczenko. Wiley, pp. 1–33.
Rehman, H.-U., Wan, G., Ullah, A., and Shaukat, B. (2019).“Individual and combination approaches to forecasting
hierarchical time series with correlated data: an empirical study.” In: <em>Journal of Management Analytics</em> 6(3),
pp. 231–249.
Remlinger, C., Alasseur, C., Briere, M., and Mikael, J. (2022).“Expert Aggregation for Financial Forecasting.” In:
<em>SSRN e-Print</em>.
Reschenhofer, E., Mangat, M. K., Zwatz, C., and Guzmics, S. (2020).“Evaluation of current research on stock
return predictability.” In: <em>Journal of Forecasting</em> 39(2), pp. 334–351.
Risse, M. (2017).“Combining Wavelet Decomposition with Machine Learning to Forecast Gold Returns.” In: <em>SSRN
e-Print</em>.
Roccazzella, F., Gambetti, P., and Vrins, F. (2022).“Optimal and robust combination of forecasts via constrained
optimization and shrinkage.” In: <em>International Journal of Forecasting</em>.
Roncalli, T. (2021).“Advanced Course in Asset Management.” In: <em>SSRN e-Print</em>.
Rossi, B. (2020).“Forecasting in the Presence of Instabilities: How Do We Know Whether Models Predict Well and
How to Improve Them.” In: <em>Journal of Economic Literature</em>.
Roy, R. (2021).“A six-factor asset pricing model: The Japanese evidence.” In: <em>Financial Planning Review</em> 4(1).
Rožanec, J., Trajkova, E., Kenda, K., Fortuna, B., and Mladenić, D. (2021).“Explaining Bad Forecasts in Global
Time Series Models.” In: <em>Applied Sciences</em> 11(19), p. 9243.
Ruan, J., Wu, W., and Luo, J. (2021).“Stock Price Prediction Under Anomalous Circumstances.” In: <em>arXiv e-Print</em>.
Ryll, L. and Seidens, S. (2019).“Evaluating the Performance of Machine Learning Algorithms in Financial Market
Forecasting: A Comprehensive Survey.” In: <em>arXiv e-Print</em>.
Rytchkov, O. and Zhong, X. (2020).“Information Aggregation and P-Hacking.” In: <em>Management Science</em> 66(4),
pp. 1509–1782.
Salinas, D., Flunkert, V., and Gasthaus, J. (2020).“DeepAR: Probabilistic Forecasting with Autoregressive Recur-
rent Networks.” In: <em>International Journal of Forecasting</em> 36 (3), pp. 1181–1191.
Salisu, A. A. and Tchankam, J. P. (2022).“US Stock return predictability with high dimensional models.” In:
<em>Finance Research Letters</em> 45 (102194), pp. 153–163.
Salles, R., Pacitti, E., Bezerra, E., Porto, F., and Ogasawara, E. (2022).“TSPred: A framework for nonstationary
time series prediction.” In: <em>Neurocomputing</em> 467, pp. 197–202.
Samuels, J. D. and Sekkel, R. M. (2017).“Model Confidence Sets and forecast combination.” In: <em>International
Journal of Forecasting</em> 33(1), pp. 48–60.
Sarmas, E., Xidonas, P., and Doukas, H. (2020). <em>Multicriteria Portfolio Construction with Python</em>. Springer Inter-
national Publishing.
Seca, D. (2021).“TimeGym: Debugging for Time Series Modeling in Python.” In: <em>arXiv e-Print</em>.
Sharma, A., Syrgkanis, V., Zhang, C., and Kiciman, E. (2021). “DoWhy: Addressing Challenges in Expressing and
Validating Causal Assumptions.” In: <em>arXiv e-Print</em>.
Sharma, P. N., Shmueli, G., Sarstedt, M., Danks, N., and Ray, S. (2020).“Prediction-Oriented Model Selection in
Partial Least Squares Path Modeling.” In: <em>Decision Sciences</em>.
Shaub, D. (2020).“Fast and accurate yearly time series forecasting with forecast combinations.” In: <em>International
Journal of Forecasting</em> 33(1), pp. 116–120.
Shi, X., Xu, D., and Zhang, Z. (2022). “Deep Learning Algorithms for Hedging with Frictions.” In: <em>arXiv e-Print</em>.
Siami-Namini, S., Tavakoli, N., and Namin, A. S. (2019).“A Comparative Analysis of Forecasting Financial Time
Series Using ARIMA, LSTM, and BiLSTM.” In: <em>arXiv e-Print</em>.
Siebert, J., Gross, J., and Schroth, C. (2021).“A systematic review of Python packages for time series analysis.”
In: <em>Engineering Proceedings</em> 5(1) (22).
Siliverstovs, B. and Wochner, D. (2021).“State-Dependent Evaluation of Predictive Ability.” In: <em>Journal of Fore-
casting</em> 40(3), pp. 547–574.
Simos, T. E., Mourtas, S. D., and Katsikis, V. N. (2021).“Time-varying Black–Litterman portfolio optimization
using a bio-inspired approach and neuronets.” In: <em>Applied Soft Computing</em> 112, p. 107767.</p>
<p>Smith, S. C., Bulkley, G., and Leslie, D. S. (2020).“Equity Premium Forecasts with an Unknown Number of
Structural Breaks.” In: <em>Journal of Financial Econometrics</em> 18(1), pp. 59–94.
Smith, S. C. and Timmermann, A. (2021).“Break Risk.” In: <em>The Review of Financial Studies</em> 34(4), pp. 2045–2100.
Smyl, S. (2020).“A hybrid method of exponential smoothing and recurrent neural networks for time series fore-
casting.” In: <em>International Journal of Forecasting</em> 36(1) (1), pp. 75–85.
Snow, D. (2019).“Machine learning in asset management.” In: <em>SSRN e-Print</em>.
Snow, D. (2020a).“Machine Learning in Asset Management - Part 2: Portfolio Construction - Weight Optimization.”
In: <em>The Journal of Financial Data Science</em> 2 (2), pp. 17–24.
Snow, D. (2020b).“Machine Learning in Asset Management Part 1: Portfolio Construction Trading Strategies.” In:
<em>The Journal of Financial Data Science</em> 2(1) (1), pp. 10–23.
Son, B. and Lee, J. (2022).“Graph-based multi-factor asset pricing model.” In: <em>Finance Research Letters</em> 44 (102032).
Spiliotis, E., Abolghasemi, M., Hyndman, R. J., Petropoulos, F., and Assimakopoulos, V. (2021).“Hierarchical
forecast reconciliation with machine learning.” In: <em>Applied Soft Computing</em> 112, p. 107756.
Spiliotis, E., Nikolopoulos, K., and Assimakopoulos, V. (2019).“Tales from tails: On the empirical distributions of
forecasting errors and their implication to risk.” In: <em>International Journal of Forecasting</em> 35(2), pp. 687–698.
Stauskas, O. and Westerlund, J. (2022).“Tests of Equal Forecasting Accuracy for Nested Models with Estimated
CCE Factors.” In: <em>Journal of Business &amp; Economic Statistics</em> , pp. 1–14.
Stein, T. (2021).“Out-of-Sample Equity Premium Prediction: Combination Forecasts with Frequency-Decomposed
Variables.” In: <em>2nd Frontiers of Factor Investing Conference</em>.
Stivers, A. (2018).“Equity premium predictions with many predictors: A risk-based explanation of the size and
value factors.” In: <em>Journal of Empirical Finance</em> 45, pp. 126–140.
Stoyanov, S. V. and Fabozzi, F. A. (2021).“Dynamics of Equity Factor Returns and Asset Pricing.” In: <em>Journal of
Financial Econometrics</em>.
Suhonen, A., Lennkh, M., and Perez, F. (2017).“Quantifying Backtest Overfitting in Alternative Beta Strategies.”
In: <em>The Journal of Portfolio Management</em> 43 (2), pp. 90–104.
Svensson, M. (2018).“An Evaluation of Methods for Combining Univariate Time Series Forecasts.” MA thesis. Lund
University.
Tadayon, M. and Iwashita, Y. (2020).“Comprehensive Analysis of Time Series Forecasting Using Neural Networks.”
In: <em>arXiv e-Print</em>.
Taggart, R. J. (2021). “Evaluation of point forecasts for extreme events using consistent scoring functions.” In: <em>arXiv
e-Print</em>.
Taillardat, M., Fougeres, A.-L., Naveau, P., and de Fondeville, R. (2022). “Extreme events evaluation using CRPS
distributions.” In: <em>arXiv e-Print</em>.
Talagala, T. S., Li, F., and Kang, Y. (2021).“FFORMPP: Feature-based forecast model performance prediction.”
In: <em>arXiv e-Print</em>.
Talagala, T. S., Li, F., and Kang, Y. (2022).“FFORMPP: Feature-based forecast model performance prediction.”
In: <em>International Journal of Forecasting</em>.
Tang, X., Hu, F., and Wang, P. (2018).“Out-of-sample equity premium prediction: A scenario analysis approach.”
In: <em>Journal of Forecasting</em> 37(5), pp. 604–626.
Tatsat, H., Puri, S., and Lookabaugh, B. (2020). <em>Machine Learning and Data Science Blueprints for Finance: From
Building Trading Strategies to Robo-Advisors Using Python</em>. O’Reilly. 400 pp.
Taylor, J. W. and Taylor, K. S. (2021).“Combining probabilistic forecasts of COVID-19 mortality in the United
States.” In: <em>European Journal of Operational Research</em>.
Theodosiou, F. and Kourentzes, N. (2021).“Forecasting with Deep Temporal Hierarchies.” In: <em>SSRN e-Print</em>.
Thomson, M. E., Pollock, A. C., Onkal, D., and Gonul, M. S. (2019).“Combining forecasts: Performance and
coherence.” In: <em>International Journal of Forecasting</em> 35(2), pp. 474–484.
Thorarinsdottir, T. L. (2021).“Forecast evaluation.” In: <em>CUSO winter school</em>.
Tilly, S., Ebner, M., and Livan, G. (2021).“Macroeconomic forecasting through news, emotions and narrative.” In:
<em>Expert Systems with Applications</em> 175, p. 114760.
Tilly, S. and Livan, G. (2021).“Macroeconomic forecasting with statistically validated knowledge graphs.” In: <em>Expert
Systems with Applications</em> 186, p. 115765.
Timmermann, A. (2018).“Forecasting methods in finance.” In: <em>Annual Review of Financial Economics</em> 10(1),
pp. 449–470.</p>
<p>Trucı́os, C., Mazzeu, J. H. G., Hallin, M., Hotta, L. K., Pereira, P. L. V., and Zevallos, M. (2021).“Forecasting
Conditional Covariance Matrices in High-Dimensional Time Series: a General Dynamic Factor Approach.” In:
<em>Journal of Business &amp; Economic Statistics</em> , pp. 1–35.
Tuck, J., Barratt, S., and Boyd, S. (2022).“Portfolio Construction Using Stratified Models.” In: <em>Machine Learning
in Financial Markets: A guide to contemporary practices</em>. Ed. by A. Capponi and C.-A. Lehalle. Cambridge
University Press.
Tunaru, D., Fabozzi, F. A., and Fabozzi, F. J. (2021).“Testing the Forecasting Ability of Multi-Factor Models on
Non-US Interbank Rates.” In: <em>The Journal of Fixed Income</em> 31(2).
Ungolo, F., Sherris, M., and Zhou, Y. (2021).“affine_mortality: A Github repository for estimation, analysis, and
projection of affine mortality models.” In: <em>SSRN e-Print</em>.
Vaiciukynas, E., Danenas, P., Kontrimas, V., and Butleris, R. (2022).“Two-Step Meta-Learning for Time-Series
Forecasting Ensemble.” In: <em>IEEE Access</em> 9, pp. 62687–62696.
Vamossy, D. and Skog, R. (2021). “EmTract: Investor Emotions and Market Behavior.” In: <em>arXiv e-Print</em>.
van Dijk, D. and Franses, P. H. (2019).“Combining expert-adjusted forecasts.” In: <em>Journal of Forecasting</em> 38(5),
pp. 415–421.
Vanini, P. (2020).“Asset Management.” In: <em>SSRN e-Print</em>.
Vincent, K., Hsu, Y.-C., and Lin, H.-W. (2020).“Investment styles and the multiple testing of cross-sectional stock
return predictability.” In: <em>Journal of Financial Markets</em>.
Vinod, H. D. (2021).“R Package GeneralCorr Functions for Portfolio Choice.” In: <em>SSRN e-Print</em>.
Viswanathan, T. and Stephen, M. (2020).“Does Machine Learning Algorithms Improve Forecasting Accuracy?
Predicting Stock Market Index Using Ensemble Model.” In: <em>Advances in Distributed Computing and Machine
Learning</em>. Springer Singapore, pp. 511–519.
Vovk, V. and Wang, R. (2021).“E-values: Calibration, combination, and applications.” In: <em>Annals of Statistics</em>
49(3), pp. 1736–1753.
Wang, H., Ahluwalia, H. S., Aliaga-Diaz, R. A., and Davis, J. H. (2021a).“The Best of Both Worlds: Forecasting US
Equity Market Returns Using a Hybrid Machine Learning Time Series Approach.” In: <em>The Journal of Financial
Data Science</em> 3(2), pp. 9–20.
Wang, R. and Ramdas, A. (2020).“False discovery rate control with e-values.” In: <em>arXiv e-Print</em>.
Wang, Y., Hao, X., and Wu, C. (2021b).“Forecasting stock returns: A time-dependent weighted least squares
approach.” In: <em>Journal of Financial Markets</em> 53 (100568), p. 100568.
Wang, Y., Smola, A., Maddix, D., Gasthaus, J., Foster, D., and Januschowski, T. (2019).“Deep Factors for Fore-
casting.” In: <em>Proceedings of Machine Learning Research</em> 97, pp. 6607–6617.
Weigand, A. (2019).“Machine learning in empirical asset pricing.” In: <em>Financial Markets and Portfolio Management</em>
33, pp. 93–104.
Weiss, C. E., Raviv, E., and Roetzer, G. (2018).“Forecast Combinations in R using the ForecastComb Package.”
In: <em>The R Journal</em> 10(2), pp. 262–281.
Wellens, A. P., Udenio, M., and Boute, R. N. (2022).“Transfer learning for hierarchical forecasting: Reducing
computational efforts of M5 winning methods.” In: <em>International Journal of Forecasting</em>.
Wen, D., He, M., Zhang, Y., and Wang, Y. (2022).“Forecasting realized volatility of Chinese stock market: A simple
but efficient truncated approach.” In: <em>Journal of Forecasting</em>.
Westerlund, J., Karabiyik, H., and Narayan, P. (2017).“Testing for Predictability in panels with General Predictors.”
In: <em>Journal of Applied Econometrics</em> 32(3), pp. 554–574.
Winkler, R. L. (2015).“Equal Versus Differential Weighting in Combining Forecasts.” In: <em>Risk Analysis</em> 35(11),
pp. 16–18.
Wu, H., Xu, J., Wang, J., and Long, M. (2022).“Autoformer: Decomposition Transformers with Auto-Correlation
for Long-Term Series Forecasting.” In: <em>arXiv e-Print</em>.
Wu, Q., Brinton, C. G., Zhang, Z., Pizzoferrato, A., Liu, Z., and Cucuringu, M. (2021a). “Equity2Vec: End-to-end
Deep Learning Framework for Cross-sectional Asset Pricing.” In: <em>arXiv e-Print</em>.
Wu, X., Zhang, D., Guo, C., He, C., Yang, B., and Jensen, C. S. (2021b). “AutoCTS: Automated Correlated Time
Series Forecasting – Extended Version.” In: <em>arXiv e-Print</em>.
Xie, A. (2021).“Forecasting Long-Term Equity Returns: A Comparison of Popular Methodologies.” In: <em>SSRN e-
Print</em>.
Xu, W., Liu, W., Bian, J., Yin, J., and Liu, T.-Y. (2021). “Instance-wise Graph-based Framework for Multivariate
Time Series Forecasting.” In: <em>arXiv e-Print</em>.</p>
<p>Xu, W., Liu, W., Wang, L., Xia, Y., Bian, J., Yin, J., and Liu, T.-Y. (2022). “HIST: A Graph-based Framework
for Stock Trend Forecasting via Mining Concept-Oriented Shared Information.” In: <em>arXiv e-Print</em>.
Yang, J.-Y., Zhu, H., Hou, Y.-J., Zhang, P., and Zhou, C.-C. (2021). “Why Existing Machine Learning Methods
Fails At Extracting the Information of Future Returns Out of Historical Stock Prices : the Curve-Shape-Feature
and Non-Curve-Shape-Feature Modes.” In: <em>arXiv e-Print</em>.
Yang, L., Li, J., Dong, R., Zhang, Y., and Smyth, B. (2022). “NumHTML: Numeric-Oriented Hierarchical Trans-
former Model for Multi-task Financial Forecasting.” In: <em>arXiv e-Print</em>.
Yang, Y., UY, M. C. S., and Huang, A. (2020). “FinBERT: A Pretrained Language Model for Financial Commu-
nications.” In: <em>arXiv e-Print</em>.
Yara, F. B., Boons, M., and Tamoni, A. (2021).“Value return predictability across asset classes and commonalities
in risk premia.” In: <em>Review of Finance</em> 25(2), pp. 449–484.
Yeoleka, A., Patel, S., Talla, S., Puthucode, K. R., Ahmadzadeh, A., Sadykov, V. M., and Angryk, R. A. (2021).
“Feature Selection on a Flare Forecasting Testbed: A Comparative Study of 24 Methods.” In: <em>arXiv e-Print</em>.
Yin, A. (2021).“Equity premium prediction: keep it sophisticatedly simple.” In: <em>Quantitative Finance and Economics</em>
5(2), pp. 264–286.
Yu, L., Hardle, W. K., Borke, L., and Benschop, T. (2020).“An AI approach to measuring financial risk.” In: <em>SSRN
e-Print</em>.
Zang, C. (2017).“Deep Learning in Multiple Multistep Time Series Prediction.” In: <em>arXiv e-Print</em>.
Zeng, Z., Balch, T., and Veloso, M. (2021).“Deep Video Prediction for Time Series Forecasting.” In: <em>arXiv e-Print</em>.
Zhan, T. and Xiao, F. (2021).“A Fast Evidential Approach for Stock Forecasting.” In: <em>arXiv e-Print</em>.
Zhang, H. (2021).“Empirical asset pricing and ensemble machine learning.” PhD thesis. Tilburg University.
Zhao, L. (2020).“Essays on Asset Pricing: A Model Comparison Perspective.” PhD thesis. Washington University
in St. Louis.
Zhao, Y. (2021).“The robustness of forecast combination in unstable environments: a Monte Carlo study of advanced
algorithms.” In: <em>Empirical Economics</em> 61, pp. 173–199.
Zhao, Y., Wang, Y., Liu, J., Xia, H., Xu, Z., Hong, Q., Zhou, Z., and Petzold, L. (2021). “Empirical Quantitative
Analysis of COVID-19 Forecasting Models.” In: <em>arXiv e-Print</em>.
Zhu, L., Basu, S., Jarrow, R. A., and Wells, M. T. (2021). “High-Dimensional Estimation, Basis Assets, and the
Adaptive Multi-Factor Model.” In: <em>arXiv e-Print</em>.
Zhu, Y. and Timmermann, A. (2020).“Can Two Forecasts Have the Same Conditional Expected Accuracy?” In:
<em>arXiv e-Print</em>.
Ziel, F. and Berk, K. (2019).“Multivariate Forecasting Evaluation: On Sensitive and Strictly Proper Scoring Rules.”
In: <em>arXiv e-Print</em>.</p>
<h2>Appendix A: Overviews of investment processes and models in QWIM</h2>
<h3>References</h3>
<p>List of references:
Coqueret and Guida ( <em>Machine Learning for Factor Investing: R Version</em> , 2020)
Dixon et al. ( <em>Machine Learning in Finance: from theory to practice</em> , 2020)
Fabozzi et al. ( <em>Asset Management: Tools and Issues</em> , 2021)
Grealish and Kolm (“Robo-Advisory: From Investing Principles and Algorithms to Future Developments,” 2021)
Homescu (“Many risks, one (optimal) portfolio,” 2014)
Homescu (“Better Investing Through Factors, Regimes and Sensitivity Analysis,” 2015)
Jansen ( <em>Machine Learning for Algorithmic Trading (Second Edition)</em> , 2020)
Jurczenko et al. ( <em>Machine Learning for Asset Management</em> , 2020)
Kritzman et al. ( <em>A Practitioner’s Guide to Asset Allocation</em> , 2017)
Lopez de Prado ( <em>Machine learning for asset managers</em> , 2020)
Maschner et al. (“Modern Asset Management,” 2021)
Perrin and Roncalli (“Machine Learning Optimization Algorithms &amp; Portfolio Allocation,” 2020)
Roncalli (“Advanced Course in Asset Management,” 2021)
Vanini (“Asset Management,” 2020)</p>
<h3>Online courses</h3>
<p>List of online courses:</p>
<ul>
<li>Investment Management with Python and Machine Learning Specialization</li>
</ul>
<p><code>Introduction to Portfolio Construction and Analysis with Python
Advanced Portfolio Construction and Analysis with Python
Python and Machine Learning for Asset Management
Python and Machine Learning for Asset Management with Alternative Data Sets</code>
- Machine Learning and Reinforcement Learning in Finance Specialization</p>
<p><code>Guided Tour of Machine Learning in Finance
Fundamentals of Machine Learning in Finance
Reinforcement Learning in Finance
Overview of Advanced Methods of Reinforcement Learning in Finance</code>
- Investment Management Specialization</p>
<p><code>Understanding Financial Markets
Meeting Investors’ Goals
Portfolio and Risk Management
Securing Investment Returns in the Long Run
Planning your Client’s Wealth over a 5-year Horizon</code>
- Investment and Portfolio Management Specialization</p>
<p><code>Global Financial Markets and Instruments
Portfolio Selection and Risk Management
Biases and Portfolio Selection
Investment Strategies and Portfolio Analysis
Build a Winning Investment Portfolio</code></p>
<h2>Appendix C: Comparison of investment using portfolios metrics and benchmark portfolios</h2>
<h2>mark portfolios</h2>
<p>For your QWIM project it is likely that you would compare investment portfolios constructed using your method(s)
versus benchmark portfolios constructed using most common "optimal portfolio" types used in the industry and in
academia. See below for an example of how this can be done.</p>
<p><code>Figure 6: Example of portfolio optimization process</code>
Source:PyPortfolioOpt</p>
<h3>Portfolio optimization methods</h3>
<p>List of portfolio optimization methods may include (seeRoncalli (“Advanced Course in Asset Management,” 2021)
and Perrin and Roncalli (“Machine Learning Optimization Algorithms &amp; Portfolio Allocation,” 2020)for a com-
prehensive overview of such methods):</p>
<ul>
<li>equal weighting</li>
<li>mean variance optimization (Markowitz)</li>
<li>minimum variance optimization</li>
<li>maximum diversification</li>
<li>risk budgeting/risk parity</li>
<li>hierarchical risk parity</li>
<li>Black-Litterman</li>
<li>robust versions of some the above portfolio optimization methods</li>
</ul>
<p>Some relevant links:</p>
<ul>
<li>Portfolio Optimization: A General Framework for Portfolio Choice</li>
<li>Performance of risk-based asset allocation strategies</li>
<li>Revisiting the Portfolio Optimization Machine Portfolio</li>
<li>Construction Techniques Applied to Traditional Multi Asset Portfolios</li>
</ul>
<h3>Python and R packages/codes for portfolio optimization</h3>
<ul>
<li>Codes mentioned inSnow (“Machine Learning in Asset Management - Part 2: Portfolio Construction - Weight
    Optimization,” 2020)</li>
<li>Empyrial</li>
<li>MLFinLab</li>
<li>Optimal Portfolio</li>
<li>PortfolioAnalytics</li>
<li>PortfolioLab</li>
<li>PyPortfolioOpt</li>
<li>Quantropy</li>
<li>Riskfolio-Lib</li>
<li>RiskPortfolios</li>
<li>riskparityportfolio</li>
</ul>
<h3>Portfolio metrics</h3>
<p>List of portfolio metrics may include some of the following (seeBacon (“Performance Attribution: History and
Progress,” 2019)for a comprehensive list):</p>
<ul>
<li>Sharpe ratio</li>
<li>Sortino ratio</li>
<li>Information ratio</li>
<li>Maximum Drawdown</li>
<li>expected shortfall</li>
<li>maximum loss</li>
<li>and more.</li>
</ul>
<p>Some relevant links:</p>
<ul>
<li>Portfolio metrics</li>
<li>Picking the Right Risk-Adjusted Performance Metric</li>
<li>Risk-Adjusted Performance Measurement – State of the Art</li>
<li>An Investor’s Guide to the Risk Versus Return Conundrum</li>
<li>How sharp is the Sharpe ratio? Risk-adjusted Performance Measures</li>
</ul>
<h3>Python and R packages/codes for portfolio metrics and performance evaluation</h3>
<ul>
<li>bt</li>
<li>empyrical</li>
<li>ffn</li>
<li>JFE</li>
<li>MLFinLab</li>
<li>PerformanceAnalytics</li>
<li>portfolioBacktest</li>
<li>Portfolio Optimization and Performance Evaluation</li>
<li>Pyfolio</li>
<li>QuantStats</li>
<li>Riskfolio-Lib</li>
<li>tidyquant</li>
</ul>
<h3>How to compare investment portfolios</h3>
<p>Let us consider portfolio optimization methods (selected from the ones implemented in Python and/or R packages
mentioned above, such as PyPortfolioOpt) which rely on based on expected returns and expected covariance matrix.
One would construct two portfolios (let’s call them Traditional and Enhanced) using the same portfolio op-
timization method(s), where the only difference would be in terms of the inputs (expected returns and expected
covariance matrix) to the optimization method:
As of the date of portfolio construction, expected returns and expected covariance matrix can be either calculated
using only historical data or, respectively, output from your model. Then one would compare side-by-side various
portfolio metrics for these two portfolios. Comparison would be done across the entire Out-Of-Sample period, and
also across each market regine period.
NOTE: If you have N forecasting methods used in your coding framework, then for each optimizaton method
you would end up with (1+N) optimal portfolios
To exemplifty, let’s say that you want to construct portfolios at date of June 20, 2019, and you have data as
below</p>
<ul>
<li>Range of entire dataset: January 1st, 1990 - August 1, 2020</li>
<li>Range of Training dataset: January 1st, 1990- February 20, 2017</li>
<li>Range of Test dataset: February 20, 2017 - August 1, 2020</li>
</ul>
<p>For Traditional portfolio:</p>
<ul>
<li>vector of expected means is calculated based on historical data available at June 20, 2019 (namely from 1990
    to June 19, 2019)</li>
<li>expected covariance matrix is calculated based on historical data available at June 20, 2019 (namely from
    1990 to June 19, 2019)</li>
</ul>
<p>For Enhanced portfolio:</p>
<ul>
<li>
<p>vector of expected means is calculated based on forecasted values available at June 20, 2019 and obtained
    using the forecasted model trained on given training dataset (which is from 1990 to 2017)</p>
</li>
<li>
<p>expected covariance matrix is calculated based on forecasted values available at June 20, 2019 and obtained
    using the forecasted model trained on given training dataset (which is from 1990 to 2017)</p>
</li>
</ul>
<p>Then one would compare various portfolio metrics among the two portfolios. These metrics can be calculated on
following time periods:</p>
<ul>
<li>from date of portfolio construction (June 20, 2019) to last date for which you have data (August 1, 2020)</li>
<li>from starting date of dataset (January 1st, 1990) to last date for which you have data (August 1, 2020)</li>
<li>from starting date of dataset (January 1st, 1990) to date of portfolio construction (June 20, 2019)</li>
</ul>
<p>So you would have side-by-side comparisons of portfolio metrics for each of the above 3 time periods.
Portfolio metrics can be calculated using various Python and/or R packages mentioned above.</p>